#!/usr/bin/env ruby
# frozen_string_literal: true
require 'date'
require 'digest'
require 'json'
require 'net/http'
require 'optparse'
require 'ostruct'
require 'pathname'
require 'set'
require 'uri'

# TODO:
# * Allow mapping channel call sign/number combos to custom names, which take
#   preference over any that are auto-detected

# Returns the SHA-512 digest of the given stringified values.
def digest(
  *values, # Array<any>
  format: :hex, # :hex | :base64 | :bubblebabble
  length: 999_999_999 # The maximum length at which to truncate the resulting digest
)
  d = Digest::SHA2.new(512)
  values.each { |v| d.update(v.to_s) }

  hash =
    case format
      when :hex then d.hexdigest()
      when :base64 then d.base64digest()
      when :bubblebabble then d.bubblebabble()
      else
        raise ArgumentError, "Invalid format: #{format.inspect}"
    end

  hash[0...length]
end

PROGRAM_NAME = 'tv_grab_zap2it'
VERSION = Gem::Version.new('0.0.1').freeze
RELEASE =
  begin
    # Compute a truncated SHA-512 digest of the current file and use this as our
    # release identifier. Since the entire program is in a single file, this is
    # guaranteed to provide a unique and stable value for each "build".
    file_data = File.read(Pathname.new(__FILE__).realpath)

    digest(file_data, format: :hex, length: 8)
  end

# Compile the version information into a single string for easy use elsewhere.
VERSION_INFO = "#{PROGRAM_NAME} v#{VERSION} (#{RELEASE})"

XMLTV_DESCRIPTION = 'USA/Canada (zap2it.com)'
XMLTV_CAPABILITIES = %w[baseline cache].freeze

ZAP2IT_API_HOST = 'https://tvlistings.zap2it.com'
WIKIPEDIA_API_HOST = 'https://en.wikipedia.org/w/api.php'

# The name of the Wikipedia "template" that contains the information we're
# looking for. This is basically a built-in directive for displaying information
# about something in a standardized way, in our case a television station.
#
# See https://en.wikipedia.org/wiki/Template:Infobox_television_station for the
# template's documentation.
WIKIPEDIA_TV_STATION_INFOBOX_TEMPLATE_TITLE = 'Infobox television station'

# All the tokens we care about for use with parsing Wikitext. We don't do a
# terribly great job at parsing, mostly only caring about the things represented
# here in a lightweight way.
WIKITEXT_TOKENS = [
  # A template looks like `{{template_name|arg1 = foo|arg2 = bar}}` and is
  # basically a function for generating other text.
  #
  # See
  # https://en.wikipedia.org/wiki/Help:Wikitext#Templates_and_transcluding_pages
  # for more information.
  WIKITEXT_TOKEN_TEMPLATE_START = '{{',
  WIKITEXT_TOKEN_TEMPLATE_END = '}}',

  # These are... something. I'm not sure what they're _actually_ called, but
  # they appear to be template-like, just with a different syntax.
  WIKITEXT_TOKEN_CLASS_START = '{|',
  WIKITEXT_TOKEN_CLASS_END = '|}',

  # A link forms an edge in the graph that is Wikipedia. It typically looks like
  # `[[Some page title]]` or `[[Some page title|Some human-readable page
  # title]]`.
  #
  # See https://en.wikipedia.org/wiki/Help:Wikitext#Links_and_URLs for
  # more information.
  WIKITEXT_TOKEN_LINK_START = '[[',
  WIKITEXT_TOKEN_LINK_END = ']]',

  # Makes text appear bolded.
  #
  # See https://en.wikipedia.org/wiki/Help:Wikitext#Text_formatting for more
  # information.
  WIKITEXT_TOKEN_BOLD = "'''",

  # Generic tokens used as part of other things, like templates and links.
  WIKITEXT_TOKEN_EQUALS = '=',
  WIKITEXT_TOKEN_PIPE = '|',
].freeze

# A HTML/XML tag, e.g. `<ref>`.
WIKITEXT_REGEXP_TOKENS = [
  WIKITEXT_TOKEN_START_TAG = %r[<\s*[^/]\s*[^>]+?\s*>].freeze,
  WIKITEXT_TOKEN_END_TAG = %r[<\s*/\s*[^>]+?\s*>].freeze,
].freeze

# A regexp that matches any one Wikitext token as its first group, useful with
# `String#split` to obtain tokens and "regular" text.
WIKITEXT_TOKENIZER = %r[
  (
    # Turn our token list into something like `a|b|c?d`
    #{WIKITEXT_TOKENS.map { |t| Regexp.escape(t) }.join('|')}
    | #{WIKITEXT_REGEXP_TOKENS.join('|')}
  )
]x.freeze

# Useful time constants for obtaining a number of seconds via multiplication.
class TimeInterval
  SECONDS = 1
  MINUTES = 60 * SECONDS
  HOURS = 60 * MINUTES
  DAYS = 24 * HOURS
  WEEKS = 7 * DAYS
end.freeze

# A builder-style class for parsing and validating arguments in a standardized
# way.
class Arg
  attr_reader \
    :name,
    :parsers,
    :value

  def initialize(name, parsers, value)
    @name = name
    @parsers = parsers
    @value = value
  end

  # Create a new builder with the given argument name and nothing else.
  def self.name(
    name # String
  )
    unless name.is_a?(String) && !name.empty?
      raise ArgumentError, 'name must be a non-empty string'
    end

    Arg.new(name.strip, [], nil)
  end

  # Add a parser and return a new `Arg` that inherits its other values, but has
  # another parser added to its list.
  def parser(
    &block # (value: typeof prior parser) => any
  )
    Arg.new(
      @name,
      @parsers + [block],
      @value,
    )
  end

  # Ensures that the passed value is an instance of at least one of the given
  # classes using `#is_a?`.
  def is_a(first_class, *other_classes)
    classes = [first_class, *other_classes].freeze
    self.parser do |value|
      value_is_a = classes.any? do |potential_class|
        value.is_a?(potential_class)
      end

      unless value_is_a
        class_names = classes.map(&:name).join(', ')

        # Don't add "one of" if there's only a single class to check.
        one_of_text =
          if classes.size < 2
            ''
          else
            'one of '
          end

        raise ArgumentError, "{{ name }} must be an instance of #{one_of_text}#{class_names}, but was an instance of #{value.class.name}"
      end

      value
    end
  end

  # Ensures the passed value is one of the given values.
  def one_of(potential_value, *other_values)
    potential_values = [potential_value, *other_values].freeze
    self.parser do |value|
      unless potential_values.include?(value)
        values_text = potential_values.map(&:to_s).join(', ')

        # Don't add "one of" if there's only a single value to check.
        one_of_text =
          if potential_values.empty?
            ' '
          else
            'one of '
          end

        raise ArgumentError, "{{ name }} was expected to be #{one_of_text}#{values_text}, but was #{value}"
      end

      value
    end
  end

  def non_empty
    self.parser do |value|
      if value.empty?
        raise ArgumentError, "{{ name }} must not be empty"
      end

      value
    end
  end

  # Validates that a value is non-`nil` and returns that value.
  def non_nil
    self.parser do |value|
      if value.nil?
        raise ArgumentError, "{{ name }} is required to be non-`nil`"
      end

      value
    end
  end

  def strip
    self.is_a(String).parser do |value|
      value.strip
    end
  end

  def substitute(
    regexp, # Regexp
    substitution # String
  )
    self.is_a(String).parser do |value|
      value.gsub(regexp, substitution)
    end
  end

  def downcase
    self.is_a(String).parser do |value|
      value.downcase
    end
  end

  def upcase
    self.is_a(String).parser do |value|
      value.upcase
    end
  end

  def symbolize
    self.parser do |value|
      value.to_sym
    end
  end

  def int
    self.is_a(String).parser do |value|
      begin
        Integer(value, 10)
      rescue ArgumentError => err
        raise ArgumentError, "{{ name }} of #{value.inspect} is not a valid integer: #{err}"
      end

      value
    end
  end

  def time
    self.is_a(Integer).parser do |value|
      Time.at(value)
    end
  end

  def min(
    n # Integer
  )
    self.parser do |value|
      if value < n
        raise ArgumentError, "{{ name }} of #{value.inspect} must not be less than #{n}"
      end
    end
  end

  def max(
    n # Integer
  )
    self.parser do |value|
      if value > max
        raise ArgumentError, "{{ name }} of #{value.inspect} must not be greater than #{n}"
      end
    end
  end

  def path_name
    self.parser do |value|
      Pathname
        .new(value)
        .cleanpath
        .expand_path
    end
  end

  def exists
    self.parser.path_name do |value|
      unless value.exist?
        raise ArgumentError, "{{ name }} of #{value} does not exist"
      end
    end
  end

  # Opens an IO stream from the given path name.
  def stream(
    mode = 'r' # https://ruby-doc.org/core-2.7.2/IO.html#method-c-new-label-IO+Open+Mode
  )
    self.path_name.parser do |path|
      fd = path.sysopen(mode)
      IO.open(fd, mode)
    end
  end

  def uri
    self.parser do |value|
      URI.parse(value)
    rescue URI::InvalidURIError => err
      raise ArgumentError, "{{ name }} of #{value.inspect} is not a valid URI: #{err}"
    end
  end

  # Thread our value through each configured parser in turn, passing the result
  # of each parser/validator through to the next. Returns the output of the
  # final parser given and sets `.value` to the same.
  #
  # Converts all raised errors into `ArgumentError` instances and re-raises
  # them.
  def parse(
    value # any
  )
    begin
      @value = @parsers.inject(value) do |current_value, parser|
        parser.call(current_value)
      end
    rescue StandardError => err
      new_err_class = ArgumentError

      # Replace any occurence of `{{ name }}` with our configured name to make
      # the errors prettier.
      message = err.message.gsub(/\{\{\s*name\s*\}\}/i, @name)

      # Preserve the original error class name if it's not the one we're
      # "casting" to.
      if err.class.name != new_err_class.name
        message = "#{err.class.name}: #{message}"
      end

      raise new_err_class, message
    end
  end
end

# A provider, something that has a "lineup" associated with it. Typically this
# is something like "Local over-the-air stations" or "AT&T Uverse Cable" or
# something of this nature.
#
# These are only unique given a combination of country, postal code, and
# "headend" id!
class Provider
  attr_reader \
    :country,
    :id,
    :postal_code,
    :name,
    :type

  def initialize(
    country: nil, # Symbol
    id: nil, # String, the `headendId` value from the API
    name: nil, # String
    postal_code: nil, # String
    type: nil # String
  )
    @id = Arg
      .name('id')
      .non_nil
      .is_a(String)
      .non_empty
      .parse(id)

    @name = Arg
      .name('name')
      .non_nil
      .is_a(String)
      .non_empty
      .parse(name)

    @postal_code = Arg
      .name('postal_code')
      .non_nil
      .non_empty
      .is_a(String)
      .parse(postal_code)

    @type = Arg
      .name('type')
      .non_nil
      .is_a(String)
      .non_empty
      .downcase
      .symbolize
      .parse(type)

    @country = Arg
      .name('country')
      .one_of(:USA, :CAN)
      .parse(country)
  end

  def to_s
    name_s =
      if name.nil?
        ''
      else
        "#{name}, "
      end

    "#{name_s}#{country} #{postal_code} (#{id})"
  end
end

# A channel in a lineup.
class Channel
  attr_reader \
    :provider,
    :call_sign,
    :name,
    :id,
    :number,
    :image_url

  def initialize(
    provider: nil, # Provider
    call_sign: nil, # String
    name: nil, # String | nil
    id: nil, # String
    number: nil, # String
    image_url: nil # String
  )
    @provider = Arg
      .name('provider')
      .is_a(Provider)
      .parse(provider)

    @call_sign = Arg
      .name('call_sign')
      .non_nil
      .is_a(String)
      .strip
      .non_empty
      .parse(call_sign)

    # Name is optional.
    @name =
      unless name.nil?
        Arg
          .name('name')
          .is_a(String)
          .strip
          .non_empty
          .parse(name)
      end

    @id = Arg
      .name('id')
      .non_nil
      .is_a(String)
      .strip
      .non_empty
      .parse(id)

    @number = Arg
      .name('number')
      .non_nil
      .is_a(String)
      .strip
      .non_empty
      .parse(number)

    @image_url = Arg
      .name('image_url')
      .non_nil
      .is_a(String)
      .strip
      .uri
      .parser do |uri|
        # The URL comes in "naked" and needs to be secure-ified.
        uri.scheme = 'https'

        # The URL _also_ comes in with `w=123`, but if we strip this off then
        # we'll get back the original, full-size picture instead.
        uri.query = nil
      end
      .parse(image_url)
  end

  # An RFC-2838 id for this channel.
  def xml_id
    # See https://tools.ietf.org/html/rfc2838 for more information about this id
    # format. Since we can't really determine the actual "owner" of the network,
    # we attribute it to zap2it via their internal id instead.
    "tv:#{id}.zap2it.com"
  end

  def to_xml
    raise 'TODO: Implement Channel#to_xml'
  end

  def to_s
    "#{number} #{call_sign} (#{id})"
  end
end

# One program/timeslot combination in a channel's lineup.
class Program
  attr_reader \
    :channel,
    :series_id,
    :program_id,
    :start_time,
    :end_time,
    :season,
    :episode,
    :title,
    :secondary_title,
    :description,
    :image_url,
    :previously_shown_time,
    :rating,
    :tags,
    :genres,
    :original_release_date,
    :is_movie

  def initialize(
    channel: nil, # Channel
    series_id: nil, # String
    program_id: nil, # String
    start_time: nil, # Time
    end_time: nil, # Time
    season: nil, # Integer | nil
    episode: nil, # Integer | nil
    title: nil, # String
    secondary_title: nil, # String | nil
    description: nil, # String | nil
    image_url: nil, # String
    previously_shown_time: nil, # String | nil
    rating: nil, # String | nil
    tags: nil, # Array<String>
    genres: nil, # Array<String>
    original_release_date: nil, # Time | nil
    is_movie: nil, # Boolean
    is_new: nil, # Boolean
    is_live: nil, # Boolean
    is_premier: nil, # Boolean
    is_finale: nil, # Boolean
    is_generic: nil # Boolean
  )
    @channel = Arg
      .name('channel')
      .is_a(Channel)
      .parse(channel)

    @series_id = Arg
      .name('series_id')
      .is_a(String)
      .strip
      .non_empty
      .parse(series_id)

    @program_id = Arg
      .name('program_id')
      .is_a(String)
      .strip
      .non_empty
      .parse(program_id)

    @start_time = Arg
      .name('start_time')
      .is_a(Time)
      .parse(start_time)

    @end_time = Arg
      .name('end_time')
      .is_a(Time)
      .parser do |t|
        unless t > @start_time
          raise ArgumentError, '{{ name }} must be later than start_time'
        end
        t
      end
      .parse(end_time)

    unless season.nil?
      @season = Arg
        .name('season')
        .is_a(Integer)
        .min(1)
        .parse(season)
    end

    unless episode.nil?
      @episode = Arg
        .name('episode')
        .is_a(Integer)
        .min(0) # We allow zero since this can indicate "special" episodes.
        .parse(episode)
    end

    @title = Arg
      .name('title')
      .is_a(String)
      .strip
      .non_empty
      .parse(title)

    @secondary_title =
      unless secondary_title.nil?
        Arg
          .name('secondary_title')
          .is_a(String)
          .strip
          .non_empty
          .parse(secondary_title)
      end

    unless @description.nil?
      @description = Arg
        .name('description')
        .is_a(String)
        .strip
        .non_empty
        .parse(description)
    end

    @image_url = Arg
      .name('image_url')
      .non_nil
      .is_a(String)
      .strip
      .uri
      .parse(image_url)

    @previously_shown_time =
      unless previously_shown_time.nil?
        Arg
          .name('previously_shown_time')
          .is_a(Time)
          .parse(previously_shown_time)
      end

    @rating =
      unless rating.nil?
        Arg
          .name('rating')
          .is_a(String)
          .strip
          .non_empty
          .parse(rating)
      end

    unless original_release_date.nil?
      @original_release_date = Arg
        .name('original_release_date')
        .is_a(Time)
        .parse(original_release_date)
    end

    @is_movie = Arg
      .name('is_movie')
      .one_of(true, false)
      .parse(is_movie)

    @is_new = Arg
      .name('is_new')
      .one_of(true, false)
      .parse(is_new)

    @is_live = Arg
      .name('is_live')
      .one_of(true, false)
      .parse(is_live)

    @is_premier = Arg
      .name('is_premier')
      .one_of(true, false)
      .parse(is_premier)

    @is_finale = Arg
      .name('is_finale')
      .one_of(true, false)
      .parse(is_finale)

    @is_generic = Arg
      .name('is_generic')
      .one_of(true, false)
      .parse(is_generic)

    # TODO: Validate these more thoroughly!
    @tags = Arg
      .name('tags')
      .is_a(Array)
      .parse(tags)

    # TODO: Validate these more thoroughly!
    @genres = Arg
      .name('genres')
      .is_a(Array)
      .parse(genres)
  end

  def movie?; @is_movie; end
  def new?; @is_new; end
  def live?; @is_live; end
  def premier?; @is_premier; end
  def finale?; @is_finale; end
  def generic?; @is_generic; end

  def to_xml
    raise 'TODO: Implement Program#to_xml'
  end
end

# A config-specific struct that allows for easy access to arbitrary values while
# also providing some helper methods that we use elsewhere.
class Config < OpenStruct
  # Require that the given configuration value be present in the parsed
  # configuration, then return it if present.
  def require(
    key, # Symbol
    name = nil # String, defaults to being auto-generated from `key`
  ) # typeof self[key]
    # "Humanize" the config key if an explicit name wasn't given.
    if name.nil?
      name = key
        .to_s
        .gsub(/_+/, ' ')
        .strip
    end

    value = self[key]

    if value.nil?
      article_text =
        if /^[aeiou]/.match?(name)
          "An"
        else
          "A"
        end
      raise ArgumentError, "#{article_text} #{name} is required but was not specified; see --help for details."
    end

    value
  end
end

# A simple cache, either in-memory or backed by a file. All cached values are
# converted to symbolized JSON hashes/arrays.
class Cache
  attr_reader \
    :path

  # Reads existing values from the given path, if any, otherwise uses a
  # memory-only backing store.
  def initialize(
    path = nil # Pathname | nil
  )
    @path = path
    @is_dirty = false

    @cache = {}

    # Initialize the cache from the backing store, if given.
    if !path.nil? && path.exist?
      begin
        @cache = path.open('r') do |f|
          # We use `JSON#parse` instead of `JSON#load` so we don't have to trust
          # the input file we were given, which the documentation suggests we must
          # do if using `JSON#load`.
          JSON.parse(f.read, symbolize_names: true)
        end
      rescue StandardError => err
        raise IOError, "Unable to parse cache at #{path}: #{err}"
      end

      unless @cache.is_a?(Hash)
        raise IOError, "#{path} contains invalid cache file contents"
      end
    end
  end

  # Returns the number of entries in the cache.
  #
  # Note that this count includes expired entries that haven't yet been pruned!
  def size
    @cache.size
  end

  # Returns whether the cache has been modified since being loaded.
  def dirty?
    @is_dirty
  end

  # Write the cache to the originally-configured path, if any. If no path was
  # originally configured, does nothing.
  def persist! # self
    # If we were given no path, we can't persist the cache.
    return if @path.nil?

    # If we're not dirty, i.e. if the cache hasn't been modified, do nothing to
    # save loads of time. While this will leave "dirty" entries in the cache,
    # it's still better than having to re-persist the possibly very large cache
    # if nothing has actually changed!
    return unless dirty?

    # Remove expired values from the cache prior to dumping it so we won't waste
    # time and space storing and serializing expired entries.
    prune!

    @path.open('w') do |f|
      # Since we control this, we know it's non-circular.
      json_text = JSON.fast_generate(@cache, object_nl: "\n")
      f.write(json_text)
    end

    # Once the cache has been persisted, it's no longer considered "dirty" since
    # it hasn't changed since the last write to disk.
    @is_dirty = false

    self
  end

  # Prunes all currently-expired entries from the cache.
  def prune! # self
    now = Time.now

    @cache.each_pair do |key, entry|
      if entry_expired?(entry, now: now)
        # Only mark the cache as dirty if something is actually pruned from it.
        @is_dirty = true
        @cache.delete(key)
      end
    end

    self
  end

  # Retrieves the given key from the cache if present, returning `nil` if not
  # present.
  def get(
    cache_key, # String | Symbol
    now: Time.now # Time
  )
    cache_key = cache_key.to_sym

    # If the entry has expired, delete it from the cache entirely and return
    # `nil`.
    entry = @cache[cache_key]
    if entry.nil?
      nil
    elsif entry_expired?(entry, now: now)
      # If we're expiring an entry, we've modified the cache and it should be
      # re-persisted.
      @is_dirty = true
      @cache.delete(cache_key)
      nil
    else
      entry.fetch(:value)
    end
  end

  # Adds the given key/value to the cache, overwriting any existing one. Returns
  # the value.
  def set(
    cache_key, # String | Symbol,
    value, # JSON
    ttl_seconds, # Integer
    now: Time.now # Time
  ) # typeof value
    # Since something is being updated, mark the cache as "dirty" so we'll know
    # to actually persist it if asked.
    @is_dirty = true

    Arg
      .name('ttl_seconds')
      .is_a(Integer)
      .min(1)
      .parse(ttl_seconds)
    cache_key = cache_key.to_sym

    @cache[cache_key] = {
      created_at: now.to_i,
      ttl_seconds: ttl_seconds,
      value: value
    }

    value
  end

  # Load a JSON value from the cache. If no value is found, the given block is
  # run and the resulting value is stored in the cache under the given key
  # before being returned.
  def fetch(
    cache_key, # String | Symbol
    ttl_seconds, # Integer
    &block # () => JSON
  ) # the cached value
    Arg
      .name('ttl_seconds')
      .is_a(Integer)
      .min(1)
      .parse(ttl_seconds)

    # Return the value directly if it's already cached.
    now = Time.now
    value = get(cache_key, now: now)
    return value unless value.nil?

    # If not already cached, run the block and set the cache to the new value,
    # then return the new value.
    set(cache_key, yield, ttl_seconds, now: now)
  end

  private

  def entry_expired?(entry, now: Time.now)
    now > Time.at(entry.fetch(:created_at) + entry.fetch(:ttl_seconds))
  end
end

# Returns a new `Time` truncated to the given unit.
def truncate_time(
  time, # Time
  unit # Integer seconds, e.g. 3600 for an hour or 86400 for a day
) # Time
  time - (time.to_i % unit)
end

# Parse the configuration from the command line options.
def parse_config_from_args(
  args # An arguments array, typically `ARGV`.
)
  config = Config.new({
    # The `OptionParser` instance that parsed this config. This is useful so
    # whoever gets the config can inspect e.g. all the parsed arguments that
    # created it.
    option_parser: nil, # OptionParser | nil

    # These `return_*?` options indicate that the program should run in the
    # requisite "mode" and do something, then exit.

    # Generic hacks for better `--help` output
    return_help_text?: false,
    return_version_info?: false,

    # XMLTV-specific
    return_capabilities?: false,
    return_description?: false,

    # Program-specific
    return_providers?: false,
    return_channels?: false,
    return_lineup?: false,

    # XMLTV-specific configuration, mostly optional.
    cache: Cache.new, # Cache, in-memory by default
    day_offset: 0, # Integer
    days_to_fetch: 999_999_999, # Integer
    output_file: $stdout, # File-like
    quiet?: false,

    # Program-specific configuration, mostly required.
    channel_tokens: Set.new, # Set<String>
    country: nil, # :USA | :CAN
    postal_code: nil, # String
    provider_id: nil, # String
  })

  option_parser = OptionParser.new do |opts|
    opts.program_name = PROGRAM_NAME
    opts.version = "v#{VERSION}" # Pre-pending `v` makes it look nicer
    opts.release = RELEASE

    ############################################################################
    # GENERIC OPTIONS
    ############################################################################

    # Manually specifying these ensures they show up in `--help` output instead
    # of simply being silently available without obvious provenance.

    opts.on(
      '-h',
      '--help',
      'Output program help text and exit.'
    ) do
      config[:return_help_text?] = true
    end

    opts.on(
      '-v',
      '--version',
      'Output program version information and exit.'
    ) do
      config[:return_version_info?] = true
    end

    ############################################################################
    # XMLTV OPTIONS
    ############################################################################

    #
    # Minimum required options
    #

    opts.on(
      '--description',
      'Output our XMLTV description and exit.'
    ) do
      config[:return_description?] = true
    end

    opts.on(
      '--capabilities',
      'Output our supported XMLTV capabilities and exit.'
    ) do
      config[:return_capabilities?] = true
    end

    #
    # `baseline` options
    #

    opts.on(
      '-q',
      '--quiet',
      'Suppress non-error output.'
    ) do
      config[:quiet?] = true
    end

    output_flag = '--output'
    opts.on(
      '-o PATH',
      "#{output_flag} PATH",
      'Write output to this file instead of standard output.'
    ) do |p|
      config.output_file = Arg
        .name(output_flag)
        .path_name
        .stream('w')
        .parse(p)
    end

    days_flag = '--days'
    opts.on(
      '-d DAYS',
      "#{days_flag} DAYS",
      'The number of days of data to obtain, default "as many as are available"'
    ) do |d|
      config.days_to_fetch = Arg
        .name(days_flag)
        .int
        .min(1)
        .parse(d)
    end

    offset_flag = '--offset'
    opts.on(
      '-f DAYS',
      "#{offset_flag} DAYS",
      'Obtain data starting on the date this many days from today. Default is 0, i.e. "today", 1 is "tomorrow", etc.'
    ) do |o|
      config.day_offset = Arg
        .name(offset_flag)
        .int
        .parse(o)
    end

    config_file_flag = '--config-file'
    opts.on(
      "#{config_file_flag} PATH",
      'The path to the (optional) configuration file for this grabber.'
    ) do |p|
      config_path = Arg
        .name(config_file_flag)
        .path_name
        .exists
        .parse(p)

      # Reads command line options from the given file and parses them.
      #
      # We do this "in line" so that options are applied "logically", i.e.
      # options that come _before_ this flag are overwritten, but options that
      # come _after_ this flag overwrite previously-loaded options.
      opts.load(config_path)
    end

    #
    # `cache` options
    #

    cache_flag = '--cache'
    opts.on(
      "#{cache_flag} PATH",
      'If given, a file in which to cache network requests across invocations.'
    ) do |p|
      cache_filename = Arg
        .name(cache_flag)
        .path_name
        .parse(p)

      config.cache = Cache.new(cache_filename)
    end

    ############################################################################
    # PROGRAM OPTIONS
    ############################################################################

    country_flag = '--country'
    opts.on(
      '-c COUNTRY',
      "#{country_flag} COUNTRY",
      'The country for which to fetch data, one of USA or CAN.'
    ) do |s|
      config.country = Arg
        .name(country_flag)
        .strip
        .one_of('USA', 'CAN')
        .symbolize
        .parse(s)
    end

    postal_code_flag = '--postal-code'
    opts.on(
      '-p CODE',
      "#{postal_code_flag} CODE",
      'The postal code for which to fetch data. Something like `12345` for the USA or `A1A 1A1` for Canada.`'
    ) do |s|
      config.postal_code = Arg
        .name(postal_code_flag)
        .strip
        .substitute(/\s+/, ' ') # Condense spaces for uniformity
        .non_empty
        .parse(s)
    end

    show_provider_info_flag = '--show-providers'
    opts.on(
      show_provider_info_flag,
      "Output provider information for the given #{country_flag} and #{postal_code_flag} values."
    ) do |s|
      config[:return_providers?] = true
    end

    provider_id_flag = '--provider-id'
    opts.on(
      '-r ID',
      "#{provider_id_flag} ID",
      "The provider id for which to fetch data, obtained with the help of the #{show_provider_info_flag} flag."
    ) do |s|
      config.provider_id = Arg
        .name(provider_id_flag)
        .strip
        .non_empty
        .parse(s)
    end

    show_channels_flag = '--show-channels'
    opts.on(
      show_channels_flag,
      "Output the channels for the given #{country_flag}, #{postal_code_flag}, and #{provider_id_flag} values.",
    ) do
      config[:return_channels?] = true
    end

    channels_flag = '--channels'
    opts.on(
      "#{channels_flag} CHANNELS",
      "A comma-delimited list of channel call signs, numbers, and/or ids for which to download data. If non are provided, all available channels will be downloaded.",
    ) do |s|
      raw_channels = Arg
        .name(channels_flag)
        .strip
        .non_empty
        .parse(s)

      config.channel_tokens = raw_channels
        .split(/,/)
        .map(&:strip) # Handle interior whitespace
        .reject(&:empty?) # Remove empty strings
        .to_set
    end

    show_lineup_flag = '--show-lineup'
    opts.on(
      show_lineup_flag,
      "Output the program lineup(s) for the given #{channels_flag} values.",
    ) do
      config[:return_lineup?] = true
    end
  end

  # It's useful for external things to have access to this at times, e.g. for
  # printing help text.
  config.option_parser = option_parser

  # Parse the given arguments into our config object.
  option_parser.parse(args)

  config
end

# Makes a GET request to some host/path combination and returns the JSON
# response with symbolized keys. If the request fails or its response body can't
# be parsed, raises an `IOError`.
def fetch(
  *path, # Array<any>
  query: nil, # Hash<Symbol, any> | nil
  data: nil, # Hash<Symbol | String, JSON> | nil
  cache: Cache.new, # Cache
  ttl_seconds: nil # Integer
) # JSON
  full_path = File.join(*path.map(&:to_s))

  uri =
    begin
      u = URI.parse(full_path)

      unless query.nil?
        u.query = URI.encode_www_form(query.to_a)
      end

      u
    rescue URI::InvalidURIError => err
      raise ArgumentError, "Invalid URI from path #{full_path.inspect}: #{err}"
    end

  cache_key = digest(
    u, # The full request URL
    URI.encode_www_form(data || {}), # POST data for the request, if any
    format: :base64,
    length: 32
  )

  cache.fetch(cache_key, ttl_seconds) do
    # Don't allow things to take too long since we might be making a lot of
    # requests.
    timeout = 15 * TimeInterval::SECONDS
    res =
      Net::HTTP.start(
        uri.host,
        uri.port,
        use_ssl: u.scheme == 'https',
        open_timeout: timeout,
        read_timeout: timeout,
        write_timeout: timeout,
        ssl_timeout: timeout,
      ) do |http|
        method = data.nil? ? :GET : :POST
        body_data =
          if data.nil?
            nil
          else
            URI.encode_www_form(data)
          end
        http.send_request(
          method,
          uri.request_uri,
          body_data,
          { 'User-Agent' => VERSION_INFO }
        )
      end

    unless res.is_a?(Net::HTTPSuccess)
      raise IOError, "Got HTTP #{res.code} from #{uri}: #{res.message}"
    end

    # Parse the body as JSON, the only response format we expect to receive.
    begin
      # Guess what? The API returns improperly-escaped JSON strings!
      #
      # These seem to take the form of attempts to escape quotes within strings,
      # but accidentally escaping the backslash instead of the quote itself,
      # e.g. we've seen things like:
      #
      # ```json
      # {
      #    "prop": "Foo \\"bar\\" baz!"
      # }
      # ```
      #
      # These should _actually_ be:
      #
      # ```json
      # {
      #    "prop": "Foo \"bar\" baz!"
      # }
      # ```
      #
      # We correct the mistake before attempting to parse the body.
      corrected_body = res.body.gsub(/\\\\"/, '\\"')
      JSON.parse(corrected_body, symbolize_names: true)
    rescue JSON::ParserError => err
      raise IOError, "Failed to parse HTTP response from #{uri}: #{err}"
    end
  end
end

# Retrieves the Wikitext content from the given page info with revisions if it
# has any, otherwise returns `nil`.
def get_page_content(
  page_info # Wikipedia page info JSON response
) # String | nil
  page_content_container = page_info.dig(:revisions, 0, :slots, :main)
  return nil if page_content_container.nil?

  # We expect only Wikitext content and fail on anything else.
  return nil unless page_content_container.fetch(:contentformat) == 'text/x-wiki'

  page_content_container.fetch(:content)
end

# Parse all the links from the given page's content, if any, returning up to
# `limit` of them (default no limit).
def get_page_link_titles(
  page_info, # Wikipedia page info JSON response
  limit: 999_999_999
) # Array<String>, possibly empty
  # Parse the page content to find links, returning them if found.
  page_content = get_page_content(page_info)
  return [] if page_content.nil?

  # This doesn't perfectly match all links since it'll probably miss those with
  # `]` or `|` in their titles, but it's good enough for our purposes.
  page_content
    .scan(
      %r[
        # Link open token
        \[\[

        \s*

        # One or more chars that's not a pipe or a likely close token, consuming
        # as little as possible to obtain this (i.e. excluding trailing
        # whitespace). This is the title of the link, i.e. the page title to
        # which it refers.
        (
          [^\|\]]+?
        )

        # Optionally, a pipe followed by some more _optional_ text that's not a
        # close token. This is the "text" of the link, what's displayed to the
        # user if different from the title given. If a pipe with no following
        # text is given, this gets auto-looked up from the linked page,
        # apparently.
        (?:
          \s*

          \|

          \s*

          [^\]]*
        )?

        \s*

        # Link close token
        \]\]
      ]x
    )
    .lazy
    .map(&:first) # Obtain the first group, i.e. the page title
    .map(&:strip) # Ensure it has no extra whitespace
    .take(limit)
    .to_a
end

# Returns `true` when the given page info looks like it represents a television
# station with affiliation information, `false` otherwise.
def does_page_contain_tv_affiliation_info(
  page_info, # Wikipedia page info JSON response
  general_call_sign: nil # The page's call sign, ignored if not provided
) # Boolean
  # The page's title starts with our general call sign, if given.
  has_title_prefix =
    if general_call_sign.nil?
      true
    else
      page_info.fetch(:title).start_with?(general_call_sign)
    end

  # The page content looks like it probably contains the TV station affiliation
  # information template.
  page_content = get_page_content(page_info)
  has_tv_info = /\{\{\s*#{Regexp.escape(WIKIPEDIA_TV_STATION_INFOBOX_TEMPLATE_TITLE)}/.match?(page_content)

  has_title_prefix && has_tv_info
end

# Retrive the Wikipedia page information for a TV station and obtain the
# affiliate information from said page. If no affiliate information can be found
# for the given call sign prefix, returns `nil`.
def fetch_station_affiliate_info(
  country, # :USA | :CAN
  call_sign, # String, something like `KABCDT1`
  cache: Cache.new # Cache
) # Hash<String channel number, String affiliate name> | nil
  # TODO: Support Canada as well!
  return nil unless country == :USA

  # There's no need to update the page information frequently since it shouldn't
  # change very often at all, or at least not in a way that materially affects
  # us.
  ttl_seconds = 30 * TimeInterval::DAYS

  # First, do a general search for pages prefixed with the "general" call sign,
  # i.e. the first four characters of the call sign we were given. This turns
  # something like `KVUEDT` into `KVUE`, which is the title a Wikipedia entry
  # for a TV station call sign almost universally starts with.
  #
  # If we find a page that contains a TV station affiliate information template
  # and has a title that starts with our general call sign, this almost
  # certainly the page we're looking for!
  #
  # For an example query request, see
  # https://en.wikipedia.org/w/api.php?action=query&format=json&prop=revisions&generator=allpages&formatversion=2&rvprop=content&rvslots=main&gapprefix=KADT&gaplimit=10
  general_call_sign = call_sign[0...4]
  general_query_response = fetch(
    WIKIPEDIA_API_HOST,

    query: {
      # Get a JSON response in the latest stable version.
      format: :json,
      formatversion: 2,

      # We're looking for something!
      action: :query,

      # Get general and template page information. A "template" is essentially a
      # standardized way of displaying information on a Wikipedia page.
      prop: 'revisions',
      generator: :allpages,

      # Include the latest revision so we can parse its text to obtain the data
      # we're looking for.
      rvprop: :content,
      rvslots: :main,

      # Look for pages that start with our general call sign. We limit the
      # number of results since if nothing shows up in the first handful, it's
      # quite unlikely that we're going to find anything relevant anyway.
      gapprefix: general_call_sign,
      gaplimit: 10,
    },

    cache: cache,
    ttl_seconds: ttl_seconds,
  )

  # Pull the page info values from the query response.
  general_query_page_info_responses = general_query_response.dig(:query, :pages)

  # If we got no responses at all, we're out of luck.
  if general_query_page_info_responses.nil? || general_query_page_info_responses.empty?
    return nil
  end

  # First, look for _any_ response that happens to fit exactly what we were
  # looking for.
  page_info = general_query_page_info_responses.find do |general_page_info|
    does_page_contain_tv_affiliation_info(
      general_page_info,
      general_call_sign: general_call_sign,
    )
  end

  # If we didn't find any page info directly, generate and fetch a list of page
  # titles at which to look for TV affiliate information. These can come from
  # redirects and/or disambiguation pages.
  #
  # For an example query that contains both of these kinds of results, see
  # https://en.wikipedia.org/w/api.php?action=query&format=json&prop=revisions&generator=allpages&formatversion=2&rvprop=content&rvslots=main&gapprefix=KADT&gaplimit=10
  if page_info.nil?
    # Match strings that look like they might be TV station call sign page
    # titles.
    starts_with_call_sign_regex = /^[KW][A-Z0-9]{3}/

    # Generate an array of links that look like they might contain the
    # information we seek.
    promising_page_titles = []

    # First, add call-sign-like redirect pages since these are the most likely
    # source of data.
    general_query_page_info_responses.each do |general_page_info|
      # Is the page a redirect? Add it if the redirect link looks like a call
      # sign page.
      content = get_page_content(general_page_info)
      if /#REDIRECT/.match(content)
        # The first link in a redirect page is the page to which it redirects.
        redirect_page_title = get_page_link_titles(general_page_info, limit: 1)
          .first

        if starts_with_call_sign_regex.match?(redirect_page_title)
          promising_page_titles << redirect_page_title
        end
      end
    end

    # Next, add links that look like they point at call sign page titles.
    #
    # This "pattern" is indicative of disambiguation pages that link off to
    # other TV stations, e.g. because the call sign overlaps with some other
    # thing that uses the same four general call sign letters.
    #
    # Luckily for us, it seems that most (all?) television station pages have
    # titles that start with their general call signs!
    general_query_page_info_responses.each do |general_page_info|
      page_link_titles = get_page_link_titles(general_page_info)
      page_link_titles.each do |title|
        if starts_with_call_sign_regex.match?(title)
          promising_page_titles << title
        end
      end
    end

    # Remove any duplicate page titles so we don't waste our time re-fetching
    # and re-parsing duplicate responses.
    promising_page_titles.uniq!

    # Try each page title one-by-one until (if...) we find a page that matches
    # our requirements.
    #
    # For an example request for a particular page, see
    # https://en.wikipedia.org/w/api.php?action=query&format=json&prop=revisions&generator=allpages&formatversion=2&rvprop=content&rvslots=main&gapfrom=KADT-LD&gapto=KADT-LD&gaplimit=1
    promising_page_titles.each do |page_title|
      potential_page_info_response = fetch(
        WIKIPEDIA_API_HOST,

        query: {
          format: :json,
          formatversion: 2,

          action: :query,

          prop: 'revisions',
          generator: :allpages,

          rvprop: :content,
          rvslots: :main,

          # Look for one page with the exact title we're looking for.
          gapfrom: page_title,
          gapto: page_title,
          gaplimit: 1,
        },

        cache: cache,
        ttl_seconds: ttl_seconds,
      )

      # If we got no response at all, we're out of luck.
      potential_page_info = potential_page_info_response.dig(:query, :pages, 0)
      next if potential_page_info.nil?

      # If it's not a TV station page, keep looking.
      next unless does_page_contain_tv_affiliation_info(potential_page_info)

      # Otherwise, we found our page!
      page_info = potential_page_info
      break
    end
  end

  # If we _still_ didn't find what we're looking for, there's no hope left.
  return nil if page_info.nil?

  # Grab the Wikitext markup from the first (and only) revision that should have
  # been returned; we'll parse this to obtain our data.
  page_wikitext = get_page_content(page_info)
  return nil if page_wikitext.nil?

  # Parse the page and its required TV station infobox template. If we can't do
  # either of these, we're not going to be able to extract any useful
  # information from the page.
  wikitext_tokens = tokenize_wikitext(page_wikitext)
  infobox_attributes_to_tokens = parse_station_infobox(wikitext_tokens)
  return nil if infobox_attributes_to_tokens.nil?

  parse_station_affiliations(infobox_attributes_to_tokens)
end

# Tokenizes some text, assumed to be Wikitext, and returns a lazy enumerable
# over its tokens.
def tokenize_wikitext(
  s # String
) # Array<String>
  s
    .split(WIKITEXT_TOKENIZER)
    .lazy
    .map(&:strip)
    .reject(&:empty?)
    .to_a
end

# Extracts the first television station infobox template from the given token
# stream and returns it, or returns `nil` if no such template could be found.
def parse_station_infobox(
  wikitext_tokens # Array<String>
) # Hash<Symbol parameter name, Array<String Wikitext tokens>> | nil
  # We're looking for text much like the following:
  #
  # ```wikitext
  # {{Infobox television station
  # | foo = ...
  # ...
  # | bar = ...
  # }}
  #
  # To obtain it, we search through the token stream to find its first
  # occurrence, then parse each attribute's tokens into a hash until the
  # template ends.
  # ```

  infobox_hash = {}

  template_start_pattern = [
    WIKITEXT_TOKEN_TEMPLATE_START,
    WIKIPEDIA_TV_STATION_INFOBOX_TEMPLATE_TITLE,
  ].freeze
  found_infobox_template = false

  # Whether we're parsing an argument of the template, i.e. something like:
  # ```wikitext
  # | foo = [[bar]]
  # ```
  current_argument_name = nil

  seen_tokens = []
  depth = 0
  wikitext_tokens.each do |token|
    seen_tokens << token

    # Look for the beginning of our station info template.
    if (
        !found_infobox_template &&
        seen_tokens[-template_start_pattern.length..] == template_start_pattern
    )
      # Throw everything else away since at this point we only care about
      # looking for the coming argument token patterns.
      seen_tokens.clear
      depth = 0
      found_infobox_template = true
      next
    end

    # Find the affiliations section once we're inside the station info template.
    if (
        found_infobox_template &&
        current_argument_name.nil? &&
        depth.zero? &&

        # Check whether the token stream looks like an argument definition,
        # something like:
        # ```wikitext
        # | foo =
        # ```
        seen_tokens[-3] == WIKITEXT_TOKEN_PIPE &&
        seen_tokens[-1] == WIKITEXT_TOKEN_EQUALS
    )
      current_argument_name = seen_tokens[-2]
      seen_tokens.clear
      next
    end

    # Once we've returned to zero depth and found the end of the argument
    # definition (either the end of the parent template or we've encountered the
    # next template attribute), all the tokens we've seen make up the contents
    # of the argument and we can continue to the next argument.
    if (
        found_infobox_template &&
        !current_argument_name.nil? &&
        depth.zero? &&
        [WIKITEXT_TOKEN_PIPE, WIKITEXT_TOKEN_TEMPLATE_END].include?(token)
    )
      # Remove the trailing token since this may be necessary to start parsing
      # the next argument by matching the expected `| foo =` token sequence.
      trailing_token = seen_tokens.pop

      # Stash this argument's tokens in the resulting hash.
      infobox_hash[current_argument_name.to_sym] = seen_tokens.dup

      # If we encounter a closing template token to finish parsing this
      # argument, we've finished parsing the entire template!
      break if trailing_token == WIKITEXT_TOKEN_TEMPLATE_END

      # Reset state so we can parse the next argument, including re-adding the
      # trailing token we saved earlier so we can match the next argument.
      seen_tokens.clear
      seen_tokens << trailing_token
      current_argument_name = nil
      next
    end

    # Generally, track the current parse depth so we'll know when to stop
    # parsing the current argument attribute and the overall template.
    case token
      when WIKITEXT_TOKEN_TEMPLATE_START
        depth += 1
      when WIKITEXT_TOKEN_CLASS_START
        depth += 1
      when WIKITEXT_TOKEN_LINK_START
        depth += 1

      when WIKITEXT_TOKEN_TEMPLATE_END
        depth -= 1
      when WIKITEXT_TOKEN_CLASS_END
        depth += 1
      when WIKITEXT_TOKEN_LINK_END
        depth -= 1
    end
  end

  # If we never found the template we were looking for, signal as much by
  # returning `nil`.
  return nil unless found_infobox_template

  infobox_hash
end

# Retrieve the channel numbers, digital and/or virtual, that a given station
# broadcasts under.
def parse_channel_numbers(
  infobox_attributes_to_tokens # Hash<Symbol parameter name, Array<String Wikitext tokens>>
) # Array<Integer channel numbers>
  # The first integer in a channel info attribute should be the channel number;
  # the attribute token text should look something like:
  # ```wikitext
  # 33 ([[ultra high frequency|UHF]])`
  # ```
  infobox_attributes_to_tokens
    .fetch_values(:digital, :virtual) { [] }
    .map(&:join)
    .inject([]) do |all_channel_numbers, channel_number_wikitext|
      match = /(?<channel_number>\d+)/.match(channel_number_wikitext)
      next all_channel_numbers if match.nil?

      all_channel_numbers << match[:channel_number].to_i
      all_channel_numbers
    end
end

# Given some Wikitext markup, attempts to parse out TV station affiliations from
# the corresponding tokens, returning `nil` if unable to do so.
def parse_station_affiliations(
  infobox_attributes_to_tokens # Hash<Symbol parameter name, Array<String Wikitext tokens>>
) # Hash<String station number, String station name> | nil
  # Either of these attributes can contain the information we seek, namely a
  # list of channel numbers to affiliate names. We prefer the attribute that
  # seems to most commonly contain the information, `affiliations`.
  affiliations_tokens = infobox_attributes_to_tokens.fetch(:affiliations)
  if affiliations_tokens.nil? || affiliations_tokens.empty?
    affiliations_tokens = infobox_attributes_to_tokens.fetch(:subchannels)
  end

  # If the infobox has no affiliations, we're out of luck.
  return nil if affiliations_tokens.nil? || affiliations_tokens.empty?

  # Affiliations infoboxes don't seem to have a "standard" channel number by
  # which they list their affiliations. E.g., a station may operate under
  # channels 21 and 43, but the affiliations will only be listed with channel
  # 21.
  #
  # Alternatively, an affiliations section may list only a single affiliate with
  # _no_ channel numbers at all!
  #
  # In these cases, we want to know what the actual channel number(s) for the
  # station are so we can map _all_ channels to their corresponding
  # affiliations, and/or do a "speculative" mapping if the affiliation(s) have
  # no channel numbers listed.
  channel_numbers = parse_channel_numbers(infobox_attributes_to_tokens)

  # We expect most affiliations to come in the form of an "unbulleted list"
  # (https://en.wikipedia.org/wiki/Template:Unbulleted_list). We parse out the
  # leading pipe so that from here on, every time we see a pipe we can take all
  # the tokens we've accumulated and turn them into a human-readable name and
  # channel number.
  ubl_template_start_pattern = [
    WIKITEXT_TOKEN_TEMPLATE_START,
    'ubl',
    WIKITEXT_TOKEN_PIPE,
  ].freeze

  br_tag_token = %r[^<\s*br\s*/?\s*>$]

  # We now attempt to detect and post-process the major affiliations attribute
  # variants we see.
  if affiliations_tokens[0...ubl_template_start_pattern.length] == ubl_template_start_pattern
    # This is the "classic" and most common by far variant where the
    # affiliations list contains a single `ubl` template of channel numbers
    # mapped to station names. In this case, we do nothing at all!
  elsif affiliations_tokens.find { |t| br_tag_token.match?(t) }
    # Sometimes, we get a "manually-constructed list" that was created with
    # `<br>` and/or `<br />` tags instead of a "fancy" list created with the
    # `ubl` template.
    #
    # When we see a manually-constructed list, we convert it into a `ubl`
    # template so we can parse the `ubl` format later.
    affiliations_tokens = [
      *ubl_template_start_pattern,

      # Replace all the `<br>` tags with pipes since the tags are effectively
      # acting as list item separators.
      *affiliations_tokens.map do |token|
        if br_tag_token.match?(token)
          WIKITEXT_TOKEN_PIPE
        else
          token
        end
      end,

      WIKITEXT_TOKEN_TEMPLATE_END,
    ]
  else
    # If it doesn't match either one of these, treat whatever's already there as
    # a single list item.

    # If it doesn't look like the affiliate text contains a channel number, give
    # it bunch of channel numbers corresponding to the channel numbers we parsed
    # earlier. If for some reason we didn't parse _any_ channel numbers, give it
    # a fake "catch all" channel number of `*`.
    #
    # E.g. we're going to turn something like `Network` into a bunch of pairs
    # like `'''21:''' Network`, `'''21.1:''' Network`, etc. assuming that we'll
    # catch the _actual_ channel number in there somewhere.

    # If we couldn't find even a single channel number for this channel, give up
    # entirely since the data we're looking at doesn't seem reliable anyway.
    return nil if channel_numbers.empty?

    # If it looks like the affiliations contain no channel numbers, re-map them
    # to a bunch of common channel numbers instead since presumably at least one
    # of the resulting entries is the actual channel number.
    remapped_affiliations_tokens =
      if affiliations_tokens.count(WIKITEXT_TOKEN_BOLD) < 2
        channel_numbers.flat_map do |channel_parent_number|
          # Include both the "parent" number as well as variants for a bunch of
          # "sub" numbers, e.g. if the channel number is 21 then return `["21",
          # "21.1", "21.2", ...]`
          (0..9).flat_map do |channel_sub_number|
            channel_number =
              if channel_sub_number.zero?
                channel_parent_number.to_s
              else
                "#{channel_parent_number}.#{channel_sub_number}"
              end

            [
              # Channel number
              WIKITEXT_TOKEN_BOLD,
              "#{channel_number}:",
              WIKITEXT_TOKEN_BOLD,

              # Name
              *affiliations_tokens,

              # Argument separator for the next channel number
              WIKITEXT_TOKEN_PIPE,
            ]
          end
        end[0...-1] # Omit the trailing argument separator
      else
        affiliations_tokens
      end

    affiliations_tokens = [
      *ubl_template_start_pattern,
      *remapped_affiliations_tokens,
      WIKITEXT_TOKEN_TEMPLATE_END,
    ]
  end

  if affiliations_tokens[0...ubl_template_start_pattern.length] != ubl_template_start_pattern
    return nil
  else
    # Strip off the template start tokens since we don't need them.
    ubl_template_start_pattern.length.times { affiliations_tokens.shift }
  end

  raw_affiliations = []

  found_link = false
  found_tag = false
  affiliation_tokens = []
  affiliations_tokens.each do |token|
    affiliation_tokens << token

    # Skip bold text markers entirely, effectively un-bolding all bolded text.
    if token == WIKITEXT_TOKEN_BOLD
      affiliation_tokens.pop
      next
    end

    # Skip all tags and their contents. Typically, these are `ref` tags and we
    # don't care about them.
    if !found_tag && WIKITEXT_TOKEN_START_TAG.match?(token)
      affiliation_tokens.pop
      found_tag = true
      next
    end

    # Discard all tokens until we find a matching end tag.
    if found_tag
      affiliation_tokens.pop

      if WIKITEXT_TOKEN_END_TAG.match?(token)
        found_tag = false
      end

      next
    end

    if !found_link && token == WIKITEXT_TOKEN_LINK_START
      found_link = true
      next
    end

    # "Unwrap" links to obtain only their text, if any. See
    # https://en.wikipedia.org/wiki/Help:Wikitext#Links_and_URLs for more
    # information on Wikitext link syntax.
    if found_link && token == WIKITEXT_TOKEN_LINK_END
      # Rewind the token stream to the start of the link, consuming all the
      # tokens for specific processing here.
      link_text_tokens = []
      until link_text_tokens.last == WIKITEXT_TOKEN_LINK_START || affiliation_tokens.empty? do
        link_text_tokens << affiliation_tokens.pop
      end

      # Reverse the tokens we just collected them in reverse order to obtain the
      # "natural" order for further processing.
      link_text_tokens.reverse!

      # Strip off the leading and trailing link markers. If we didn't find the
      # appropriate tokens there, explode since something is wrong.
      first_link_token = link_text_tokens.shift
      last_link_token = link_text_tokens.pop
      if first_link_token != WIKITEXT_TOKEN_LINK_START && last_link_token != WIKITEXT_TOKEN_LINK_END
        raise IOError, 'Encountered invalid Wikitext link'
      end

      # "Unwrap" the link to contain only its actual text.
      #
      # These are most of the potential Wikitext link formats:
      # ```wikitext
      # [[standard link]]
      # [[link|renamed link]]
      # [[auto-renamed link|]]
      # [[blended link]]ing
      # [[Wikipedia:Page link#To a section]]
      # [[it:Otra lingua]]
      # [[Wiktionary:intra-wiki link]]
      # ```
      #
      # In practice, we hope to only see standard and renamed links here, though
      # technically any of these is possible.
      if link_text_tokens.include?(WIKITEXT_TOKEN_PIPE)
        unpiped_tokens = []
        found_pipe = false
        link_text_tokens.each do |token|
          unpiped_tokens << token

          if !found_pipe && token == WIKITEXT_TOKEN_PIPE
            unpiped_tokens.clear
            found_pipe = true
          end
        end

        # If we found no tokens after the pipe, simply remove the pipe from the
        # original tokens and treat the remaining tokens as the text of the
        # link. This happens with auto-renamed links, which look like
        # `[[Foo|]]`; in this case, we use the standard link text before the
        # pipe.
        if unpiped_tokens.empty?
          link_text_tokens.reject! { |t| t == WIKITEXT_TOKEN_PIPE }
        else
          link_text_tokens = unpiped_tokens
        end
      end

      # Add the unwrapped link text tokens back to the original token stream.
      affiliation_tokens.concat(link_text_tokens)
      found_link = false
      next
    end

    # If we find a bare pipe token or the end of the `ubl` template, this
    # indicates we've completed an item and can output it for later processing.
    if !found_link && token == WIKITEXT_TOKEN_PIPE || token == WIKITEXT_TOKEN_TEMPLATE_END
      affiliation_tokens.pop
      raw_affiliations << affiliation_tokens.join
      affiliation_tokens.clear
    end
  end

  # We should have consumed _all_ the affiliation tokens since they should have
  # been only a single `ubl` template instance. If they weren't we have no idea
  # what the affiliations contained and are unlikely to be able to parse out
  # anything useful from our resulting text affiliations.
  return nil unless affiliation_tokens.empty?

  # If we found multiple channel numbers, we'll re-map those we _did_ find in
  # the affiliations to their associated "parent" numbers. By looking only for
  # channel numbers we know we have, we avoid "weirdness" of re-mapping
  # affilations with channel numbers we don't already know about.
  channel_numbers_matcher =
    if channel_numbers.length < 2
      nil
    else
      /^(?:#{channel_numbers.map { |n| Regexp.escape(n.to_s) }.join('|')})/
    end

  # Turn the text affiliations into channel numbers and channel names.
  raw_affiliations.inject({}) do |all, raw_affiliation|
    # We expect the text we have at this point to look like:
    # ```
    # 1.2: Channel name (maybe some text in parenthesis)
    # ```

    # Split into the "channel" and "name" parts we expect.
    channel, raw_name = raw_affiliation
      .split(':', 2)
      .map(&:strip)
      .reject(&:empty?)

    # This channel/name didn't conform to our expected format.
    next all if channel.nil? || raw_name.nil?

    # Strip all parenthesized text from the channel name since it's usually
    # something like `(O&O)` for "owned and operated", which is irrelevant for
    # our purposes.
    name = raw_name
      .gsub(/\([^)]+\)/, '')
      .strip

    # If the name was _only_ parenthesized, it'd be a very odd name indeed!
    next all if name.empty?


    if channel_numbers_matcher.nil?
      all[channel] = name
    else
      channel_numbers.each do |channel_number|
        remapped_channel_number = channel.gsub(
          channel_numbers_matcher,
          channel_number.to_s
        )
        all[remapped_channel_number] = name
      end
    end

    all
  end
end

# Downloads and returns lineup (i.e. provider) data for the given country and
# postal code.
def fetch_providers(
  country, # :USA | :CAN
  postal_code, # String, e.g. `12345` or `A1A 1A1`
  cache: Cache.new # Cache
) # Array<Provider>
  provider_json = fetch(
    ZAP2IT_API_HOST,
    '/gapzap_webapi/api/Providers/getPostalCodeProviders',
    country,
    postal_code,
    '/gapzap/en',

    # This response is expected to change very infrequently, if ever.
    cache: cache,
    ttl_seconds: TimeInterval::WEEKS,
  )

  provider_json
    .fetch(:Providers)
    .sort_by { |p| [p.fetch(:type), p.fetch(:name)] } # A human-friendly sort for display
    .map do |p|
      Provider.new(
        country: country,
        id: p.fetch(:headendId),
        name: p.fetch(:name),
        postal_code: postal_code,
        type: p.fetch(:type),
      )
    end
end

# Fetch overall provider info and return the provider that corresponds to the
# given parameters. If no provider is found, returns `nil`.
def fetch_provider(
  country, # :USA | :CAN
  postal_code, # String
  provider_id, # String
  cache: Cache.new # Cache
) # Provider | nil
  providers = fetch_providers(
    country,
    postal_code,
    cache: cache,
  )

  providers.find do |provider|
    (
      provider.country == country &&
      provider.postal_code == postal_code &&
      provider.id == provider_id
    )
  end
end

# Downloads and returns the channels available for the given provider at the
# given time/time span, potentially limiting them to some list of channels.
def fetch_channels(
  provider, # Provider
  date = Time.now, # Time, but will be truncated to its date component
  time_span_hours = 6, # 1 | 2 | 3 | 4 | 5 | 6
  channel_tokens: Set.new, # Enumerable<String>. If non-empty, channels to fetch
  cache: Cache.new # Cache
) # Array<Channel>
  # We need fast random access to these.
  channel_tokens = channel_tokens.to_set

  # When fetching channels, we really only care about "right now" and don't
  # expect the values to change much over time. However, we cache the response
  # for a while anyway.
  ttl_seconds = 1 * TimeInterval::DAYS

  lineup_json = fetch(
    ZAP2IT_API_HOST,
    '/api/grid',
    query: {
      # Unused since we give `headendId`, but apparently necessary anyway!
      lineupId: '-',

      # Provider info
      headendId: provider.id,
      country: provider.country,
      postalCode: provider.postal_code,

      # Time info, truncated to "today" for caching purposes.
      timespan: time_span_hours,
      time: truncate_time(date, ttl_seconds).to_i, # Needs to be Unix time, apparently
    },

    cache: cache,
    ttl_seconds: ttl_seconds,
  )

  # We sort by channel number for a human-friendly display and limit our
  # channels to the ones requested.
  raw_channels = lineup_json
    .fetch(:channels)
    .sort_by do |c|
      [
        (Float(c.fetch(:channelNo)) rescue c.fetch(:channelNo)),
        c.fetch(:callSign),
      ]
    end
    .select do |c|
      (
        # If we weren't given any channels to which to limit our fetch, keep all
        # channels.
        channel_tokens.empty? ||

        # Otherwise, keep only channels that have an identifier that matches one
        # of the given tokens.
        channel_tokens.include?(c.fetch(:channelId)) ||
        channel_tokens.include?(c.fetch(:callSign)) ||
        channel_tokens.include?(c.fetch(:channelNo))
      )
    end

  # If we got no channels after filtering things down, clearly tell the user
  # what happened.
  if raw_channels.empty?
    channel_tokens_string = channel_tokens
      .to_a
      .sort
      .join(', ')
    raise ArgumentError, "Could not find any channels matching these tokens: #{channel_tokens_string}"
  end

  raw_channels.map do |c|
    affiliates = fetch_station_affiliate_info(
      provider.country,
      c.fetch(:callSign),
      cache: cache,
    ) || {}

    channel_name = affiliates[c.fetch(:channelNo)]

    Channel.new(
      provider: provider,
      call_sign: c.fetch(:callSign),
      name: channel_name,
      id: c.fetch(:channelId),
      number: c.fetch(:channelNo),
      image_url: c.fetch(:thumbnail),
    )
  end
end

# Fetch overall channel info and return the channel that corresponds to the
# given token. If no channel is found, returns `nil`.
def fetch_channel(
  provider, # Provider
  token, # String, one of a channel call sign, number, or id
  date = Time.now, # Time, but will be truncated to its date component
  time_span_hours = 6, # 1 | 2 | 3 | 4 | 5 | 6
  cache: Cache.new # Cache
) # Channel | nil
  channels = fetch_channels(
    provider,
    date,
    time_span_hours,
    cache: cache
  )

  channels.find do |channel|
    (
      channel.call_sign == token ||
      channel.id == token ||
      channel.number == token
    )
  end
end

def get_lineup_date_cache_key(
  channel, # Channel
  start_time # Time, but will be truncated to its date
) # String
  digest(
    # These are the same values as required by the actual API request, so we use
    # them for symmetry.
    channel.id,
    channel.provider.country,
    channel.provider.postal_code,
    channel.provider.id,

    # The date string for which we want to find guide data.
    #
    # We use our own to ensure that the cached date is always consistent,
    # regardless of what/how the zap2it API decides to return the date keys for
    # the lineup. Mainly, this is guarding against zap2it returning something
    # like `2020-1-1` instead of `2020-01-01`.
    truncate_time(start_time, TimeInterval::DAYS).strftime('%Y-%m-%d'),
  )
end

# Attempts to fetch and return detailed program information for the given
# program/series combination. This works for both movies _and_ televsion shows!
def fetch_program_details(
  series_id, # String, the `seriesId` value fetched from lineup program info
  program_id, # String, the `tmsId` fetched from lineup program info
  season_index: nil, # Integer, one-based, the season to fetch. `nil` means "latest season", 1 means "first season", etc.
  cache: Cache.new # Cache
) # JSON program details
  Arg
    .name('series_id')
    .is_a(String)
    .non_empty
    .parse(series_id)
  Arg
    .name('program_id')
    .is_a(String)
    .non_empty
    .parse(program_id)
  unless season_index.nil?
    Arg
      .name('season_index')
      .is_a(Integer)
      .min(1)
      .parse(season_index)
  end

  # The API only allows us to look up detailed information for a whole season at
  # a time. Since we often don't even know which season a particular episode
  # came from since the initial API call returns null, we instead always start
  # by fetching the latest season, then walking the season list backwards until
  # we find the episode we're looking for.
  #
  # We fetch from latest to earliest under the assumption that most often newer
  # episodes are aired in preference to older ones.

  # When fetching the latest season, we don't cache the result for very long.
  # This ensures that unfinished seasons are re-checked regularly, but "old"
  # seasons are cached for a good long time since presumably they won't change
  # very often at all!
  ttl_seconds =
    if season_index.nil?
      TimeInterval::DAYS
    else
      90 * TimeInterval::DAYS
    end

  # TODO: "Local programming" (and presumably other things) have a series id but
  # making this request doesn't return anything _at all_, i.e. returns zero
  # bytes, when we attempt to fetch it. Deal with this!
  season_info = fetch(
    ZAP2IT_API_HOST,
    '/gapzap_webapi/api/program/PostEpisodeGuide',

    data: {
      # A magical required value!
      aid: :gapzap,

      # Essentially, "fetch all the data in a single page". This seems to work
      # without issue in all attempted cases, so... we don't have to paginate!
      pageNo: 1,
      pageSize: 999,

      # Fetch the series/season we care about right now.
      programSeriesID: series_id,
      season: season_index || -1, # Convert `nil` to `-1` for "latest season"
    },

    cache: cache,
    ttl_seconds: ttl_seconds,
  )

  episodes_info = season_info
    .fetch(:episodeGuideTab)
    .fetch(:season)
    .fetch(:episodes)

  # See if this response contained the episode information we're looking for.
  #
  # The `tmsId` value we get in `program_id` is prefixed with `SH` or `sh`
  # coming from the program guide, but is listed as `ep` in the season
  # details...
  #
  # We munge during the comparison with the program details episode id to
  # account for this and any general variation on it.
  id_munge_prefix_regex = /^(SH|EP)/i
  munged_query_episode_id = program_id.gsub(id_munge_prefix_regex, '')
  episode_details = episodes_info.find do |episode_info|
    munged_candidate_episode_id = episode_info.fetch(:tmsID).gsub(id_munge_prefix_regex, '')
    munged_candidate_episode_id == munged_query_episode_id
  end

  # If we didn't find what we were looking for in this season, iterate backwards
  # to the next season until we either find our episode or run out of seasons.
  if episode_details.nil?
    potential_season_indices = season_info
      .fetch(:episodeGuideTab)
      .fetch(:seasons)
      .map { |s| s.to_i(10) } # We expect all seasons to be string integers
      .sort
      .reverse

    # Turn `nil` into the latest season id.
    current_season_index =
      if season_index.nil?
        # Use the largest season index, i.e. the latest season, if we used the
        # "shortcut" of `nil` to fetch the season.
        potential_season_indices.max
      else
        # Otherwise, use the value we were given.
        season_index
      end

    previous_season_index = current_season_index - 1
    return (
      # Season 0 appears to be a nonsense season and doesn't actually count for
      # anything. It looks like it contains a bunch of dummy episodes most of
      # the time, none of which appears to contain useful information anyway!
      if !potential_season_indices.include?(previous_season_index) || previous_season_index.zero?
        # If we get to the point where we want to search through the episodes of
        # the prior season, but there isn't a season that fits the bill, we're
        # out of seasons and aren't going to find the episode details we want.
        nil
      else
        # Recursively fetch and look through the prior season's details.
        fetch_program_details(
          series_id,
          program_id,
          season_index: previous_season_index,
          cache: cache,
        )
      end
    )
  end

  # We found our desired episode details and can parse them!

  # Can be the empty string when a rating is (presumably) unknown.
  display_rating = episode_details.fetch(:displayRating)
  rating =
    if display_rating.nil? || display_rating.empty?
      nil
    else
      display_rating
    end

  # Movies have their release year populated with a string year like `1976` but
  # an original air date of `1000-01-01`, whereas TV show episodes have an
  # accurate original air date but an empty string release year. We'll also
  # occasionally get neither!
  raw_release_year = episode_details.fetch(:releaseYear, '')
  raw_original_air_date = episode_details.fetch(:originalAirDate, '')
  original_release_date =
    if !raw_release_year.empty?
      # We use the first of the year as a dummy day since having a release year
      # almost certainly means we're dealing with a movie for which we _only_
      # have a year anyway.
      Date.parse("#{raw_release_year}-01-01").to_time
    elsif !raw_original_air_date.empty?
      Date.parse(raw_original_air_date).to_time
    else
      # This is the typical "dummy date" we see in the API responses, so we use
      # it for parity.
      Date.parse('1000-01-01')
    end

  # If one or both of the potential dates was nonsense, we set the value to
  # `nil` to clearly indicate as much.
  original_release_date = nil if original_release_date.year < 1500

  title = episode_details.fetch(:episodeTitle).strip
  title = nil if title.empty?

  description = episode_details.fetch(:synopsis).strip
  description = nil if description.empty?

  # A season of zero appears to indicate some sort of dummy, placeholder season
  # most (all?) of the time and hence should act as if it's not present at all.
  # If present, these are 1-based, though "special" episodes (e.g. clip shows or
  # series retrospectives) might get an episode number of `0`.
  season = episode_details.fetch(:seasonNumber, '-1').to_i(10)
  season = nil if season <= 0

  episode = episode_details.fetch(:episodeNumber, '-1').to_i(10)
  episode = nil if episode < 0

  tags = episode_details
    .fetch(:tags)
    .split('|')
    .map(&:strip)
    .reject(&:empty?)

  genres = episode_details
    .fetch(:programGenres)
    .split('|')
    .map(&:strip)
    .reject(&:empty?)

  is_new = episode_details.fetch(:isNew)
  is_live = episode_details.fetch(:isLive)
  is_premier = episode_details.fetch(:isPremier)
  is_finale = episode_details.fetch(:isFinale)

  {
    # We use the id we were given since it may differ from the munged one the
    # episode info contains; we'll treat the input id as the canonical one.
    program_id: program_id,
    season: season,
    episode: episode,
    title: title,
    description: description,
    original_release_date: original_release_date,
    rating: rating,
    tags: tags,
    genres: genres,
    new?: is_new,
    live?: is_live,
    premier?: is_premier,
    finale?: is_finale,
  }
end

# Downloads and returns all the program data for the given date, if possible.
def fetch_lineup(
  channel, # Channel
  start_date = Time.now, # Time, but will be truncated to its date component
  days_to_fetch: 999_999_999, # The maximum number of days of data to fetch
  cache: Cache.new # Cache
)
  # First, we check the cache for any existing data for the given date. This is
  # necessary since every request to the API contains 14 days of data, however
  # it gets cached by `fetch` as one giant blob.
  #
  # If/when we do a _real_ fetch for all this data, we cache it individually by
  # date so we can look up only the required date when necessary and save a
  # bunch of effort around effectively only being able to fetch data for "today
  # + 14 days" from zap2it and not being able to specify an arbitrary single
  # date.
  lineup_date_cache_key = get_lineup_date_cache_key(channel, start_date)
  lineup_data = cache.get(lineup_date_cache_key)

  if lineup_data.nil?
    # If we didn't have this data in the cache, the first question we have to
    # ask is "can we even obtain this lineup data?". The zap2it API effectively
    # only allows queries for one day in the past to one day in the future, and
    # requires returning 14 days of data starting at the given date.
    #
    # We therefore must check to see whether the date we were given falls into
    # that range, and only if it does can we attempt to make an API request to
    # retrieve its data.
    now = Time.now
    min_start_time = truncate_time(now, TimeInterval::DAYS) - TimeInterval::DAYS
    max_start_time = truncate_time(now, TimeInterval::DAYS) + TimeInterval::DAYS
    if start_date < min_start_time
      raise ArgumentError, "Start date is too early, cannot be earlier than #{min_start_time}"
    elsif start_date > max_start_time
      raise ArgumentError, "Start date is too late, cannot be later than #{max_start_time}"
    end

    # We cache for only a single day since program lineup info might change for
    # whatever reason, especially further in the future where there's less
    # certainty.
    ttl_seconds = TimeInterval::DAYS

    lineup_data_for_fortnight = fetch(
      ZAP2IT_API_HOST,
      '/api/sslgrid',

      data: {
        # A magical required value!
        aid: :gapzap,

        # Channel/provider info
        prgsvcid: channel.id,
        countryCode: channel.provider.country,
        postalCode: channel.provider.postal_code,
        headendId: channel.provider.id,

        # The number of hours of program lineup data to fetch.
        #
        # Only seems to support extremely specific values in certain contexts,
        # `336` (i.e. 14 days) being the most (only?) reliable value.
        timespan: 14 * TimeInterval::DAYS / TimeInterval::HOURS,

        # The Unix time at which to start fetching data. Appears to support one
        # day in the past to one day in the future, but will still only return up
        # to `timespan` hours of data.
        #
        # To simplify reasoning about what date(s) to return etc., we truncate the
        # given start date to midnight and fetch data starting then.
        timestamp: truncate_time(start_date, TimeInterval::DAYS).to_i
      },

      cache: cache,
      ttl_seconds: ttl_seconds,
    )

    # The lineup data comes in mapped date string to array of programs; we
    # convert this into a cache entry per date.
    lineup_data_for_fortnight.each_pair do |_, lineup_programs|
      # Grab the apparent date from the lineup data itself. This ensures that
      # we'll always have a consistent date format for our cache.
      first_program_info = lineup_programs.first

      if first_program_info.nil?
        raise IOError, "Unexepectedly found an empty lineup for channel #{channel}"
      end

      first_program_time = Time.at(first_program_info.fetch(:progDateUnix))

      # Update the cache with the information for this date alone, allowing it
      # to be accessed separately from the other lineups from the API request
      # from which it was fetched.
      cache_key = get_lineup_date_cache_key(channel, first_program_time)
      cache.set(cache_key, lineup_programs, ttl_seconds, now: now)
    end

    # Now that we've (hopefully...) cached the lineup for our original start
    # date, we can re-fetch it directly from the cache.
    lineup_data = cache.get(lineup_date_cache_key)
    if lineup_data.nil?
      raise IOError, "Unexepectedly failed to look up fetched lineup date in cache for channel #{channel}"
    end
  end

  lineup_data.map do |basic_program_info|
    series_id = basic_program_info.fetch(:seriesId)
    program_id = basic_program_info.fetch(:program).fetch(:tmsId)

    # First, grab whatever information we can from the program info we already
    # have. This often isn't complete, but if we can't fetch any more details
    # than this we'll have to make due with all that we got.
    start_time = Time.at(basic_program_info.fetch(:startTime))
    end_time = Time.at(basic_program_info.fetch(:endTime))

    title = basic_program_info.fetch(:program).fetch(:title)
    secondary_title =  basic_program_info.fetch(:program).fetch(:episodeTitle)
    description = basic_program_info.fetch(:program).fetch(:shortDesc)
    image_url = "https://zap2it.tmsimg.com/assets/#{basic_program_info.fetch(:thumbnail)}.jpg"

    # Season and episode come in as either `nil` or a string integer.
    season = basic_program_info.fetch(:program).fetch(:season)
    season = season.to_i(10) unless season.nil?
    episode = basic_program_info.fetch(:program).fetch(:episode)
    episode = episode.to_i(10) unless episode.nil?

    rating = basic_program_info.fetch(:rating)

    # These don't show up at all in the basic info :(
    genres = []

    raw_release_year = basic_program_info.fetch(:program).fetch(:releaseYear)
    original_release_date =
      if raw_release_year.nil?
        nil
      else
        Date.parse("#{raw_release_year.rjust(4, '0')}-01-01").to_time
      end

    # Movies have series ids that start with `MV`!
    is_movie = /^MV/i.match?(basic_program_info.fetch(:seriesId))

    # Generic comes in as `"0"` or `"1"` and appears to indicate things like
    # "paid programming".
    is_generic = basic_program_info.fetch(:program).fetch(:isGeneric) == '1'

    # These "bits" are special in that in the basic info they come back as tags,
    # but in the enhanced info they come back as their own individual flags. We
    # search the basic info for any tags that indicate these flags, then Boolean
    # "or" them with any that come back from the enhanced info.
    #
    # TODO: Verify each tag name. I've only actually _seen_ `New`, so the rest
    # of the tag names are guesses for now!
    flag_tags = [
      new_tag = 'New',
      live_tag = 'Live',
      premier_tag = 'Premier',
      finale_tag = 'Finale',
    ]
    is_new = basic_program_info.fetch(:tag).include?(new_tag)
    is_live = basic_program_info.fetch(:tag).include?(live_tag)
    is_premier = basic_program_info.fetch(:tag).include?(premier_tag)
    is_finale = basic_program_info.fetch(:tag).include?(finale_tag)

    # Omit any of the "special" tags we just checked for since they're redundant
    # if included in both places.
    #
    # NOTE: The tags at the "top level" here in the basic info are actually
    # _better_ than the tags we get from the enhanced info! The "enhanced" tags
    # come in something like `STEREO|CC` and have to be manually cleaned, so we
    # prefer the "basic" tags instead.
    tags = (basic_program_info.fetch(:tags, [])).reject { |t| flag_tags.include?(t) }

    # Now, fetch detailed program info if possible and improve what we've
    # already gathers.
    enhanced_program_info = fetch_program_details(series_id, program_id, cache: cache)
    unless enhanced_program_info.nil?
      # We prefer the detailed version of this data if available, otherwise we
      # fall back to the "basic" version
      season = enhanced_program_info.fetch(:season, season)
      episode = enhanced_program_info.fetch(:episode, episode)

      # We "merge" these basic values with the enhanced ones.
      is_new = is_new || enhanced_program_info.fetch(:new?)
      is_live = is_live || enhanced_program_info.fetch(:live?)
      is_premier = is_premier || enhanced_program_info.fetch(:premier?)
      is_finale = is_finale || enhanced_program_info.fetch(:finale?)

      # If any of these values seem to be more "substantial" than the ones we
      # got from the basic info, we'll use them instead.
      secondary_title = secondary_title || enhanced_program_info.fetch(:title)
      if (enhanced_program_info.fetch(:title) || '').length > (secondary_title || '').length
        secondary_title = enhanced_program_info.fetch(:title)
      end

      description = description || enhanced_program_info.fetch(:description)
      if (enhanced_program_info.fetch(:description) || '').length > (description || '').length
        description = enhanced_program_info.fetch(:description)
      end

      # For these, we prefer the "enhanced" version wherever available.
      original_release_date = enhanced_program_info.fetch(:original_release_date, original_release_date)
      rating = enhanced_program_info.fetch(:rating, rating)

      # These don't show up at all in the "basic" info, so we _must_ use those
      # that come back from the enhanced info.
      genres = enhanced_program_info.fetch(:genres)
    end

    Program.new(
      channel: channel,
      series_id: series_id,
      program_id: program_id,
      start_time: start_time,
      end_time: end_time,
      image_url: image_url,
      season: season,
      episode: episode,
      title: title,
      secondary_title: secondary_title,
      description: description,
      rating: rating,
      original_release_date: original_release_date,
      tags: tags,
      genres: genres,
      is_movie: is_movie,
      is_new: is_new,
      is_live: is_live,
      is_premier: is_premier,
      is_finale: is_finale,
      is_generic: is_generic,

      # TODO: Where would we find this?
      previously_shown_time: nil,
    )
  end
end

# Formats the given data nicely for display in the terminal as a table and
# returns the result.
#
# No more columns than those given will be displayed. If any row contains too
# few values for the given columns, empty strings will be displayed in the
# "holes".
def display_table(
  columns, # Array<Symbol | String>
  rows, # Array<Tuple<any>>
  column_padding: 2 # Integer, the space between columns
) # String
  # Turn everything into a string and resize all the rows to be no longer than
  # the number of columns. Any "missing" columns will be omitted from the output
  # entirely.
  max_column_value_lengths = columns.map { 0 }
  prepared_rows = [columns, *rows].map do |row|
     row
       .slice(0, columns.length) # Truncate to column length
       .map(&:to_s) # Stringify contents
       .each_with_index do |value, column_index| # Update max value size
         value_length = value.length
         if max_column_value_lengths[column_index] < value_length
           max_column_value_lengths[column_index] = value_length
         end
       end
  end

  column_padder = ' ' * column_padding
  prepared_rows.each_with_index.map do |row, index|
    row
      .each_with_index
      .map { |r, i| r.ljust(max_column_value_lengths[i]) }
      .join(column_padder)
      .strip
  end.join("\n")
end

# Returns a string for terminal output representing provider information.
def display_providers(
  country, # :USA | :CAN
  postal_code, # String
  cache: Cache.new # Cache
) # String
  providers = fetch_providers(
    country,
    postal_code,
    cache: cache
  )

  display_table(
    [:NAME, :TYPE, :ID],
    providers.map { |p| [p.name, p.type, p.id] },
  )
end

# Returns a string for terminal output representing the channel list for the
# given provider.
def display_channels(
  provider, # Provider
  channel_tokens: Set.new, # Enumerable<String> | nil. If non-empty, channels to display
  cache: Cache.new # Cache
) # String
  channels = fetch_channels(
    provider,
    channel_tokens: channel_tokens,
    cache: cache,
  )

  display_table(
    [:'CALL_SIGN', :NUMBER, :NAME, :ID],
    channels.map { |c| [c.call_sign, c.number, c.name, c.id] },
  )
end

# Returns a string for terminal output representing the program lineup for the
# given channel.
def display_lineup(
  channels, # Channel
  start_date = Time.now, # Time, inclusive, but will be truncated to its date component
  end_date = start_date + TimeInterval.DAYS, # Time, exclusive, but will be truncated to its date component
  cache: Cache.new # Cache
)
  programs = channels.flat_map do |channel|
    fetch_lineup(
      channel,
      start_date,
      cache: cache,
    )
  end

  date_format = '%Y-%m-%d'
  time_format = '%l:%M %p'
  display_table(
    [:CHANNEL, :START_DATE, :START, :END, :TITLE],
    programs.map do |p|
      [
        p.channel.call_sign,
        p.start_time.strftime(date_format),
        p.start_time.strftime(time_format),
        p.end_time.strftime(time_format),
        p.title
      ]
    end,
  )
end

def main!
  config = parse_config_from_args(ARGV).freeze

  $output = config.output_file
  cache = config.cache

  # Output program help text.
  #
  # This is handled as the very first "mode" since it's the most important part
  # of any command; its presence anywhere in the arguments indicates the user
  # doesn't know what to do and needs our guidance!
  if config.return_help_text?
    $output.puts(config.option_parser.help())
    return
  end

  # Output program version information.
  if config.return_version_info?
    $output.puts(VERSION_INFO)
    return
  end

  # Output our XMLTV description.
  if config.return_description?
    $output.puts(XMLTV_DESCRIPTION)
    return
  end

  # Output our XMLTV capabilities.
  if config.return_capabilities?
    XMLTV_CAPABILITIES.each do |capability|
      $output.puts(capability)
    end
    return
  end

  # Download and print out the configured lineup information.
  if config.return_providers?
    country = config.require(:country)
    postal_code = config.require(:postal_code)

    output = display_providers(
      country,
      postal_code,
      cache: cache,
    )
    $output.puts(output)
    return
  end

  # Download and print out a listing of the channels for the given provider
  # information.
  if config.return_channels?
    country = config.require(:country)
    postal_code = config.require(:postal_code)
    provider_id = config.require(:provider_id)

    provider = fetch_provider(
      country,
      postal_code,
      provider_id,
      cache: cache,
    )

    if provider.nil?
      raise ArgumentError, "No provider found for country #{country}, postal code #{postal_code}, and id #{provider_id}"
    end

    output = display_channels(
      provider,
      channel_tokens: config.channel_tokens,
      cache: cache,
    )
    $output.puts(output)
    return
  end

  # Download and print out a listing of the channel lineups for the given
  # channels and times.
  if config.return_lineup?
    country = config.require(:country)
    postal_code = config.require(:postal_code)
    provider_id = config.require(:provider_id)

    provider = fetch_provider(
      country,
      postal_code,
      provider_id,
      cache: cache,
    )

    if provider.nil?
      raise ArgumentError, "No provider found for country #{country}, postal code #{postal_code}, and id #{provider_id}"
    end

    # TODO: Obey --days and --day-offset values here!

    channels = fetch_channels(
      provider,
      channel_tokens: config.channel_tokens,
      cache: cache,
    )

    now = Time.now
    output = display_lineup(
      channels,
      now,
      now + TimeInterval::DAYS,
      cache: cache,
    )
    $output.puts(output)
  end
ensure
  # Always attempt to persist the cache once we're done, assuming it got
  # initialized.
  #
  # We don't have to worry about some partially-valid state since the cache
  # doesn't store entries that generate an exception during population, meaning
  # that whatever _does_ end up in the cache was at least valid enough to have
  # been generated by the code.
  cache.persist! unless cache.nil?
end

if __FILE__ == $0
  main!
end
