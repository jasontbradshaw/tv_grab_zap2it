#!/usr/bin/env ruby
# frozen_string_literal: true
require 'digest'
require 'json'
require 'net/http'
require 'optparse'
require 'ostruct'
require 'pathname'
require 'set'
require 'uri'

# Returns the SHA-512 digest of the given stringified values.
def digest(
  *values, # Array<any>
  format: :hex, # :hex | :base64 | :bubblebabble
  length: 999_999_999 # The maximum length at which to truncate the resulting digest
)
  d = Digest::SHA2.new(512)
  values.each { |v| d.update(v.to_s) }

  hash =
    case format
      when :hex then d.hexdigest()
      when :base64 then d.base64digest()
      when :bubblebabble then d.bubblebabble()
      else
        raise ArgumentError, "Invalid format: #{format.inspect}"
    end

  hash[0...length]
end

XMLTV_DESCRIPTION = 'USA/Canada (zap2it.com)'
XMLTV_CAPABILITIES = %w[baseline cache].freeze

ZAP2IT_API_HOST = 'https://tvlistings.zap2it.com'
WIKIPEDIA_API_HOST = 'https://en.wikipedia.org/w/api.php'

# The name of the Wikipedia "template" that contains the information we're
# looking for. This is basically a built-in directive for displaying information
# about something in a standardized way, in our case a television station.
#
# See https://en.wikipedia.org/wiki/Template:Infobox_television_station for the
# template's documentation.
WIKIPEDIA_TV_STATION_AFFILIATION_TEMPLATE_TITLE = 'Infobox television station'

PROGRAM_NAME = 'tv_grab_zap2it'
VERSION = Gem::Version.new('0.0.1').freeze
RELEASE =
  begin
    # Compute a truncated SHA-512 digest of the current file and use this as our
    # release identifier. Since the entire program is in a single file, this is
    # guaranteed to provide a unique and stable value for each "build".
    file_data = File.read(Pathname.new(__FILE__).realpath)

    digest(file_data, format: :hex, length: 8)
  end

# Compile the version information into a single string for easy use elsewhere.
VERSION_INFO = "#{PROGRAM_NAME} v#{VERSION} (#{RELEASE})"

# Useful time constants for obtaining a number of seconds via multiplication.
class TimeInterval
  SECONDS = 1
  MINUTES = 60 * SECONDS
  HOURS = 60 * MINUTES
  DAYS = 24 * HOURS
  WEEKS = 7 * DAYS
end.freeze

# A builder-style class for parsing and validating arguments in a standardized
# way.
class Arg
  attr_reader \
    :name,
    :parsers,
    :value

  def initialize(name, parsers, value)
    @name = name
    @parsers = parsers
    @value = value
  end

  # Create a new builder with the given argument name and nothing else.
  def self.name(
    name # String
  )
    unless name.is_a?(String) && !name.empty?
      raise ArgumentError, 'name must be a non-empty string'
    end

    Arg.new(name.strip, [], nil)
  end

  # Add a parser and return a new `Arg` that inherits its other values, but has
  # another parser added to its list.
  def parser(
    &block # (value: typeof prior parser) => any
  )
    Arg.new(
      @name,
      @parsers + [block],
      @value,
    )
  end

  # Ensures that the passed value is an instance of at least one of the given
  # classes using `#is_a?`.
  def is_a(first_class, *other_classes)
    classes = [first_class, *other_classes].freeze
    self.parser do |value|
      value_is_a = classes.any? do |potential_class|
        value.is_a?(potential_class)
      end

      unless value_is_a
        class_names = classes.map(&:name).join(', ')

        # Don't add "one of" if there's only a single class to check.
        one_of_text =
          if classes.empty?
            ' '
          else
            'one of '
          end

        raise ArgumentError, "{{ name }} must be an instance of #{one_of_text}#{class_names}, but was an instance of #{value.class.name}"
      end

      value
    end
  end

  # Ensures the passed value is one of the given values.
  def one_of(potential_value, *other_values)
    potential_values = [potential_value, *other_values].freeze
    self.parser do |value|
      unless potential_values.include?(value)
        values_text = potential_values.map(&:to_s).join(', ')

        # Don't add "one of" if there's only a single value to check.
        one_of_text =
          if potential_values.empty?
            ' '
          else
            'one of '
          end

        raise ArgumentError, "{{ name }} was expected to be #{one_of_text}#{values_text}, but was #{value}"
      end

      value
    end
  end

  def non_empty
    self.parser do |value|
      if value.empty?
        raise ArgumentError, "{{ name }} must not be empty"
      end

      value
    end
  end

  # Validates that a value is non-`nil` and returns that value.
  def non_nil
    self.parser do |value|
      if value.nil?
        raise ArgumentError, "{{ name }} is required to be non-`nil`"
      end

      value
    end
  end

  def strip
    self.is_a(String).parser do |value|
      value.strip
    end
  end

  def substitute(
    regexp, # Regexp
    substitution # String
  )
    self.is_a(String).parser do |value|
      value.gsub(regexp, substitution)
    end
  end

  def downcase
    self.is_a(String).parser do |value|
      value.downcase
    end
  end

  def upcase
    self.is_a(String).parser do |value|
      value.upcase
    end
  end

  def symbolize
    self.parser do |value|
      value.to_sym
    end
  end

  def int
    self.is_a(String).parser do |value|
      begin
        Integer(value, 10)
      rescue ArgumentError => err
        raise ArgumentError, "{{ name }} of #{value.inspect} is not a valid integer: #{err}"
      end

      value
    end
  end

  def min(
    n # Integer
  )
    self.parser do |value|
      if value < min
        raise ArgumentError, "{{ name }} of #{value.inspect} must not be less than #{n}"
      end
    end
  end

  def max(
    n # Integer
  )
    self.parser do |value|
      if value > max
        raise ArgumentError, "{{ name }} of #{value.inspect} must not be greater than #{n}"
      end
    end
  end

  def path_name
    self.parser do |value|
      Pathname
        .new(value)
        .cleanpath
        .expand_path
    end
  end

  def exists
    self.parser.path_name do |value|
      unless value.exist?
        raise ArgumentError, "{{ name }} of #{value} does not exist"
      end
    end
  end

  # Opens an IO stream from the given path name.
  def stream(
    mode = 'r' # https://ruby-doc.org/core-2.7.2/IO.html#method-c-new-label-IO+Open+Mode
  )
    self.path_name.parser do |path|
      fd = path.sysopen(mode)
      IO.open(fd, mode)
    end
  end

  def uri
    self.parser do |value|
      URI.parse(value)
    rescue URI::InvalidURIError => err
      raise ArgumentError, "{{ name }} of #{value.inspect} is not a valid URI: #{err}"
    end
  end

  # Thread our value through each configured parser in turn, passing the result
  # of each parser/validator through to the next. Returns the output of the
  # final parser given and sets `.value` to the same.
  #
  # Converts all raised errors into `ArgumentError` instances and re-raises
  # them.
  def parse(
    value # any
  )
    begin
      @value = @parsers.inject(value) do |current_value, parser|
        parser.call(current_value)
      end
    rescue StandardError => err
      new_err_class = ArgumentError

      # Replace any occurence of `{{ name }}` with our configured name to make
      # the errors prettier.
      message = err.message.gsub(/\{\{\s*name\s*\}\}/i, @name)

      # Preserve the original error class name if it's not the one we're
      # "casting" to.
      if err.class.name != new_err_class.name
        message = "#{err.class.name}: #{message}"
      end

      raise new_err_class, message
    end
  end
end

# A provider, something that has a "lineup" associated with it. Typically this
# is something like "Local over-the-air stations" or "AT&T Uverse Cable" or
# something of this nature.
#
# These are only unique given a combination of country, postal code, and
# "headend" id!
class Provider
  attr_reader \
    :country,
    :id,
    :postal_code,
    :name,
    :type

  def initialize(
    country: nil, # Symbol
    id: nil, # String, the `headendId` value from the API
    name: nil, # String
    postal_code: nil, # String
    type: nil # String
  )
    @id = Arg
      .name('id')
      .non_nil
      .is_a(String)
      .non_empty
      .parse(id)

    @name = Arg
      .name('name')
      .non_nil
      .is_a(String)
      .non_empty
      .parse(name)

    @postal_code = Arg
      .name('postal_code')
      .non_nil
      .non_empty
      .is_a(String)
      .parse(postal_code)

    @type = Arg
      .name('type')
      .non_nil
      .is_a(String)
      .non_empty
      .downcase
      .symbolize
      .parse(type)

    @country = Arg
      .name('country')
      .one_of(:USA, :CAN)
      .parse(country)
  end
end

# A channel in a lineup.
class Channel
  attr_reader \
    :provider,
    :call_sign,
    :name,
    :id,
    :number,
    :image_url

  def initialize(
    provider: nil, # Provider
    call_sign: nil, # String
    name: nil, # String | nil
    id: nil, # String
    number: nil, # String
    image_url: nil # String
  )
    @provider = Arg
      .name('provider')
      .is_a(Provider)
      .parse(provider)

    @call_sign = Arg
      .name('call_sign')
      .non_nil
      .is_a(String)
      .strip
      .non_empty
      .parse(call_sign)

    # Name is optional.
    @name =
      unless name.nil?
        Arg
          .name('name')
          .is_a(String)
          .strip
          .non_empty
          .parse(name)
      end

    @id = Arg
      .name('id')
      .non_nil
      .is_a(String)
      .strip
      .non_empty
      .parse(id)

    @number = Arg
      .name('number')
      .non_nil
      .is_a(String)
      .strip
      .non_empty
      .parse(number)

    @image_url = Arg
      .name('image_url')
      .non_nil
      .is_a(String)
      .strip
      .uri
      .parser do |uri|
        # The URL comes in "naked" and needs to be secure-ified.
        uri.scheme = 'https'

        # The URL _also_ comes in with `w=123`, but if we strip this off then
        # we'll get back the original, full-size picture instead.
        uri.query = nil
      end
      .parse(image_url)
  end

  # An RFC-2838 id for this channel.
  def xml_id
    # See https://tools.ietf.org/html/rfc2838 for more information about this id
    # format. Since we can't really determine the actual "owner" of the network,
    # we attribute it to zap2it via their internal id instead.
    "tv:#{id}.zap2it.com"
  end

  def to_xml
    raise 'TODO: Implement Channel#to_xml'
  end
end

# An episode number in some episode numbering system, e.g. `S01E02` in the
# `onscreen` system.
class EpisodeNumber
  attr_reader \
    :value,
    :system

  def initialize(
    value: nil, # String
    system: nil # :xmltv_ns | :onscreen
  )
    raise 'TODO: Implement EpisodeNumber'
  end

  def to_xml
    raise 'TODO: Implement EpisodeNumber#to_xml'
  end
end

# One program/timeslot combination in a channel's lineup.
class Program
  attr_reader \
    :channel,
    :start_time,
    :stop_time,
    :episode_numbers,
    :title,
    :secondary_title,
    :description,
    :image_url,
    :previously_shown_time,
    :rating,
    :categories

  def initialize(
    channel: nil, # Channel
    start_time: nil, # Time, inclusive
    stop_time: nil, # Time, exclusive
    episode_numbers: nil, # Array<EpisodeNumber>
    title: nil, # String
    secondary_title: nil, # String | nil
    description: nil, # String
    image_url: nil, # URI
    previously_shown_time: nil, # Time | nil
    rating: nil, # Rating | nil
    categories: nil # Set<String>
  )
    raise 'TODO: Implement Program'
  end

  def to_xml
    raise 'TODO: Implement Program#to_xml'
  end
end

# A config-specific struct that allows for easy access to arbitrary values while
# also providing some helper methods that we use elsewhere.
class ConfigOpenStruct < OpenStruct
  # Require that the given configuration value be present in the parsed
  # configuration, then return it if present.
  def require(
    key, # Symbol
    name = nil # String, defaults to being auto-generated from `key`
  ) # typeof self[key]
    # "Humanize" the config key if an explicit name wasn't given.
    if name.nil?
      name = key
        .to_s
        .gsub(/_+/, ' ')
        .strip
    end

    value = self[key]

    if value.nil?
      article_text =
        if /^[aeiou]/.match?(name)
          "An"
        else
          "A"
        end
      raise ArgumentError, "#{article_text} #{name} is required but was not specified; see --help for details."
    end

    value
  end
end

# A simple cache, either in-memory or backed by a file. All cached values are
# converted to nested `OpenStruct` instances.
class OpenStructCache
  attr_reader \
    :path

  # Reads existing values from the given path, if any, otherwise uses a
  # memory-only backing store.
  def initialize(
    path = nil # Pathname | nil
  )
    @path = path
    @cache = {}

    # Initialize the cache from the backing store, if given.
    if !path.nil? && path.exist?
      @cache = path.open('r') do |f|
        # We load all our JSON in this method using `OpenStruct` instances in
        # place of hashes; this makes access much more convenient!
        #
        # We use `JSON#parse` instead of `JSON#load` so we don't have to trust
        # the input file we were given, which the documentation suggests we must
        # do if using `JSON#load`.
        JSON.parse(f.read, symbolize_names: true)
      end

      unless @cache.is_a?(Hash)
        raise IOError, "#{path} contains invalid cache file contents"
      end

      # Pre-parse all cached values so we can return them directly rather than
      # re-parsing them on every access.
      @cache.each do |(_, entry)|
        entry[:value] = to_deep_openstruct(entry[:value])
      end
    end
  end

  def size
    @cache.size
  end

  # Write the cache to the originally-configured path, if any. If no path was
  # originally configured, does nothing.
  def persist! # self
    # If we were given no path, we can't persist the cache.
    return if @path.nil?

    # Serialize all values in the cache prior to dumping them.
    now = Time.now
    serialized_cache = @cache.inject({}) do |all, (key, entry)|
      # Don't serialize expired entries, effectively pruning them from the
      # persisted cache.
      return all if entry_expired?(entry, now: now)

      # This does a shallow clone so we can then modify any remaining items
      # ourselves.
      new_entry = entry.dup
      new_entry[:value] = to_deep_hash(new_entry[:value])

      all[key] = new_entry
      all
    end

    @path.open('w') do |f|
      JSON.dump(serialized_cache, f)
    end

    self
  end

  # Load a JSON value from the cache. If no value is found, the given block is
  # run and the resulting value is stored in the cache before being returned.
  def lookup(
    cache_key, # String | Symbol
    ttl_seconds: 0, # Integer
    &block # () => JSON
  ) # cached value converted to a nested `OpenStruct<JSON>`
    cache_key = cache_key.to_sym

    now = Time.now
    cache_entry =
      if @cache.include?(cache_key)
        entry = @cache[cache_key]

        # If the entry has expired, delete it from the cache entirely; we'll
        # create a new entry later if we can.
        if entry_expired?(entry, now: now)
          @cache.delete(cache_key)
          nil
        else
          entry
        end
      else
        nil
      end

    # If no entry was cached or the cached entry has expired, create a new entry
    # and add it to the cache.
    if cache_entry.nil?
      value = to_deep_openstruct(yield)
      cache_entry = {
        created_at: now.to_i,
        ttl_seconds: ttl_seconds,
        value: value
      }

      # Only add an entry to the cache if it makes sense to do so!
      if ttl_seconds > 0
        @cache[cache_key] = cache_entry
      end
    end

    cache_entry[:value]
  end

  private

  def entry_expired?(entry, now: Time.now)
    now > Time.new(entry[:created_at] + entry[:ttl_seconds])
  end

  # Converts a nested JSON-ish `OpenStruct` instance into a normal hash, where
  # all `OpenStruct` instances and hashes are replaced with symbolized hashes.
  def to_deep_hash(
    value # OpenStruct<JSON>, possibly nested
  ) # Hash<Symbol, JSON>
    if value.is_a?(OpenStruct)
      value.to_h do |key, value|
        [key.to_sym, to_deep_hash(value)]
      end
    elsif value.is_a?(Hash)
      value.map do |key, value|
        [key.to_sym, to_deep_hash(value)]
      end.to_h
    elsif value.is_a?(Array)
      value.map { |v| to_deep_hash(v) }
    else
      value
    end
  end

  # Converts a JSON-ish value into a nested `OpenStruct` instance, where all
  # hashes and `OpenStruct` instances are replaced with `OpenStruct`s.
  def to_deep_openstruct(
    value # JSON-ish
  ) # OpenStruct<JSON>, possibly nested
    if value.is_a?(OpenStruct)
      o = OpenStruct.new
      value.each_pair do |k, v|
        o[k] = to_deep_openstruct(v)
      end
      o
    elsif value.is_a?(Hash)
      o = OpenStruct.new
      value.each do |k, v|
        o[k] = to_deep_openstruct(v)
      end
      o
    elsif value.is_a?(Array)
      value.map { |v| to_deep_openstruct(v) }
    else
      value
    end
  end
end

# Returns a new `Time` truncated to the given unit.
def truncate_time(
  time, # Time
  unit # Integer seconds, e.g. 3600 for an hour or 86400 for a day
)
  time - (time.to_i % unit)
end

# Parse the configuration from the command line options.
def parse_config_from_args(
  args # An arguments array, typically `ARGV`.
)
  config = ConfigOpenStruct.new({
    # The `OptionParser` instance that parsed this config. This is useful so
    # whoever gets the config can inspect e.g. all the parsed arguments that
    # created it.
    option_parser: nil, # OptionParser | nil

    # These `return_*?` options indicate that the program should run in the
    # requisite "mode" and do something, then exit.

    # Generic hacks for better `--help` output
    return_help_text?: false,
    return_version_info?: false,

    # XMLTV-specific
    return_capabilities?: false,
    return_description?: false,

    # Program-specific
    return_channels?: false,
    return_providers?: false,

    # XMLTV-specific configuration, mostly optional.
    cache: OpenStructCache.new, # Cache, in-memory by default
    day_offset: 0, # Integer
    days_to_fetch: 999_999_999, # Integer
    output_file: $stdout, # File-like
    quiet?: false,

    # Program-specific configuration, mostly required.
    channel_tokens: Set.new, # Set<String>
    country: nil, # :USA | :CAN
    postal_code: nil, # String
    provider_id: nil, # String
  })

  option_parser = OptionParser.new do |opts|
    opts.program_name = PROGRAM_NAME
    opts.version = "v#{VERSION}" # Pre-pending `v` makes it look nicer
    opts.release = RELEASE

    ############################################################################
    # GENERIC OPTIONS
    ############################################################################

    # Manually specifying these ensures they show up in `--help` output instead
    # of simply being silently available without obvious provenance.

    opts.on(
      '-h',
      '--help',
      'Output program help text and exit.'
    ) do
      config[:return_help_text?] = true
    end

    opts.on(
      '-v',
      '--version',
      'Output program version information and exit.'
    ) do
      config[:return_version_info?] = true
    end

    ############################################################################
    # XMLTV OPTIONS
    ############################################################################

    #
    # Minimum required options
    #

    opts.on(
      '--description',
      'Output our XMLTV description and exit.'
    ) do
      config[:return_description?] = true
    end

    opts.on(
      '--capabilities',
      'Output our supported XMLTV capabilities and exit.'
    ) do
      config[:return_capabilities?] = true
    end

    #
    # `baseline` options
    #

    opts.on(
      '-q',
      '--quiet',
      'Suppress non-error output.'
    ) do
      config[:quiet?] = true
    end

    output_flag = '--output'
    opts.on(
      '-o PATH',
      "#{output_flag} PATH",
      'Write output to this file instead of standard output.'
    ) do |p|
      config.output_file = Arg
        .name(output_flag)
        .path_name
        .stream('w')
        .parse(p)
    end

    days_flag = '--days'
    opts.on(
      '-d DAYS',
      "#{days_flag} DAYS",
      'The number of days of data to obtain, default "as many as are available"'
    ) do |d|
      config.days_to_fetch = Arg
        .name(days_flag)
        .int
        .min(1)
        .parse(d)
    end

    offset_flag = '--offset'
    opts.on(
      '-f DAYS',
      "#{offset_flag} DAYS",
      'Obtain data starting on the date this many days from today. Default is 0, i.e. "today", 1 is "tomorrow", etc.'
    ) do |o|
      config.day_offset = Arg
        .name(offset_flag)
        .int
        .parse(o)
    end

    config_file_flag = '--config-file'
    opts.on(
      "#{config_file_flag} PATH",
      'The path to the (optional) configuration file for this grabber.'
    ) do |p|
      config_path = Arg
        .name(config_file_flag)
        .path_name
        .exists
        .parse(p)

      # Reads command line options from the given file and parses them.
      #
      # We do this "in line" so that options are applied "logically", i.e.
      # options that come _before_ this flag are overwritten, but options that
      # come _after_ this flag overwrite previously-loaded options.
      opts.load(config_path)
    end

    #
    # `cache` options
    #

    cache_flag = '--cache'
    opts.on(
      "#{cache_flag} PATH",
      'If given, a file in which to cache network requests across invocations.'
    ) do |p|
      cache_filename = Arg
        .name(cache_flag)
        .path_name
        .parse(p)

      config.cache = OpenStructCache.new(cache_filename)
    end

    ############################################################################
    # PROGRAM OPTIONS
    ############################################################################

    country_flag = '--country'
    opts.on(
      '-c COUNTRY',
      "#{country_flag} COUNTRY",
      'The country for which to fetch data, one of USA or CAN.'
    ) do |s|
      config.country = Arg
        .name(country_flag)
        .strip
        .one_of('USA', 'CAN')
        .symbolize
        .parse(s)
    end

    postal_code_flag = '--postal-code'
    opts.on(
      '-p CODE',
      "#{postal_code_flag} CODE",
      'The postal code for which to fetch data. Something like `12345` for the USA or `A1A 1A1` for Canada.`'
    ) do |s|
      config.postal_code = Arg
        .name(postal_code_flag)
        .strip
        .substitute(/\s+/, ' ') # Condense spaces for uniformity
        .non_empty
        .parse(s)
    end

    show_provider_info_flag = '--show-providers'
    opts.on(
      show_provider_info_flag,
      "Output provider information for the given #{country_flag} and #{postal_code_flag} values."
    ) do |s|
      config[:return_providers?] = true
    end

    provider_id_flag = '--provider-id'
    opts.on(
      '-r ID',
      "#{provider_id_flag} ID",
      "The provider id for which to fetch data, obtained with the help of the #{show_provider_info_flag} flag."
    ) do |s|
      config.provider_id = Arg
        .name(provider_id_flag)
        .strip
        .non_empty
        .parse(s)
    end

    show_channels_flag = '--show-channels'
    opts.on(
      show_channels_flag,
      "Output the channels for the given #{country_flag}, #{postal_code_flag}, and #{provider_id_flag} values.",
    ) do
      config[:return_channels?] = true
    end

    channels_flag = '--channels'
    opts.on(
      "#{channels_flag} CHANNELS",
      "A comma-delimited list of channel call signs, numbers, and/or ids for which to download data. If non are provided, all available channels will be downloaded.",
    ) do |s|
      raw_channels = Arg
        .name(channels_flag)
        .strip
        .non_empty
        .parse(s)

      config.channel_tokens = raw_channels
        .split(/,/)
        .map(&:strip) # Handle interior whitespace
        .reject(&:empty?) # Remove empty strings
        .to_set
    end
  end

  # It's useful for external things to have access to this at times, e.g. for
  # printing help text.
  config.option_parser = option_parser

  # Parse the given arguments into our config object.
  option_parser.parse(args)

  config
end

# Makes a GET request to some host/path combination and returns the JSON
# response converted to a deep `OpenStruct`. If the request fails or its
# response body can't be parsed, raises an `IOError`.
def fetch(
  *path, # Array<any>
  query: nil, # Hash<Symbol, any> | nil
  data: nil, # Hash<Symbol | String, JSON> | nil
  cache: OpenStructCache.new, # OpenStructCache
  ttl_seconds: 0 # Integer
) # OpenStruct<JSON> | Array<OpenStruct>, both possibly nested
  full_path = File.join(*path.map(&:to_s))

  uri =
    begin
      u = URI.parse(full_path)

      unless query.nil?
        u.query = URI.encode_www_form(query.to_a)
      end

      u
    rescue URI::InvalidURIError => err
      raise ArgumentError, "Invalid URI from path #{full_path.inspect}: #{err}"
    end

  cache_key = digest(
    u, # The full request URL
    URI.encode_www_form(data || {}), # POST data for the request, if any
    format: :base64,
    length: 32
  )

  cache.lookup(cache_key, ttl_seconds: ttl_seconds) do
    # Don't allow things to take too long since we might be making a lot of
    # requests.
    timeout = 15 * TimeInterval::SECONDS
    res =
      Net::HTTP.start(
        uri.host,
        uri.port,
        use_ssl: u.scheme == 'https',
        open_timeout: timeout,
        read_timeout: timeout,
        write_timeout: timeout,
        ssl_timeout: timeout,
      ) do |http|
        method = data.nil? ? :GET : :POST
        http.send_request(method, uri.request_uri, data, {
          'User-Agent' => VERSION_INFO
        })
      end

    unless res.is_a?(Net::HTTPSuccess)
      raise IOError, "Got HTTP #{res.code} from #{uri}: #{res.message}"
    end

    # Parse the body as JSON, the only response format we expect to receive.
    begin
      JSON.parse(res.body)
    rescue JSON::ParserError => err
      raise IOError, "Failed to parse HTTP response from #{uri}: #{err}"
    end
  end
end

# Retrieves the Wikitext content from the given page info with revisions if it
# has any, otherwise returns `nil`.
def get_page_content(
  page_info # OpenStruct, Wikipedia page info response
) # String | nil
  page_content_container = page_info.dig(:revisions, 0, :slots, :main)
  return nil if page_content_container.nil?

  # We expect only Wikitext content and fail on anything else.
  return nil unless page_content_container.contentformat == 'text/x-wiki'

  page_content_container.content
end

# Parse all the links from the given page's content, if any, returning up to
# `limit` of them (default no limit).
def get_page_link_titles(
  page_info, # OpenStruct, Wikipedia page info response
  limit: 999_999_999
) # Array<String>, possibly empty
  # Parse the page content to find links, returning them if found.
  page_content = get_page_content(page_info)
  return [] if page_content.nil?

  # This doesn't perfectly match all links since it'll probably miss those with
  # `]` or `|` in their titles, but it's good enough for our purposes.
  page_content
    .scan(
      %r[
        # Link open token
        \[\[

        \s*

        # One or more chars that's not a pipe or a likely close token, consuming
        # as little as possible to obtain this (i.e. excluding trailing
        # whitespace). This is the title of the link, i.e. the page title to
        # which it refers.
        (
          [^\|\]]+?
        )

        # Optionally, a pipe followed by some more _optional_ text that's not a
        # close token. This is the "text" of the link, what's displayed to the
        # user if different from the title given. If a pipe with no following
        # text is given, this gets auto-looked up from the linked page,
        # apparently.
        (?:
          \s*

          \|

          \s*

          [^\]]*
        )?

        \s*

        # Link close token
        \]\]
      ]x
    )
    .lazy
    .map(&:first) # Obtain the first group, i.e. the page title
    .map(&:strip) # Ensure it has no extra whitespace
    .take(limit)
    .to_a
end

# Returns `true` when the given page info looks like it represents a television
# station with affiliation information, `false` otherwise.
def does_page_contain_tv_affiliation_info(
  page_info, # OpenStruct, Wikipedia page info response
  general_call_sign: nil # The page's call sign, ignored if not provided
) # Boolean
  # The page's title starts with our general call sign, if given.
  has_title_prefix =
    if general_call_sign.nil?
      true
    else
      page_info.title.start_with?(general_call_sign)
    end

  # The page content looks like it probably contains the TV station affiliation
  # information template.
  page_content = get_page_content(page_info)
  has_tv_info = /\{\{\s*#{Regexp.escape(WIKIPEDIA_TV_STATION_AFFILIATION_TEMPLATE_TITLE)}/.match?(page_content)

  has_title_prefix && has_tv_info
end

# Retrive the Wikipedia page information for a TV station and obtain the
# affiliate information from said page. If no affiliate information can be found
# for the given call sign prefix, returns `nil`.
def fetch_station_affiliate_info(
  country, # :USA | :CAN
  call_sign, # String, something like `KABCDT1`
  cache: OpenStructCache.new # OpenStructCache
) # Hash<String channel number, String affiliate name> | nil
  # TODO: Support Canada as well!
  return nil unless country == :USA

  # There's no need to update the page information frequently since it shouldn't
  # change very often at all, or at least not in a way that materially affects
  # us.
  ttl_seconds = 30 * TimeInterval::DAYS

  # First, do a general search for pages prefixed with the "general" call sign,
  # i.e. the first four characters of the call sign we were given. This turns
  # something like `KVUEDT` into `KVUE`, which is the title a Wikipedia entry
  # for a TV station call sign almost universally starts with.
  #
  # If we find a page that contains a TV station affiliate information template
  # and has a title that starts with our general call sign, this almost
  # certainly the page we're looking for!
  #
  # For an example query request, see
  # https://en.wikipedia.org/w/api.php?action=query&format=json&prop=revisions&generator=allpages&formatversion=2&rvprop=content&rvslots=main&gapprefix=KADT&gaplimit=5
  general_call_sign = call_sign[0...4]
  general_query_response = fetch(
    WIKIPEDIA_API_HOST,

    query: {
      # Get a JSON response in the latest stable version.
      format: :json,
      formatversion: 2,

      # We're looking for something!
      action: :query,

      # Get general and template page information. A "template" is essentially a
      # standardized way of displaying information on a Wikipedia page.
      prop: 'revisions',
      generator: :allpages,

      # Include the latest revision so we can parse its text to obtain the data
      # we're looking for.
      rvprop: :content,
      rvslots: :main,

      # Look for pages that start with our general call sign. We limit the
      # number of results since if nothing shows up in the first handful, it's
      # quite unlikely that we're going to find anything relevant anyway.
      gapprefix: general_call_sign,
      gaplimit: 5,
    },

    cache: cache,
    ttl_seconds: ttl_seconds,
  )

  # Pull the page info values from the query response.
  general_query_page_info_responses = general_query_response.dig(:query, :pages)

  # If we got no responses at all, we're out of luck.
  if general_query_page_info_responses.nil? || general_query_page_info_responses.empty?
    return nil
  end

  # First, look for _any_ response that happens to fit exactly what we were
  # looking for.
  page_info = general_query_page_info_responses.find do |general_page_info|
    does_page_contain_tv_affiliation_info(
      general_page_info,
      general_call_sign: general_call_sign,
    )
  end

  # If we didn't find any page info directly, generate and fetch a list of page
  # titles at which to look for TV affiliate information. These can come from
  # redirects and/or disambiguation pages.
  #
  # For an example query that contains both of these kinds of results, see
  # https://en.wikipedia.org/w/api.php?action=query&format=json&prop=revisions&generator=allpages&formatversion=2&rvprop=content&rvslots=main&gapprefix=KADT&gaplimit=5
  if page_info.nil?
    # Match strings that look like they might be TV station call sign page
    # titles.
    starts_with_call_sign_regex = /^[KW][A-Z0-9]{3}/

    # Generate an array of links that look like they might contain the
    # information we seek.
    promising_page_titles = []

    # First, add call-sign-like redirect pages since these are the most likely
    # source of data.
    general_query_page_info_responses.each do |general_page_info|
      # Is the page a redirect? Add it if the redirect link looks like a call
      # sign page.
      content = get_page_content(general_page_info)
      if /#REDIRECT/.match(content)
        # The first link in a redirect page is the page to which it redirects.
        redirect_page_title = get_page_link_titles(general_page_info, limit: 1)
          .first

        if starts_with_call_sign_regex.match?(redirect_page_title)
          promising_page_titles << redirect_page_title
        end
      end
    end

    # Next, add links that look like they point at call sign page titles.
    #
    # This "pattern" is indicative of disambiguation pages that link off to
    # other TV stations, e.g. because the call sign overlaps with some other
    # thing that uses the same four general call sign letters.
    #
    # Luckily for us, it seems that most (all?) television station pages have
    # titles that start with their general call signs!
    general_query_page_info_responses.each do |general_page_info|
      page_link_titles = get_page_link_titles(general_page_info)
      page_link_titles.each do |title|
        if starts_with_call_sign_regex.match?(title)
          promising_page_titles << title
        end
      end
    end

    # Remove any duplicate page titles so we don't waste our time re-fetching
    # and re-parsing duplicate responses.
    promising_page_titles.uniq!

    # Try each page title one-by-one until (if...) we find a page that matches
    # our requirements.
    #
    # For an example request for a particular page, see
    # https://en.wikipedia.org/w/api.php?action=query&format=json&prop=revisions&generator=allpages&formatversion=2&rvprop=content&rvslots=main&gapfrom=KADT-LD&gapto=KADT-LD&gaplimit=1
    promising_page_titles.each do |page_title|
      potential_page_info_response = fetch(
        WIKIPEDIA_API_HOST,

        query: {
          format: :json,
          formatversion: 2,

          action: :query,

          prop: 'revisions',
          generator: :allpages,

          rvprop: :content,
          rvslots: :main,

          # Look for one page with the exact title we're looking for.
          gapfrom: page_title,
          gapto: page_title,
          gaplimit: 1,
        },

        cache: cache,
        ttl_seconds: ttl_seconds,
      )

      # If we got no response at all, we're out of luck.
      potential_page_info = potential_page_info_response.dig(:query, :pages, 0)
      next if potential_page_info.nil?

      # If it's not a TV station page, keep looking.
      next unless does_page_contain_tv_affiliation_info(potential_page_info)

      # Otherwise, we found our page!
      page_info = potential_page_info
      break
    end
  end

  # If we _still_ didn't find what we're looking for, there's no hope left.
  return nil if page_info.nil?

  # Grab the Wikitext markup from the first (and only) revision that should have
  # been returned; we'll parse this to obtain our data.
  page_wikitext = get_page_content(page_info)
  return nil if page_wikitext.nil?

  parse_station_affiliations(page_wikitext)
end

# Given some Wikitext markup, attempts to parse out TV station affiliations
# section token list, returning `nil` if unable to do so.
def parse_station_affiliations(
  wikitext # String, in Wikitext markup format
) # Hash<String station number, String station name>
  text_tokens = [
    token_template_start = '{{',
    token_template_end = '}}',
    token_link_start = '[[',
    token_link_end = ']]',
    token_bold = "'''",
    token_equals = '=',
    token_pipe = '|',
  ]

  # A HTML/XML tag, e.g. `<ref>`.
  regexp_tokens = [
    token_start_tag = %r[<\s*[^/]\s*[^>]+?\s*>],
    token_end_tag = %r[<\s*/\s*[^>]+?\s*>],
  ]

  # This regex groups by the tokens we're looking for, which means when we split
  # the input text we'll receive an array that contains not only the parts
  # _between_ the tokens, but also the tokens themselves. E.g. if we split 'axb'
  # by 'x', we'll get `['a', 'x', 'b']` instead of `['a', 'b']`.
  tokenizer = %r[
    (
      # Turn our token list into something like `a|b|c`
      #{text_tokens.map { |t| Regexp.escape(t) }.join('|')}
      | #{regexp_tokens.join('|')}
    )
  ]x

  # Create a token stream from our Wikitext.
  token_stream = wikitext
    .split(tokenizer)
    .lazy
    .map(&:strip)
    .reject(&:empty?)

  # We're looking for text much like the following:
  #
  # ```wikitext
  # {{Infobox television station
  # ...
  # | affiliations = ...
  # ...
  # }}
  #
  # To obtain it, we have to essentially parse the token stream to find the
  # overall template, then parse the text contained within the attribute we care
  # about to turn it into channel numbers mapped to the corresponding names,
  # unwrapping links and bold text as necessary.
  #
  # Luckily, we can ignore _most_ of what's going on and just look for the
  # particular sequences we care about...
  # ```
  template_start_pattern = [
    token_template_start,
    WIKIPEDIA_TV_STATION_AFFILIATION_TEMPLATE_TITLE,
  ].freeze
  found_tv_template = false

  affiliations_start_pattern = [
    token_pipe,
    'affiliations',
    token_equals,
  ].freeze
  found_affiliations = false

  seen_tokens = []
  depth = 0
  token_stream.each do |token|
    seen_tokens << token

    # Look for the beginning of our station info template.
    if (
        !found_tv_template &&
        seen_tokens[-template_start_pattern.length..] == template_start_pattern
    )
      # Throw everything else away since at this point we only care about
      # looking for the next pattern.
      seen_tokens.clear
      found_tv_template = true
      next
    end

    # Find the affiliations section once we're inside the station info template.
    if (
        found_tv_template &&
        !found_affiliations &&
        seen_tokens[-affiliations_start_pattern.length..] == affiliations_start_pattern
    )
      seen_tokens.clear
      found_affiliations = true
      next
    end

    # Once we're inside the affiliations list, we do proper stack-based parsing
    # and consume tokens until our depth is back to zero again, at which point
    # we re-join the contents of the stack and return the result.
    if found_affiliations
      # Once we've returned to zero depth and found the end of the affiliations
      # section (either the end of the parent template or we've encountered the
      # next template attribute), all the tokens we've seen make up the contents
      # of the affilations attribute and we can stop parsing.
      if depth.zero? && [token_pipe, token_template_end].include?(token)
        # Discard the token we just matched since it has no matching "open"
        # variant since we cleared the stack when we entered the affiliations
        # attribute.
        seen_tokens.pop
        break
      end

      # Otherwise, track the current AST depth so we'll know when to stop
      # parsing the affiliations attribute.
      case token
        when token_template_start
          depth += 1
        when token_link_start
          depth += 1

        when token_template_end
          depth -= 1
        when token_link_end
          depth -= 1
      end
    end
  end

  # If we never found the affiliations attribute, we failed to parse the input.
  return nil unless found_affiliations

  # At this point, we've contructed a list of tokens that make up the "value" of
  # the affiliations expression in the template we were looking for. We can now
  # parse _this_ list and turn it into the channel numbers/names we're looking
  # for.
  affiliation_token_stream = seen_tokens.dup

  # We expect most affiliations to come in the form of an "unbulleted list"
  # (https://en.wikipedia.org/wiki/Template:Unbulleted_list). We parse out the
  # leading pipe so that from here on, every time we see a pipe we can take all
  # the tokens we've accumulated and turn them into a human-readable name and
  # channel number.
  ubl_template_start_pattern = [
    token_template_start,
    'ubl',
    token_pipe,
  ].freeze

  # Sometimes, we get a "manually-constructed list" that was created with `<br>`
  # and/or `<br />` tags instead of a "fancy" list created with the `ubl`
  # template.
  #
  # When we see a manually-constructed list, we convert it into a `ubl` template
  # so we can parse just the single format later.
  br_tag_token = %r[^<\s*br\s*/?\s*>$]
  if affiliation_token_stream.find { |t| br_tag_token.match?(t) }
    affiliation_token_stream = [
      *ubl_template_start_pattern,

      # Replace all the `<br>` tags with pipes since the tags are effectively
      # acting as list item separators.
      *affiliation_token_stream.map do |token|
        if br_tag_token.match?(token)
          token_pipe
        else
          token
        end
      end,

      token_template_end,
    ]
  end

  # TODO:
  # Some _other_ times, we get a single affiliation with no channel number, e.g.
  # for `KADT`, and basically nothing else. In this case, convert it into a
  # single-item `ubl` template with the "special" channel number `*` to
  # represent the only affiliation we got.

  if affiliation_token_stream[0...ubl_template_start_pattern.length] != ubl_template_start_pattern
    return nil
  else
    # Strip off the template start tokens since we don't need them.
    ubl_template_start_pattern.length.times { affiliation_token_stream.shift }
  end

  raw_affiliations = []

  found_link = false
  found_tag = false
  affiliation_tokens = []
  affiliation_token_stream.each do |token|
    affiliation_tokens << token

    # Skip bold text markers entirely, effectively un-bolding all bolded text.
    if token == token_bold
      affiliation_tokens.pop
      next
    end

    # Skip all tags and their contents. Typically, these are `ref` tags and we
    # don't care about them.
    if !found_tag && token_start_tag.match?(token)
      affiliation_tokens.pop
      found_tag = true
      next
    end

    # Discard all tokens until we find a matching end tag.
    if found_tag
      affiliation_tokens.pop

      if token_end_tag.match?(token)
        found_tag = false
      end

      next
    end

    if !found_link && token == token_link_start
      found_link = true
      next
    end

    # "Unwrap" links to obtain only their text, if any. See
    # https://en.wikipedia.org/wiki/Help:Wikitext#Links_and_URLs for more
    # information on Wikitext link syntax.
    if found_link && token == token_link_end
      # Rewind the token stream to the start of the link, consuming all the
      # tokens for specific processing here.
      link_text_tokens = []
      until link_text_tokens.last == token_link_start || affiliation_tokens.empty? do
        link_text_tokens << affiliation_tokens.pop
      end

      # Reverse the tokens we just collected them in reverse order to obtain the
      # "natural" order for further processing.
      link_text_tokens.reverse!

      # Strip off the leading and trailing link markers. If we didn't find the
      # appropriate tokens there, explode since something is wrong.
      first_link_token = link_text_tokens.shift
      last_link_token = link_text_tokens.pop
      if first_link_token != token_link_start && last_link_token != token_link_end
        raise IOError, 'Encountered invalid Wikitext link'
      end

      # "Unwrap" the link to contain only its actual text.
      #
      # These are most of the potential Wikitext link formats:
      # ```wikitext
      # [[standard link]]
      # [[link|renamed link]]
      # [[auto-renamed link|]]
      # [[blended link]]ing
      # [[Wikipedia:Page link#To a section]]
      # [[it:Otra lingua]]
      # [[Wiktionary:intra-wiki link]]
      # ```
      #
      # In practice, we hope to only see standard and renamed links here, though
      # technically any of these is possible.
      if link_text_tokens.include?(token_pipe)
        unpiped_tokens = []
        found_pipe = false
        link_text_tokens.each do |token|
          unpiped_tokens << token

          if !found_pipe && token == token_pipe
            unpiped_tokens.clear
            found_pipe = true
          end
        end

        # If we found no tokens after the pipe, simply remove the pipe from the
        # original tokens and treat the remaining tokens as the text of the
        # link. This happens with auto-renamed links, which look like
        # `[[Foo|]]`; in this case, we use the standard link text before the
        # pipe.
        if unpiped_tokens.empty?
          link_text_tokens.reject! { |t| t == token_pipe }
        else
          link_text_tokens = unpiped_tokens
        end
      end

      # Add the unwrapped link text tokens back to the original token stream.
      affiliation_tokens.concat(link_text_tokens)
      found_link = false
      next
    end

    # If we find a bare pipe token or the end of the `ubl` template, this
    # indicates we've completed an item and can output it for later processing.
    if !found_link && token == token_pipe || token == token_template_end
      affiliation_tokens.pop
      raw_affiliations << affiliation_tokens.join
      affiliation_tokens.clear
    end
  end

  # We should have consumed _all_ the affiliation tokens since they should have
  # been only a single `ubl` template instance. If they weren't we have no idea
  # what the affiliations contained and are unlikely to be able to parse out
  # anything useful from our resulting text affiliations.
  return nil unless affiliation_tokens.empty?

  # Turn the text affiliations into channel numbers and channel names.
  raw_affiliations.inject({}) do |all, raw_affiliation|
    # We expect the text we have at this point to look like:
    # ```
    # 1.2: Channel name (maybe some text in parenthesis)
    # ```

    # Split into the "channel" and "name" parts we expect.
    channel, raw_name = raw_affiliation
      .split(':', 2)
      .map(&:strip)
      .reject(&:empty?)

    # This channel/name didn't conform to our expected format.
    next all if channel.nil? || raw_name.nil?

    # Strip all parenthesized text from the channel name since it's usually
    # something like `(O&O)` for "owned and operated", which is irrelevant for
    # our purposes.
    name = raw_name
      .gsub(/\([^)]+\)/, '')
      .strip

    # If the name was _only_ parenthesized, it'd be a very odd name indeed!
    next all if name.empty?

    all[channel] = name
    all
  end
end

# Downloads and returns lineup (i.e. provider) data for the given country and
# postal code.
def fetch_providers(
  country, # :USA | :CAN
  postal_code, # String, e.g. `12345` or `A1A 1A1`
  cache: OpenStructCache.new # OpenStructCache
) # Array<Provider>
  provider_json = fetch(
    ZAP2IT_API_HOST,
    '/gapzap_webapi/api/Providers/getPostalCodeProviders',
    country,
    postal_code,
    '/gapzap/en',

    # This response is expected to change very infrequently, if ever.
    cache: cache,
    ttl_seconds: 1 * TimeInterval::WEEKS,
  )

  provider_json
    .Providers
    .sort_by { |p| [p.type, p.name] } # A human-friendly sort for display
    .map do |p|
      Provider.new(
        country: country,
        id: p.headendId,
        name: p.name,
        postal_code: postal_code,
        type: p.type,
      )
    end
end

# Fetch overall provider info and return the provider that corresponds to the
# given parameters. If no provider is found, returns `nil`.
def fetch_provider(
  country, # :USA | :CAN
  postal_code, # String
  provider_id, # String
  cache: OpenStructCache.new # OpenStructCache
) # Provider | nil
  providers = fetch_providers(
    country,
    postal_code,
    cache: cache,
  )

  providers.find do |provider|
    (
      provider.country == country &&
      provider.postal_code == postal_code &&
      provider.id == provider_id
    )
  end
end

# Downloads and returns the channels available for the given provider at the
# given time/time span, potentially limiting them to some list of channels.
def fetch_channels(
  provider, # Provider
  date = Time.now, # Time, but will be truncated to its date component
  time_span_hours = 6, # 1 | 2 | 3 | 4 | 5 | 6
  channel_tokens: Set.new, # Enumerable<String>. If non-empty, channels to fetch
  cache: OpenStructCache.new # OpenStructCache
) # Array<Channel>
  # We need fast random access to these.
  channel_tokens = channel_tokens.to_set

  # When fetching channels, we really only care about "right now" and don't
  # expect the values to change much over time. However, we cache the response
  # for a while anyway.
  ttl_seconds = 1 * TimeInterval::DAYS

  lineup_json = fetch(
    ZAP2IT_API_HOST,
    '/api/grid',
    query: {
      # Unused since we give `headendId`, but apparently necessary anyway!
      lineupId: '-',

      # Provider info
      headendId: provider.id,
      country: provider.country,
      postalCode: provider.postal_code,

      # Time info, truncated to "today" for caching purposes.
      timespan: time_span_hours,
      time: truncate_time(date, ttl_seconds).to_i, # Needs to be Unix time, apparently
    },

    cache: cache,
    ttl_seconds: ttl_seconds,
  )

  # We sort by channel number for a human-friendly display and limit our
  # channels to the ones requested.
  raw_channels = lineup_json
    .channels
    .sort_by { |c| [(Float(c.channelNo) rescue c.channelNo), c.callSign] }
    .select do |c|
      (
        # If we weren't given any channels to which to limit our fetch, keep all
        # channels.
        channel_tokens.empty? ||

        # Otherwise, keep only channels that have an identifier that matches one
        # of the given tokens.
        channel_tokens.include?(c.channelId) ||
        channel_tokens.include?(c.callSign) ||
        channel_tokens.include?(c.channelNo)
      )
    end

  raw_channels.map do |c|
    affiliates = fetch_station_affiliate_info(
      provider.country,
      c.callSign,
      cache: cache,
    ) || {}

    Channel.new(
      provider: provider,
      call_sign: c.callSign,
      name: affiliates[c.channelNo],
      id: c.channelId, # TODO: Should this be `id` instead? What's the difference?
      number: c.channelNo,
      image_url: c.thumbnail,
    )
  end
end

# Fetch overall channel info and return the channel that corresponds to the
# given token. If no channel is found, returns `nil`.
def fetch_channel(
  provider, # Provider
  token, # String, one of a channel call sign, number, or id
  date = Time.now, # Time, but will be truncated to its date component
  time_span_hours = 6, # 1 | 2 | 3 | 4 | 5 | 6
  cache: OpenStructCache.new # OpenStructCache
) # Channel | nil
  channels = fetch_channels(
    provider,
    date,
    time_span_hours,
    cache: cache
  )

  channels.find do |channel|
    (
      channel.call_sign == token ||
      channel.id == token ||
      channel.number == token
    )
  end
end

# Downloads and returns all the data for the given date, if possible.
def fetch_lineup(
  channel, # Channel
  start_date = Time.now, # Time, but will be truncated to its data component
  days_to_fetch: 999_999_999, # The maximum number of days of data to fetch
  cache: OpenStructCache.new # OpenStructCache
)
  raise 'TODO: Implement fetch_lineup'
end

# Formats the given data nicely for display in the terminal as a table and
# returns the result.
#
# No more columns than those given will be displayed. If any row contains too
# few values for the given columns, empty strings will be displayed in the
# "holes".
def display_table(
  columns, # Array<Symbol | String>
  rows, # Array<Tuple<any>>
  column_padding: 2 # Integer, the space between columns
) # String
  # Turn everything into a string and resize all the rows to be no longer than
  # the number of columns. Any "missing" columns will be omitted from the output
  # entirely.
  max_column_value_lengths = columns.map { 0 }
  prepared_rows = [columns, *rows].map do |row|
     row
       .slice(0, columns.length) # Truncate to column length
       .map(&:to_s) # Stringify contents
       .each_with_index do |value, column_index| # Update max value size
         value_length = value.length
         if max_column_value_lengths[column_index] < value_length
           max_column_value_lengths[column_index] = value_length
         end
       end
  end

  column_padder = ' ' * column_padding
  prepared_rows.each_with_index.map do |row, index|
    row
      .each_with_index
      .map { |r, i| r.ljust(max_column_value_lengths[i]) }
      .join(column_padder)
      .strip
  end.join("\n")
end

# Returns a string for terminal output representing provider information.
def display_providers(
  country, # :USA | :CAN
  postal_code, # String
  cache: OpenStructCache.new # OpenStructCache
) # String
  providers = fetch_providers(
    country,
    postal_code,
    cache: cache
  )

  display_table(
    [:NAME, :TYPE, :ID],
    providers.map { |p| [p.name, p.type, p.id] },
  )
end

# Returns a string for terminal output representing the channel list for the
# given provider.
def display_channels(
  provider, # Provider
  channel_tokens: Set.new, # Enumerable<String> | nil. If non-empty, channels to display
  cache: OpenStructCache.new # OpenStructCache
) # String
  channels = fetch_channels(
    provider,
    channel_tokens: channel_tokens,
    cache: cache,
  )

  display_table(
    [:'CALL_SIGN', :NUMBER, :NAME, :ID],
    channels.map { |c| [c.call_sign, c.number, c.name, c.id] },
  )
end

def main!
  config = parse_config_from_args(ARGV).freeze

  $output = config.output_file
  cache = config.cache

  # Output program help text.
  #
  # This is handled as the very first "mode" since it's the most important part
  # of any command; its presence anywhere in the arguments indicates the user
  # doesn't know what to do and needs our guidance!
  if config.return_help_text?
    $output.puts(config.option_parser.help())
    return
  end

  # Output program version information.
  if config.return_version_info?
    $output.puts(VERSION_INFO)
    return
  end

  # Output our XMLTV description.
  if config.return_description?
    $output.puts(XMLTV_DESCRIPTION)
    return
  end

  # Output our XMLTV capabilities.
  if config.return_capabilities?
    XMLTV_CAPABILITIES.each do |capability|
      $output.puts(capability)
    end
    return
  end

  # Download and print out the configured lineup information.
  if config.return_providers?
    country = config.require(:country)
    postal_code = config.require(:postal_code)

    output = display_providers(
      country,
      postal_code,
      cache: cache,
    )
    $output.puts(output)
    return
  end

  # Download and print out a listing of the channels for the given provider
  # information.
  if config.return_channels?
    country = config.require(:country)
    postal_code = config.require(:postal_code)
    provider_id = config.require(:provider_id)

    provider = fetch_provider(
      country,
      postal_code,
      provider_id,
      cache: cache,
    )

    if provider.nil?
      raise ArgumentError, "No provider found for country #{country}, postal code #{postal_code}, and id #{provider_id}"
    end

    output = display_channels(
      provider,
      channel_tokens: config.channel_tokens,
      cache: cache,
    )
    $output.puts(output)
    return
  end
ensure
  # Always attempt to persist the cache once we're done.
  #
  # We don't have to worry about some partially-valid state since the cache
  # doesn't store entries that generate an exception during population, meaning
  # that whatever _does_ end up in the cache was at least valid enough to have
  # been generated by the code.
  cache.persist!
end

if __FILE__ == $0
  main!
end
