#!/usr/bin/env ruby
# frozen_string_literal: true
require 'date'
require 'digest'
require 'json'
require 'logger'
require 'net/http'
require 'optparse'
require 'ostruct'
require 'pathname'
require 'set'
require 'uri'
require 'zlib'

# TODO:
# * Allow mapping channel call sign/number combos to custom names, which take
#   preference over any that are auto-detected

# Returns the SHA-512 digest of the given stringified values.
def digest(
  *values, # Array<any>
  format: :hex, # :hex | :base64 | :bubblebabble
  length: 999_999_999 # The maximum length at which to truncate the resulting digest
)
  d = Digest::SHA2.new(512)
  values.each { |v| d.update(v.to_s) }

  hash =
    case format
      when :hex then d.hexdigest()
      when :base64 then d.base64digest()
      when :bubblebabble then d.bubblebabble()
      else
        raise ArgumentError, "Invalid format: #{format.inspect}"
    end

  hash[0...length]
end

PROGRAM_NAME = 'tv_grab_zap2it'
VERSION = Gem::Version.new('0.0.1').freeze
RELEASE =
  begin
    # Compute a truncated SHA-512 digest of the current file and use this as our
    # release identifier. Since the entire program is in a single file, this is
    # guaranteed to provide a unique and stable value for each "build".
    file_data = File.read(Pathname.new(__FILE__).realpath)

    digest(file_data, format: :hex, length: 8)
  end

# Compile the version information into a single string for easy use elsewhere.
VERSION_INFO = "#{PROGRAM_NAME} v#{VERSION} (#{RELEASE})"

# The number of seconds to wait for any given HTTP request to finish.
HTTP_REQUEST_TIMEOUT_SECONDS = 60

XMLTV_DESCRIPTION = 'USA/Canada (zap2it.com)'
XMLTV_CAPABILITIES = %w[baseline cache].freeze

ZAP2IT_API_HOST = 'https://tvlistings.zap2it.com'
WIKIPEDIA_API_HOST = 'https://en.wikipedia.org/w/api.php'

# The largest number of days you're allowed to fetch from the API. Apparently,
# also the _only_ number of days you're allowed to fetch...
ZAP2IT_MAXIMUM_DAYS_TO_FETCH = 14

# Rate limit ourselves to this many fetches per second globally so we don't
# overload the local networks or any of the remote APIs we're using.
FETCH_MIN_SECONDS_BETWEEN_REQUESTS = 0.5

# A map of know rating values to their corresponding pictographic icons. The
# icons are sourced from
# https://en.wikipedia.org/wiki/TV_Parental_Guidelines#Ratings.
RATINGS_TO_ICON_URLS =
  begin
    ratings_to_icon_urls = {
      'Y' => 'https://upload.wikimedia.org/wikipedia/commons/thumb/2/25/TV-Y_icon.svg/250px-TV-Y_icon.svg.png',
      'Y7' => 'https://upload.wikimedia.org/wikipedia/commons/thumb/5/5a/TV-Y7_icon.svg/250px-TV-Y7_icon.svg.png',
      'G' => 'https://upload.wikimedia.org/wikipedia/commons/thumb/5/5e/TV-G_icon.svg/250px-TV-G_icon.svg.png',
      'PG' => 'https://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/TV-PG_icon.svg/250px-TV-PG_icon.svg.png',
      '14' => 'https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/TV-14_icon.svg/250px-TV-14_icon.svg.png',
      'MA' => 'https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/TV-MA_icon.svg/250px-TV-MA_icon.svg.png',

      # This is the same as `14`, but we get it in this form instead of just a
      # literal number when no `TV-` prefix is present.
      '14+' => 'https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/TV-14_icon.svg/250px-TV-14_icon.svg.png',
    }

    # Also prefix the ratings with `TV-` since they come to us in both prefixed
    # and "naked" forms.
    ratings_to_icon_urls.to_a.each do |(rating, url)|
      ratings_to_icon_urls["TV-#{rating}"] = url
    end

    ratings_to_icon_urls.freeze
  end

# The name of the Wikipedia "template" that contains the information we're
# looking for. This is basically a built-in directive for displaying information
# about something in a standardized way, in our case a television station.
#
# See https://en.wikipedia.org/wiki/Template:Infobox_television_station for the
# template's documentation.
WIKIPEDIA_TV_STATION_INFOBOX_TEMPLATE_TITLE = 'Infobox television station'

# All the tokens we care about for use with parsing Wikitext. We don't do a
# terribly great job at parsing, mostly only caring about the things represented
# here in a lightweight way.
WIKITEXT_TOKENS = [
  # A template looks like `{{template_name|arg1 = foo|arg2 = bar}}` and is
  # basically a function for generating other text.
  #
  # See
  # https://en.wikipedia.org/wiki/Help:Wikitext#Templates_and_transcluding_pages
  # for more information.
  WIKITEXT_TOKEN_TEMPLATE_START = '{{',
  WIKITEXT_TOKEN_TEMPLATE_END = '}}',

  # These are... something. I'm not sure what they're _actually_ called, but
  # they appear to be template-like, just with a different syntax.
  WIKITEXT_TOKEN_CLASS_START = '{|',
  WIKITEXT_TOKEN_CLASS_END = '|}',

  # A link forms an edge in the graph that is Wikipedia. It typically looks like
  # `[[Some page title]]` or `[[Some page title|Some human-readable page
  # title]]`.
  #
  # See https://en.wikipedia.org/wiki/Help:Wikitext#Links_and_URLs for
  # more information.
  WIKITEXT_TOKEN_LINK_START = '[[',
  WIKITEXT_TOKEN_LINK_END = ']]',

  # Makes text appear bolded.
  #
  # See https://en.wikipedia.org/wiki/Help:Wikitext#Text_formatting for more
  # information.
  WIKITEXT_TOKEN_BOLD = "'''",

  # Generic tokens used as part of other things, like templates and links.
  WIKITEXT_TOKEN_EQUALS = '=',
  WIKITEXT_TOKEN_PIPE = '|',
].freeze

# A HTML/XML tag, e.g. `<ref>`.
WIKITEXT_REGEXP_TOKENS = [
  WIKITEXT_TOKEN_START_TAG = %r[<\s*[^/]\s*[^>]+?\s*>].freeze,
  WIKITEXT_TOKEN_END_TAG = %r[<\s*/\s*[^>]+?\s*>].freeze,
].freeze

# A regexp that matches any one Wikitext token as its first group, useful with
# `String#split` to obtain tokens and "regular" text.
WIKITEXT_TOKENIZER = %r[
  (
    # Turn our token list into something like `a|b|c?d`
    #{WIKITEXT_TOKENS.map { |t| Regexp.escape(t) }.join('|')}
    | #{WIKITEXT_REGEXP_TOKENS.join('|')}
  )
]x.freeze

# Useful time constants for obtaining a number of seconds via multiplication.
class TimeInterval
  SECONDS = 1
  MINUTES = 60 * SECONDS
  HOURS = 60 * MINUTES
  DAYS = 24 * HOURS
  WEEKS = 7 * DAYS
end.freeze

class Color
  BLACK = "\e[30m"
  RED = "\e[31m"
  GREEN = "\e[32m"
  BROWN = "\e[33m"
  BLUE = "\e[34m"
  MAGENTA = "\e[35m"
  CYAN = "\e[36m"
  WHITE = "\e[37m"

  RESET = "\e[0m"
end.freeze

# Used to extract information about a method from an entry in the call stack.
CALLER_INFO_REGEXP = /:(?<line_number>\d+):in\s+\`(?<method_name>[^']+)'$/

# Create the global logger for simple use from all places in the program. If
# this is configured further, we do so in `#main!`.
LOG = Logger.new(
  # Always log to STDERR so we don't mess with anything we're otherwise writing
  # to STDOUT. If someone wants to log to a different file, they can redirect
  # our error output to it themselves!
  $stderr,

  # The level we pick here isn't of consequence since we'll override it
  # immediately on running `#main!` with whatever the user configured, or with
  # the default option we selected when generating the config.
  level: :error,

  # Enable nice custom formatting for useful output.
  formatter: Proc.new do |severity, datetime, program_name, message|
    output_parts = []

    # Only include "fancy" output information at the more verbose log levels
    # since it's expensive to compute.
    if LOG.info?
      severity_letter, severity_color =
        case severity
          when 'DEBUG' then ['D', Color::CYAN]
          when 'INFO' then ['I', Color::WHITE]
          when 'WARN' then ['W', Color::BROWN]
          when 'ERROR' then ['E', Color::RED]
          when 'FATAL' then ['F', Color::MAGENTA]
          when 'UNKNOWN' then ['U', Color::BLUE]
        end

      output_parts << "#{severity_color}#{severity_letter}#{Color::RESET}"
      output_parts << datetime.strftime('%Y%m%d_%H%M%S.%5N')

      log_call_stack = caller(4)
      if caller_info = CALLER_INFO_REGEXP.match(log_call_stack.first)
        output_parts << "#{caller_info[:method_name]}@#{caller_info[:line_number]}"
      end

      output_parts << '--'
    end

    output_parts << message

    output_parts
      .compact
      .join(' ') + "\n"
  end,
)

# A builder-style class for parsing and validating arguments in a standardized
# way.
class Arg
  attr_reader \
    :name,
    :parsers,
    :value

  def initialize(name, parsers, value)
    @name = name
    @parsers = parsers
    @value = value
  end

  # Create a new builder with the given argument name and nothing else.
  def self.name(
    name # String
  )
    unless name.is_a?(String) && !name.empty?
      raise ArgumentError, 'name must be a non-empty string'
    end

    Arg.new(name.strip, [], nil)
  end

  # Add a parser and return a new `Arg` that inherits its other values, but has
  # another parser added to its list.
  def parser(
    &block # (value: typeof prior parser) => any
  )
    Arg.new(
      @name,
      @parsers + [block],
      @value,
    )
  end

  # Ensures that the passed value is an instance of at least one of the given
  # classes using `#is_a?`.
  def is_a(first_class, *other_classes)
    classes = [first_class, *other_classes].freeze
    self.parser do |value|
      value_is_a = classes.any? do |potential_class|
        value.is_a?(potential_class)
      end

      unless value_is_a
        class_names = classes.map(&:name).join(', ')

        # Don't add "one of" if there's only a single class to check.
        one_of_text =
          if classes.size < 2
            ''
          else
            'one of '
          end

        raise ArgumentError, "{{ name }} must be an instance of #{one_of_text}#{class_names}, but was an instance of #{value.class.name}"
      end

      value
    end
  end

  # Ensures the passed value is one of the given values.
  def one_of(potential_value, *other_values)
    potential_values = [potential_value, *other_values].freeze
    self.parser do |value|
      unless potential_values.include?(value)
        values_text = potential_values.map(&:to_s).join(', ')

        # Don't add "one of" if there's only a single value to check.
        one_of_text =
          if potential_values.empty?
            ' '
          else
            'one of '
          end

        raise ArgumentError, "{{ name }} was expected to be #{one_of_text}#{values_text}, but was #{value}"
      end

      value
    end
  end

  def non_empty
    self.parser do |value|
      if value.empty?
        raise ArgumentError, "{{ name }} must not be empty"
      end

      value
    end
  end

  # Validates that a value is non-`nil` and returns that value.
  def non_nil
    self.parser do |value|
      if value.nil?
        raise ArgumentError, "{{ name }} is required to be non-`nil`"
      end

      value
    end
  end

  def strip
    self.is_a(String).parser do |value|
      value.strip
    end
  end

  def substitute(
    regexp, # Regexp
    substitution # String
  )
    self.is_a(String).parser do |value|
      value.gsub(regexp, substitution)
    end
  end

  def downcase
    self.is_a(String).parser do |value|
      value.downcase
    end
  end

  def upcase
    self.is_a(String).parser do |value|
      value.upcase
    end
  end

  def symbolize
    self.parser do |value|
      value.to_sym
    end
  end

  def int
    self.is_a(String).parser do |value|
      begin
        Integer(value, 10)
      rescue ArgumentError => err
        raise ArgumentError, "{{ name }} of #{value.inspect} is not a valid integer: #{err}"
      end
    end
  end

  def time
    self.is_a(Integer).parser do |value|
      Time.at(value).getutc
    end
  end

  def min(
    n # Integer
  )
    self.parser do |value|
      if value < n
        raise ArgumentError, "{{ name }} of #{value.inspect} must not be less than #{n}"
      end

      value
    end
  end

  def max(
    n # Integer
  )
    self.parser do |value|
      if value > n
        raise ArgumentError, "{{ name }} of #{value.inspect} must not be greater than #{n}"
      end

      value
    end
  end

  def path_name
    self.parser do |value|
      Pathname
        .new(value)
        .cleanpath
        .expand_path
    end
  end

  def exists
    self.parser.path_name do |value|
      unless value.exist?
        raise ArgumentError, "{{ name }} of #{value} does not exist"
      end
    end
  end

  # Opens an IO stream from the given path name.
  def stream(
    mode = 'r' # https://ruby-doc.org/core-2.7.2/IO.html#method-c-new-label-IO+Open+Mode
  )
    self.path_name.parser do |path|
      fd = path.sysopen(mode)
      IO.open(fd, mode)
    end
  end

  def uri
    self.parser do |value|
      URI.parse(value)
    rescue URI::InvalidURIError => err
      raise ArgumentError, "{{ name }} of #{value.inspect} is not a valid URI: #{err}"
    end
  end

  # Thread our value through each configured parser in turn, passing the result
  # of each parser/validator through to the next. Returns the output of the
  # final parser given and sets `.value` to the same.
  #
  # Converts all raised errors into `ArgumentError` instances and re-raises
  # them.
  def parse(
    value # any
  )
    begin
      @value = @parsers.inject(value) do |current_value, parser|
        parser.call(current_value)
      end
    rescue StandardError => err
      new_err_class = ArgumentError

      # Replace any occurence of `{{ name }}` with our configured name to make
      # the errors prettier.
      message = err.message.gsub(/\{\{\s*name\s*\}\}/i, @name)

      # Preserve the original error class name if it's not the one we're
      # "casting" to.
      if err.class.name != new_err_class.name
        message = "#{err.class.name}: #{message}"
      end

      raise new_err_class, message
    end
  end
end

# A simple XML tag container that can form a tree and be printed to a string.
# This handles correct escaping of stringified values, so provide everything "as
# is" and allow this class to handle all escaping for you.
class Xml
  attr_reader \
    :name,
    :attributes,
    :children

  def initialize(
    name, # Symbol | String, with `_` converted to `-` for Symbols
    attributes = {}, # Hash<Symbol | String, any>
    *children # Array<Xml | String | nil>, `nil` and empty string values ommitted
  )
    # We convert `_` to `-` for symbols since writing symbols with hyphens in
    # their names is tedious.
    @name =
      if name.is_a?(Symbol)
        hyphenize_symbol(name)
      else
        name.to_sym
      end

    @attributes = attributes
    @children = children
      .compact
      .reject { |n| n.is_a?(String) && n.empty? }
  end

  # Print this XML tag and all its children to a correctly-escaped and formatted
  # string, something suitable for writing to a file.
  def to_s(
    depth: 0, # Integer
    prev_thing: nil # :tag | :text | nil (`nil` only for very first element)
  )
    attrs_string = @attributes
      .map do |(name, val)|
        name =
          if name.is_a?(Symbol)
            hyphenize_symbol(name)
          else
            name
          end
        " #{name}=\"#{escape_attribute_value(val.to_s)}\""
      end
      .join

    indent = "\t" * depth

    # There's no need to output the children if there are none!
    is_self_closing = @children.count.zero?
    if is_self_closing
      tag = "<#{@name}#{attrs_string} />"
      if prev_thing == :tag
        return "\n" + indent + tag
      else
        return tag
      end
    end

    # Now, we assemble the output string appropriately.
    parts = []

    if prev_thing == :tag
      parts << "\n"
      parts << indent
    end

    # Add the opening tag, which is a `:tag` for our purposes.
    parts << "<#{@name}#{attrs_string}>"
    prev_thing = :tag

    @children.each do |child|
      next if child.nil?

      if child.is_a?(String)
        parts << escape_text(child)
        prev_thing = :text
      elsif child.is_a?(Xml)
        parts << child.to_s(depth: depth + 1, prev_thing: prev_thing)
        prev_thing = :tag
      else
        raise ArgumentError, "Unknown XML child type: #{child.class}"
      end
    end

    if prev_thing == :tag
      parts << "\n"
      parts << indent
    end

    parts << "</#{@name}>"

    parts.join
  end

  private

  def hyphenize_symbol(
    sym # Symbol
  ) # Symbol
    sym
      .to_s
      .gsub(/_/, '-')
      .to_sym
  end

  # See https://stackoverflow.com/a/1091953 for information about escaping
  # XML strings.
  def escape_text(
    text # String
  ) # String
    text
      .gsub(/&/, '&amp;') # Must come first to prevent double-escaping others!
      .gsub(/</, '&lt;')
  end

  # We print all attributes surrounded by `"`, so we don't escape `'` within
  # attribute text.
  def escape_attribute_value(
    value # String
  ) # String
    value
      .gsub(/&/, '&amp;') # Must come first to prevent double-escaping others!
      .gsub(/"/, '&quot;')
      .gsub(/</, '&lt;')
  end
end

# A provider, something that has a "lineup" associated with it. Typically this
# is something like "Local over-the-air stations" or "AT&T Uverse Cable" or
# something of this nature.
#
# These are only unique given a combination of country, postal code, and
# "headend" id!
class Provider
  attr_reader \
    :country,
    :id,
    :postal_code,
    :name,
    :type

  def initialize(
    country: nil, # Symbol
    id: nil, # String, the `headendId` value from the API
    name: nil, # String
    postal_code: nil, # String
    type: nil # String
  )
    @id = Arg
      .name('id')
      .non_nil
      .is_a(String)
      .non_empty
      .parse(id)

    @name = Arg
      .name('name')
      .non_nil
      .is_a(String)
      .non_empty
      .parse(name)

    @postal_code = Arg
      .name('postal_code')
      .non_nil
      .non_empty
      .is_a(String)
      .parse(postal_code)

    @type = Arg
      .name('type')
      .non_nil
      .is_a(String)
      .non_empty
      .downcase
      .symbolize
      .parse(type)

    @country = Arg
      .name('country')
      .one_of(:USA, :CAN)
      .parse(country)
  end

  def to_s
    name_s =
      if name.nil?
        ''
      else
        "#{name}, "
      end

    "#{name_s}#{country} #{postal_code} (#{id})"
  end
end

# A channel in a lineup.
class Channel
  attr_reader \
    :provider,
    :call_sign,
    :name,
    :id,
    :number,
    :image_url

  def initialize(
    provider: nil, # Provider
    call_sign: nil, # String
    name: nil, # String | nil
    id: nil, # String
    number: nil, # String
    image_url: nil # String
  )
    @provider = Arg
      .name('provider')
      .is_a(Provider)
      .parse(provider)

    @call_sign = Arg
      .name('call_sign')
      .non_nil
      .is_a(String)
      .strip
      .non_empty
      .parse(call_sign)

    # Name is optional.
    @name =
      unless name.nil?
        Arg
          .name('name')
          .is_a(String)
          .strip
          .non_empty
          .parse(name)
      end

    @id = Arg
      .name('id')
      .non_nil
      .is_a(String)
      .strip
      .non_empty
      .parse(id)

    @number = Arg
      .name('number')
      .non_nil
      .is_a(String)
      .strip
      .non_empty
      .parse(number)

    @image_url = Arg
      .name('image_url')
      .non_nil
      .is_a(String)
      .strip
      .uri
      .parser do |uri|
        # The URL comes in "naked" and needs to be secure-ified.
        uri.scheme = 'https'

        # The URL _also_ comes in with `w=123`, but if we strip this off then
        # we'll get back the original, full-size picture instead.
        uri.query = nil

        uri
      end
      .parse(image_url)
  end

  # An RFC-2838 id for this channel.
  def xml_id
    # See https://tools.ietf.org/html/rfc2838 for more information about this id
    # format. Since we can't really determine the actual "owner" of the network,
    # we attribute it to zap2it via their internal id instead.
    "#{id}.zap2it.com"
  end

  def to_xml
    Xml.new(:channel, { id: xml_id() }, *[
      Xml.new(:icon, { src: @image_url }),
      @name && Xml.new(:display_name, {}, @name),
      Xml.new(:display_name, {}, "#{@number} #{@call_sign}"),
      Xml.new(:display_name, {}, @call_sign),
      Xml.new(:display_name, {}, @number),
    ])
  end

  def to_s
    "#{number} #{call_sign} (#{id})"
  end
end

# One program/timeslot combination in a channel's lineup.
class Program
  attr_reader \
    :channel,
    :series_id,
    :program_id,
    :start_time,
    :end_time,
    :season,
    :episode,
    :title,
    :secondary_title,
    :description,
    :image_url,
    :previously_shown_time,
    :rating,
    :tags,
    :genres,
    :original_release_date,
    :is_movie,
    :credits

  def initialize(
    channel: nil, # Channel
    series_id: nil, # String
    program_id: nil, # String
    start_time: nil, # Time
    end_time: nil, # Time
    season: nil, # Integer | nil
    episode: nil, # Integer | nil
    title: nil, # String
    secondary_title: nil, # String | nil
    description: nil, # String | nil
    image_url: nil, # String
    previously_shown_time: nil, # String | nil
    rating: nil, # String | nil
    tags: nil, # Array<String>
    genres: nil, # Array<String>
    original_release_date: nil, # Time | nil
    is_movie: nil, # Boolean
    is_new: nil, # Boolean
    is_live: nil, # Boolean
    is_premier: nil, # Boolean
    is_finale: nil, # Boolean
    is_generic: nil, # Boolean
    credits: nil # Array<Credit info hash>
  )
    @channel = Arg
      .name('channel')
      .is_a(Channel)
      .parse(channel)

    @series_id = Arg
      .name('series_id')
      .is_a(String)
      .strip
      .non_empty
      .parse(series_id)

    @program_id = Arg
      .name('program_id')
      .is_a(String)
      .strip
      .non_empty
      .parse(program_id)

    @start_time = Arg
      .name('start_time')
      .is_a(Time)
      .parse(start_time)

    @end_time = Arg
      .name('end_time')
      .is_a(Time)
      .parser do |t|
        unless t > @start_time
          raise ArgumentError, '{{ name }} must be later than start_time'
        end
        t
      end
      .parse(end_time)

    @season =
      unless season.nil?
        Arg
          .name('season')
          .is_a(Integer)
          .min(1)
          .parse(season)
      end

    @episode =
      unless episode.nil?
        Arg
          .name('episode')
          .is_a(Integer)
          .min(0) # We allow zero since this can indicate "special" episodes.
          .parse(episode)
      end

    @title = Arg
      .name('title')
      .is_a(String)
      .strip
      .non_empty
      .parse(title)

    @secondary_title =
      unless secondary_title.nil?
        Arg
          .name('secondary_title')
          .is_a(String)
          .strip
          .non_empty
          .parse(secondary_title)
      end

    @description =
      unless description.nil?
        Arg
          .name('description')
          .is_a(String)
          .strip
          .non_empty
          .parse(description)
      end

    @image_url = Arg
      .name('image_url')
      .non_nil
      .is_a(String)
      .strip
      .uri
      .parse(image_url)

    @previously_shown_time =
      unless previously_shown_time.nil?
        Arg
          .name('previously_shown_time')
          .is_a(Time)
          .parse(previously_shown_time)
      end

    @rating =
      unless rating.nil?
        Arg
          .name('rating')
          .is_a(String)
          .strip
          .non_empty
          .parse(rating)
      end

    @original_release_date =
      unless original_release_date.nil?
        Arg
          .name('original_release_date')
          .is_a(Time)
          .parse(original_release_date)
      end

    @is_movie = Arg
      .name('is_movie')
      .one_of(true, false)
      .parse(is_movie)

    @is_new = Arg
      .name('is_new')
      .one_of(true, false)
      .parse(is_new)

    @is_live = Arg
      .name('is_live')
      .one_of(true, false)
      .parse(is_live)

    @is_premier = Arg
      .name('is_premier')
      .one_of(true, false)
      .parse(is_premier)

    @is_finale = Arg
      .name('is_finale')
      .one_of(true, false)
      .parse(is_finale)

    @is_generic = Arg
      .name('is_generic')
      .one_of(true, false)
      .parse(is_generic)

    # TODO: Validate these more thoroughly!
    @tags = Arg
      .name('tags')
      .is_a(Array)
      .parse(tags)

    # TODO: Validate these more thoroughly!
    @genres = Arg
      .name('genres')
      .is_a(Array)
      .parse(genres)

    # TODO: Validate these more thoroughly!
    @credits = Arg
      .name('credits')
      .is_a(Array)
      .parse(credits)
  end

  def movie?; @is_movie; end
  def new?; @is_new; end
  def live?; @is_live; end
  def premier?; @is_premier; end
  def finale?; @is_finale; end
  def generic?; @is_generic; end

  def to_xml
    attrs = {
      start: format_xmltv_time(@start_time),
      stop: format_xmltv_time(@end_time),
      channel: @channel.xml_id,
    }

    children = []

    if @image_url
      children << Xml.new(:icon, { src: @image_url })
    end

    if @season && @episode
      id =
        [
          "S#{@season.to_s.rjust(2, '0')}",
          "E#{@episode.to_s.rjust(2, '0')}",
      ].join
      children << Xml.new(:episode_num, { system: :onscreen }, id)
    end

    children << Xml.new(:title, { lang: :en }, @title)
    if @secondary_title
      children << Xml.new(:sub_title, { lang: :en }, @secondary_title)
    end

    children << rating_xml()

    # Include both genres and tags as categories since it makes sense to be
    # able to filter by any combination of the two, e.g. for queries like
    # "HDTV Sports".
    [*@genres, *@tags].each do |category_text|
      children << Xml.new(:category, { lang: :en }, category_text)
    end

    # Include any credits.
    unless @credits.empty?
      credits_xml = Xml.new(:credits, {}, *@credits.map do |credit|
        type, name, role = credit.fetch_values(:type, :name, :role)
        credits_attrs = {}

        # Add the `role` attribute, i.e. who they played, for actors whenever
        # possible.
        role = credit.fetch(:role)
        if type == :actor && !role.nil?
          credits_attrs[:role] = role
        end

        Xml.new(type, credits_attrs, name)
      end)

      children << credits_xml
    end

    # Include the description tag last since it messes up pretty printing as
    # it contains raw text.
    if @description
      children << Xml.new(:desc, { lang: :en }, @description)
    end

    children << Xml.new(:last_chance) if finale?
    children << Xml.new(:new) if new?
    children << Xml.new(:premiere) if premier?

    Xml.new(:programme, attrs, *children)
  end

  # If a rating is present, returns an XMLTV tag representing it. Otherwise,
  # returns `nil`.
  def rating_xml # Xml | nil
    return nil if rating.nil?

    attributes = {}
    children = [
      # The rating is always included as the value of the overall tag.
      Xml.new(:value, {}, @rating)
    ]

    # If the rating is one of the "standard" US FCC values, include an `icon`
    # tag for it and identify it as such.
    if RATINGS_TO_ICON_URLS.include?(@rating)
      attributes[:system] = 'TV parental guidelines'
      children << Xml.new(:icon, {}, RATINGS_TO_ICON_URLS[@rating])
    end

    Xml.new(:rating, attributes, *children)
  end

  private

  # Format time in the way the XMLTV specification desires.
  def format_xmltv_time(
    t # Time
  ) # String
    t.strftime('%Y%m%d%H%M%S %z')
  end
end

# A config-specific struct that allows for easy access to arbitrary values while
# also providing some helper methods that we use elsewhere.
class Config < OpenStruct
  # Require that the given configuration value be present in the parsed
  # configuration, then return it if present.
  def require(
    key, # Symbol
    name = nil # String, defaults to being auto-generated from `key`
  ) # typeof self[key]
    # "Humanize" the config key if an explicit name wasn't given.
    if name.nil?
      name = key
        .to_s
        .gsub(/_+/, ' ')
        .strip
    end

    value = self[key]

    if value.nil?
      article_text =
        if /^[aeiou]/.match?(name)
          "An"
        else
          "A"
        end
      raise ArgumentError, "#{article_text} #{name} is required but was not specified; see --help for details."
    end

    value
  end

  # Pretty-print our configuration for ease of logging.
  def to_s
    "Config(\n" + each_pair.map do |(key, value)|
      # Omit certian gigantic and not-terribly-useful keys.
      if [:option_parser, :cache].include?(key)
        next nil
      end

      "  #{key.inspect} => #{value.inspect}"
    end.compact.join("\n") + "\n)"
  end
end

# A simple cache, either in-memory or backed by a file. All cached values are
# converted to symbolized JSON hashes/arrays.
class Cache
  attr_reader \
    :path

  # Reads existing values from the given path, if any, otherwise uses a
  # memory-only backing store.
  def initialize(
    path = nil # Pathname | nil
  )
    @path = path
    @is_dirty = false

    @cache = {}

    # Initialize the cache from the backing store, if given.
    if !path.nil? && path.exist?
      begin
        @cache = path.open('rb') do |f|
          compressed_json_text = f.read

          # The data is stored GZipped, so we must decompress it before parsing
          # the JSON data contained within.
          json_text = Zlib.gunzip(compressed_json_text)

          # We use `JSON#parse` instead of `JSON#load` so we don't have to trust
          # the input file we were given, which the documentation suggests we must
          # do if using `JSON#load`.
          JSON.parse(json_text, symbolize_names: true)
        end
      rescue StandardError => err
        raise IOError, "Unable to load cache at #{path}: #{err}"
      end

      unless @cache.is_a?(Hash)
        raise IOError, "#{path} contains invalid cache file contents"
      end
    end
  end

  # Returns the number of entries in the cache.
  #
  # Note that this count includes expired entries that haven't yet been pruned!
  def size
    @cache.size
  end

  # Returns whether the cache has been modified since being loaded.
  def dirty?
    @is_dirty
  end

  # Write the cache to the originally-configured path, if any. If no path was
  # originally configured, does nothing.
  def persist! # self
    # If we were given no path, we can't persist the cache.
    if @path.nil?
      LOG.debug('Unable to persist in-memory cache')
      return
    end

    # If we're not dirty, i.e. if the cache hasn't been modified, do nothing to
    # save loads of time. While this will leave "dirty" entries in the cache,
    # it's still better than having to re-persist the possibly very large cache
    # if nothing has actually changed!
    unless dirty?
      LOG.debug('Declining to persist unmodified (i.e. not dirty) cache')
      return
    end

    # Remove expired values from the cache prior to dumping it so we won't waste
    # time and space storing and serializing expired entries.
    prune!

    LOG.debug("Persisting cache to path #{@path}")
    @path.open('wb') do |f|
      # Since we control this, we know it's non-circular.
      json_text = JSON.fast_generate(@cache)

      LOG.debug("Stringified cache JSON is #{json_text.bytesize} bytes")

      # GZip the data heavily since it can get quite large (100Mb or more!) and
      # compresses very well since it's mostly JSON and contains a lot of
      # human-generated text (Wikipedia articles, plot summaries, etc.).
      compressed_json_text = Zlib.gzip(json_text, level: Zlib::BEST_COMPRESSION)

      LOG.debug do
        "Compressed cache JSON string is #{compressed_json_text.bytesize} bytes, #{100.0 * compressed_json_text.bytesize / json_text.bytesize}% of original size"
      end

      f.write(compressed_json_text)
    end

    # Once the cache has been persisted, it's no longer considered "dirty" since
    # it hasn't changed since the last write to disk.
    LOG.debug('Marking persisted cache as non-dirty')
    @is_dirty = false

    self
  end

  # Prunes all currently-expired entries from the cache.
  def prune! # self
    now = Time.now.getutc

    @cache.each_pair do |key, entry|
      if entry_expired?(entry, now: now)
        LOG.debug do
          "Pruning entry for key #{key} from cache (expired as of #{entry_expiration_time(entry, now: now)}) and marking cache as dirty"
        end

        # Only mark the cache as dirty if something is actually pruned from it.
        @is_dirty = true
        @cache.delete(key)
      end
    end

    self
  end

  # Retrieves the given key from the cache if present, returning `nil` if not
  # present.
  def get(
    cache_key, # String | Symbol
    now: Time.now.getutc # Time
  )
    cache_key = cache_key.to_sym

    # If the entry has expired, delete it from the cache entirely and return
    # `nil`.
    entry = @cache[cache_key]
    if entry.nil?
      nil
    elsif entry_expired?(entry, now: now)
      # If we're expiring an entry, we've modified the cache and it should be
      # re-persisted.
      @is_dirty = true
      @cache.delete(cache_key)
      nil
    else
      entry.fetch(:value)
    end
  end

  # Adds the given key/value to the cache, overwriting any existing one. Returns
  # the value.
  def set(
    cache_key, # String | Symbol,
    value, # JSON
    ttl_seconds, # Integer
    now: Time.now.getutc # Time
  ) # typeof value
    # Since something is being updated, mark the cache as "dirty" so we'll know
    # to actually persist it if asked.
    @is_dirty = true

    Arg
      .name('ttl_seconds')
      .is_a(Integer)
      .min(1)
      .parse(ttl_seconds)
    cache_key = cache_key.to_sym

    @cache[cache_key] = {
      created_at: now.to_i,
      ttl_seconds: ttl_seconds,
      value: value
    }

    value
  end

  # Load a JSON value from the cache. If no value is found, the given block is
  # run and the resulting value is stored in the cache under the given key
  # before being returned.
  def fetch(
    cache_key, # String | Symbol
    ttl_seconds, # Integer
    &block # () => JSON
  ) # the cached value
    Arg
      .name('ttl_seconds')
      .is_a(Integer)
      .min(1)
      .parse(ttl_seconds)

    # Return the value directly if it's already cached.
    now = Time.now.getutc
    value = get(cache_key, now: now)
    return value unless value.nil?

    # If not already cached, run the block and set the cache to the new value,
    # then return the new value.
    set(cache_key, yield, ttl_seconds, now: now)
  end

  private

  def entry_expiration_time(
    entry, # Cache entry hash
    now: Time.now.getutc # Time
  )
    Time.at(entry.fetch(:created_at) + entry.fetch(:ttl_seconds)).getutc
  end

  def entry_expired?(
    entry, # Cache entry hash
    now: Time.now.getutc # Time
  )
    now > entry_expiration_time(entry, now: now)
  end
end

# Returns a new `Time` truncated to the given unit and converted to UTC.
def truncate_time(
  time, # Time
  unit # Integer seconds, e.g. 3600 for an hour or 86400 for a day
) # Time
  (time - (time.to_i % unit)).getutc
end

# Parse the configuration from the command line options.
def parse_config_from_args(
  args # An arguments array, typically `ARGV`.
)
  config = Config.new({
    # The `OptionParser` instance that parsed this config. This is useful so
    # whoever gets the config can inspect e.g. all the parsed arguments that
    # created it.
    option_parser: nil, # OptionParser | nil

    # These `return_*?` options indicate that the program should run in the
    # requisite "mode" and do something, then exit.

    # Generic hacks for better `--help` output
    return_help_text?: false,
    return_version_info?: false,

    # XMLTV-specific
    return_capabilities?: false,
    return_description?: false,

    # Program-specific
    return_providers?: false,
    return_channels?: false,
    return_lineup?: false,

    # XMLTV-specific configuration, mostly optional.
    cache: Cache.new, # Cache, in-memory by default
    day_offset: 0, # Integer
    days_to_fetch: ZAP2IT_MAXIMUM_DAYS_TO_FETCH, # Integer
    output_file: $stdout, # File-like

    # Default to "warning" level as a nice mix between being quiet and being
    # noisy. If the user wishes to change this, they can choose a different
    # level with any of `--debug`, `--verbose`, or `--quiet`.
    log_level: :warn,

    # Program-specific configuration, mostly required.
    channel_tokens: Set.new, # Set<String>
    country: nil, # :USA | :CAN
    postal_code: nil, # String
    provider_id: nil, # String
  })

  option_parser = OptionParser.new do |opts|
    opts.program_name = PROGRAM_NAME
    opts.version = "v#{VERSION}" # Pre-pending `v` makes it look nicer
    opts.release = RELEASE

    ############################################################################
    # GENERIC OPTIONS
    ############################################################################

    # Manually specifying these ensures they show up in `--help` output instead
    # of simply being silently available without obvious provenance.

    opts.on(
      '-h',
      '--help',
      'Output program help text and exit.'
    ) do
      config[:return_help_text?] = true
    end

    opts.on(
      '-v',
      '--version',
      'Output program version information and exit.'
    ) do
      config[:return_version_info?] = true
    end

    ############################################################################
    # XMLTV OPTIONS
    ############################################################################

    #
    # Minimum required options
    #

    opts.on(
      '--description',
      'Output our XMLTV description and exit.'
    ) do
      config[:return_description?] = true
    end

    opts.on(
      '--capabilities',
      'Output our supported XMLTV capabilities and exit.'
    ) do
      config[:return_capabilities?] = true
    end

    #
    # `baseline` options
    #

    opts.on(
      '-q',
      '--quiet',
      'Log only at the error level and higher.'
    ) do
      config.log_level = :error
    end

    output_flag = '--output'
    opts.on(
      '-o PATH',
      "#{output_flag} PATH",
      'Write output to this file instead of standard output.'
    ) do |p|
      config.output_file = Arg
        .name(output_flag)
        .path_name
        .stream('w')
        .parse(p)
    end

    days_flag = '--days'
    opts.on(
      '-d DAYS',
      "#{days_flag} DAYS",
      'The number of days of data to obtain, default "as many as are available"'
    ) do |d|
      config.days_to_fetch = Arg
        .name(days_flag)
        .int
        .min(1)
        .max(ZAP2IT_MAXIMUM_DAYS_TO_FETCH)
        .parse(d)
    end

    offset_flag = '--offset'
    opts.on(
      '-f DAYS',
      "#{offset_flag} DAYS",
      'Obtain data starting on the date this many days from today. Default is 0, i.e. "today", 1 is "tomorrow", etc.'
    ) do |o|
      config.day_offset = Arg
        .name(offset_flag)
        .int
        .parse(o)
    end

    config_file_flag = '--config-file'
    opts.on(
      "#{config_file_flag} PATH",
      'The path to the (optional) configuration file for this grabber.'
    ) do |p|
      config_path = Arg
        .name(config_file_flag)
        .path_name
        .exists
        .parse(p)

      # Reads command line options from the given file and parses them.
      #
      # We do this "in line" so that options are applied "logically", i.e.
      # options that come _before_ this flag are overwritten, but options that
      # come _after_ this flag overwrite previously-loaded options.
      opts.load(config_path)
    end

    #
    # `cache` options
    #

    cache_flag = '--cache'
    opts.on(
      "#{cache_flag} PATH",
      'If given, a file in which to cache network requests across invocations.'
    ) do |p|
      cache_filename = Arg
        .name(cache_flag)
        .path_name
        .parse(p)

      config.cache = Cache.new(cache_filename)
    end

    ############################################################################
    # PROGRAM OPTIONS
    ############################################################################

    debug_flag = '--debug'
    opts.on(
      debug_flag,
      'Enable all logging; this is the most verbose level of output available.',
    ) do
      config.log_level = :debug
    end

    verbose_flag = '--verbose'
    opts.on(
      verbose_flag,
      'Enable logging at the info level and higher.',
    ) do
      config.log_level = :info
    end

    country_flag = '--country'
    opts.on(
      '-c COUNTRY',
      "#{country_flag} COUNTRY",
      'The country for which to fetch data, one of USA or CAN.'
    ) do |s|
      config.country = Arg
        .name(country_flag)
        .strip
        .one_of('USA', 'CAN')
        .symbolize
        .parse(s)
    end

    postal_code_flag = '--postal-code'
    opts.on(
      '-p CODE',
      "#{postal_code_flag} CODE",
      'The postal code for which to fetch data. Something like `12345` for the USA or `A1A 1A1` for Canada.`'
    ) do |s|
      config.postal_code = Arg
        .name(postal_code_flag)
        .strip
        .substitute(/\s+/, ' ') # Condense spaces for uniformity
        .non_empty
        .parse(s)
    end

    show_provider_info_flag = '--show-providers'
    opts.on(
      show_provider_info_flag,
      "Output provider information for the given #{country_flag} and #{postal_code_flag} values."
    ) do |s|
      config[:return_providers?] = true
    end

    provider_id_flag = '--provider-id'
    opts.on(
      '-r ID',
      "#{provider_id_flag} ID",
      "The provider id for which to fetch data, obtained with the help of the #{show_provider_info_flag} flag."
    ) do |s|
      config.provider_id = Arg
        .name(provider_id_flag)
        .strip
        .non_empty
        .parse(s)
    end

    show_channels_flag = '--show-channels'
    opts.on(
      show_channels_flag,
      "Output the channels for the given #{country_flag}, #{postal_code_flag}, and #{provider_id_flag} values.",
    ) do
      config[:return_channels?] = true
    end

    channels_flag = '--channels'
    opts.on(
      "#{channels_flag} CHANNELS",
      "A comma-delimited list of channel call signs, numbers, and/or ids for which to download data. If non are provided, all available channels will be downloaded.",
    ) do |s|
      raw_channels = Arg
        .name(channels_flag)
        .strip
        .non_empty
        .parse(s)

      config.channel_tokens = raw_channels
        .split(/,/)
        .map(&:strip) # Handle interior whitespace
        .reject(&:empty?) # Remove empty strings
        .to_set
    end

    show_lineup_flag = '--show-lineup'
    opts.on(
      show_lineup_flag,
      "Output the program lineup(s) for the given #{channels_flag} values.",
    ) do
      config[:return_lineup?] = true
    end
  end

  # It's useful for external things to have access to this at times, e.g. for
  # printing help text.
  config.option_parser = option_parser

  # Parse the given arguments into our config object.
  option_parser.parse(args)

  config
end

# A map of host names to the last time we completed a request to them. We use
# this to enforce a minimum time between HTTP requests to a given host.
LAST_FETCH_END_TIMES_BY_HOST = Hash.new { Time.at(0).getutc }

# A cache for requests that fail a full retry cycle. We use this to effectively
# skip them if they've failed in this manner recently so as not to clog up the
# request pipeline needlessly with likely-failing requests.
FETCH_RETRY_FAILURE_TIMES_BY_CACHE_KEY = Hash.new { Time.at(0).getutc }

# Makes a GET or POST request to some host/path combination and returns the JSON
# response with symbolized keys. If the request fails or its response body can't
# be parsed, raises an `IOError`.
def fetch(
  *path_parts, # Array<any>
  query: nil, # Hash<Symbol, any> | nil
  data: nil, # Hash<Symbol | String, JSON> | nil
  cache: Cache.new, # Cache
  ttl_seconds: nil, # Integer
  should_retry_posts: false # Boolean, whether POST requests should be retried
) # JSON
  full_path = File.join(*path_parts.map(&:to_s))

  uri =
    begin
      u = URI.parse(full_path)

      unless query.nil?
        u.query = URI.encode_www_form(query.to_a)
      end

      u
    rescue URI::InvalidURIError => err
      raise ArgumentError, "Invalid URI from path #{full_path.inspect}: #{err}"
    end

  LOG.info do
    parts = ["Fetching URI #{uri}"]
    parts << " with data #{data.inspect}" if data
    parts.join
  end

  cache_key = digest(
    u, # The full request URL
    URI.encode_www_form(data || {}), # POST data for the request, if any
    format: :base64,
    length: 32
  )

  # If a request has failed a full retry cycle recently, ignore it and
  # immediately raise an appropriate error so we don't clog up the request
  # pipeline with always-failing requests.
  now = Time.now.getutc
  if FETCH_RETRY_FAILURE_TIMES_BY_CACHE_KEY[cache_key] > now - (5 * TimeInterval::MINUTES)
    raise IOError, "Fast-failing HTTP request to URI #{uri} since it failed a full retry cycle recently"
  end

  LOG.debug do
    parts = ["Looking up request to #{uri}"]
    parts << " with data #{URI.encode_www_form(data)}" unless data.nil?
    parts << " in cache"
    parts.join
  end
  cache.fetch(cache_key, ttl_seconds) do
    # Delay the fetch as necessary to enforce a minimum amount of time between
    # requests to any one host.
    #
    # Note that we do this _within the cache block_ so that any already-cached
    # values aren't unnecessarily delayed.
    last_fetch_end_time_for_host = LAST_FETCH_END_TIMES_BY_HOST[uri.host]
    earliest_next_fetch_start_time = last_fetch_end_time_for_host + FETCH_MIN_SECONDS_BETWEEN_REQUESTS
    if now < earliest_next_fetch_start_time
      wait_seconds = earliest_next_fetch_start_time - now
      LOG.debug("Delaying for #{wait_seconds}s before next request to host #{uri.host}")

      sleep(wait_seconds)
    end

    begin
      method = data.nil? ? :GET : :POST

      LOG.debug("Response not found in cache, making #{method} request to URI #{uri}")

      # Don't allow things to take too long since we might be making a lot of
      # requests.
      res =
        begin
          # This is the first attempt, so we count from 1 instead of 0.
          attempts ||= 1

          response = Net::HTTP.start(
            uri.host,
            uri.port,
            use_ssl: u.scheme == 'https',
            open_timeout: HTTP_REQUEST_TIMEOUT_SECONDS,
            read_timeout: HTTP_REQUEST_TIMEOUT_SECONDS,
            write_timeout: HTTP_REQUEST_TIMEOUT_SECONDS,
            ssl_timeout: HTTP_REQUEST_TIMEOUT_SECONDS,
          ) do |http|
            body_data =
              if data.nil?
                nil
              else
                URI.encode_www_form(data)
              end

            http.send_request(
              method,
              uri.request_uri,
              body_data,

              # As a good netizen, we identify ourself unambiguously.
              { 'User-Agent' => VERSION_INFO }
            )
          end

          # According to the docs, simply calling this method should raise a
          # HTTP error if the result wasn't a success.
          response.value

          response
        rescue \
            Net::HTTPBadResponse,
            Net::HTTPFatalError,
            Net::HTTPServerError,
            Net::HTTPTooManyRequests,
            Net::OpenTimeout,
            Net::ReadTimeout,
            Net::WriteTimeout \
            => err
          # If we've failed too many times or aren't supposed to retry, give up.
          if attempts > 5 || (method == :POST && !should_retry_posts)
            LOG.debug do
              if method == :POST && !should_retry_posts
                "Declining to retry POST request to URI #{uri}"
              else
                "Failed to make HTTP #{method} request to URI #{uri} after #{attempts} attempts"
              end
            end

            # Mark this request as having failed a full retry cycle recently so
            # we can skip it if it comes up again in the near future.
            FETCH_RETRY_FAILURE_TIMES_BY_CACHE_KEY[cache_key] = Time.now.getutc

            # Convert to `IOError` so we can easily handle this in the caller if
            # desired.
            raise IOError, "Failed to make HTTP #{method} request to URI #{uri}: #{err}"
          end

          LOG.warn("Got non-successful HTTP response (#{err}); retrying (attempts so far: #{attempts})")

          backoff_seconds = 2 ** attempts
          LOG.debug("Backing off for #{backoff_seconds}s before retrying HTTP #{method} request to URI #{uri}")
          sleep(backoff_seconds)

          attempts += 1
          retry
        end

      # Parse the body as JSON, the only response format we expect to receive.
      body = res.body
      begin
        LOG.debug('Attempting to parse response body as JSON')
        JSON.parse(body, symbolize_names: true)
      rescue JSON::ParserError => err
        LOG.warn('Failed initial parse of response body, retrying with corrected body')

        # Guess what? Sometimes, the API returns improperly-escaped JSON
        # strings!
        #
        # These seem to take the form of faulty attempts to escape quotes within
        # strings, where the backslash is escaped instead of the quote itself.
        #
        # E.g. we've seen things like:
        #
        # ```json
        # {
        #    "prop": "Foo \\"bar\\" baz!"
        # }
        # ```
        #
        # These should _probably_ be:
        #
        # ```json
        # {
        #    "prop": "Foo \"bar\" baz!"
        # }
        # ```
        #
        # If the parse fails, we make a last-ditch attempt to "rescue" it by
        # replacing such known instances of bad escaping, before finally giving
        # up if we're unable to do so.
        begin
          corrected_body = body.gsub(/\\\\"/, '\\"')
          JSON.parse(corrected_body, symbolize_names: true)
        rescue JSON::ParserError => err
          raise IOError, "Failed to parse HTTP response from #{method} request to URI #{uri}: #{err}"
        end
      end
    ensure
      # We mark the _true_ end of the fetch, including response parsing, so we
      # can conservatively enforce our rate limit between _response times_
      # rather than between _request times_.
      #
      # This is useful since any given response might take several seconds to
      # come in, and we don't want to allow making another separate request
      # immediately, possibly overloading the remote server, just because the
      # prior one took a long time.
      #
      # Note that we also do this for failed responses, which ensures that even
      # if failures are happening we're being nice to the API.
      last_host_fetch_time = Time.now.getutc
      LOG.debug("Setting last fetch time for host #{uri.host} to #{last_host_fetch_time}")
      LAST_FETCH_END_TIMES_BY_HOST[uri.host] = last_host_fetch_time
    end
  end
end

# Retrieves the Wikitext content from the given page info with revisions if it
# has any, otherwise returns `nil`.
def get_page_content(
  page_info # Wikipedia page info JSON response
) # String | nil
  page_content_container = page_info.dig(:revisions, 0, :slots, :main)
  return nil if page_content_container.nil?

  # We expect only Wikitext content and fail on anything else.
  return nil unless page_content_container.fetch(:contentformat) == 'text/x-wiki'

  page_content_container.fetch(:content)
end

# Parse all the links from the given page's content, if any, returning up to
# `limit` of them (default no limit).
def get_page_link_titles(
  page_info, # Wikipedia page info JSON response
  limit: 999_999_999
) # Array<String>, possibly empty
  # Parse the page content to find links, returning them if found.
  page_content = get_page_content(page_info)
  return [] if page_content.nil?

  # This doesn't perfectly match all links since it'll probably miss those with
  # `]` or `|` in their titles, but it's good enough for our purposes.
  page_content
    .scan(
      %r[
        # Link open token
        \[\[

        \s*

        # One or more chars that's not a pipe or a likely close token, consuming
        # as little as possible to obtain this (i.e. excluding trailing
        # whitespace). This is the title of the link, i.e. the page title to
        # which it refers.
        (
          [^\|\]]+?
        )

        # Optionally, a pipe followed by some more _optional_ text that's not a
        # close token. This is the "text" of the link, what's displayed to the
        # user if different from the title given. If a pipe with no following
        # text is given, this gets auto-looked up from the linked page,
        # apparently.
        (?:
          \s*

          \|

          \s*

          [^\]]*
        )?

        \s*

        # Link close token
        \]\]
      ]x
    )
    .lazy
    .map(&:first) # Obtain the first group, i.e. the page title
    .map(&:strip) # Ensure it has no extra whitespace
    .take(limit)
    .to_a
end

# Returns `true` when the given page info looks like it represents a television
# station with affiliation information, `false` otherwise.
def does_page_contain_tv_affiliation_info(
  page_info, # Wikipedia page info JSON response
  general_call_sign: nil # The page's call sign, ignored if not provided
) # Boolean
  # The page's title starts with our general call sign, if given.
  has_title_prefix =
    if general_call_sign.nil?
      true
    else
      page_info.fetch(:title).start_with?(general_call_sign)
    end

  # The page content looks like it probably contains the TV station affiliation
  # information template.
  page_content = get_page_content(page_info)
  has_tv_info = /\{\{\s*#{Regexp.escape(WIKIPEDIA_TV_STATION_INFOBOX_TEMPLATE_TITLE)}/.match?(page_content)

  has_title_prefix && has_tv_info
end

# Retrive the Wikipedia page information for a TV station and obtain the
# affiliate information from said page. If no affiliate information can be found
# for the given call sign prefix, returns `nil`.
def fetch_station_affiliate_info(
  country, # :USA | :CAN
  call_sign, # String, something like `KABCDT1`
  cache: Cache.new # Cache
) # Hash<String channel number, String affiliate name> | nil
  LOG.info("Fetching station affiliations for channel call sign #{call_sign} and country #{country}")

  # TODO: Support Canada as well!
  return nil unless country == :USA

  # There's no need to update the page information frequently since it shouldn't
  # change very often at all, or at least not in a way that materially affects
  # us.
  ttl_seconds = 30 * TimeInterval::DAYS

  # First, do a general search for pages prefixed with the "general" call sign,
  # i.e. the first four characters of the call sign we were given. This turns
  # something like `KVUEDT` into `KVUE`, which is the title a Wikipedia entry
  # for a TV station call sign almost universally starts with.
  #
  # If we find a page that contains a TV station affiliate information template
  # and has a title that starts with our general call sign, this almost
  # certainly the page we're looking for!
  #
  # For an example query request, see
  # https://en.wikipedia.org/w/api.php?action=query&format=json&prop=revisions&generator=allpages&formatversion=2&rvprop=content&rvslots=main&gapprefix=KADT&gaplimit=10
  general_call_sign = call_sign[0...4]
  general_query_response = fetch(
    WIKIPEDIA_API_HOST,

    query: {
      # Get a JSON response in the latest stable version.
      format: :json,
      formatversion: 2,

      # We're looking for something!
      action: :query,

      # Get general and template page information. A "template" is essentially a
      # standardized way of displaying information on a Wikipedia page.
      prop: 'revisions',
      generator: :allpages,

      # Include the latest revision so we can parse its text to obtain the data
      # we're looking for.
      rvprop: :content,
      rvslots: :main,

      # Look for pages that start with our general call sign. We limit the
      # number of results since if nothing shows up in the first handful, it's
      # quite unlikely that we're going to find anything relevant anyway.
      gapprefix: general_call_sign,
      gaplimit: 10,
    },

    cache: cache,
    ttl_seconds: ttl_seconds,
  )

  # Pull the page info values from the query response.
  general_query_page_info_responses = general_query_response.dig(:query, :pages)

  # If we got no responses at all, we're out of luck.
  if general_query_page_info_responses.nil? || general_query_page_info_responses.empty?
    return nil
  end

  # First, look for _any_ response that happens to fit exactly what we were
  # looking for.
  page_info = general_query_page_info_responses.find do |general_page_info|
    does_page_contain_tv_affiliation_info(
      general_page_info,
      general_call_sign: general_call_sign,
    )
  end

  # If we didn't find any page info directly, generate and fetch a list of page
  # titles at which to look for TV affiliate information. These can come from
  # redirects and/or disambiguation pages.
  #
  # For an example query that contains both of these kinds of results, see
  # https://en.wikipedia.org/w/api.php?action=query&format=json&prop=revisions&generator=allpages&formatversion=2&rvprop=content&rvslots=main&gapprefix=KADT&gaplimit=10
  if page_info.nil?
    # Match strings that look like they might be TV station call sign page
    # titles.
    starts_with_call_sign_regex = /^[KW][A-Z0-9]{3}/

    # Generate an array of links that look like they might contain the
    # information we seek.
    promising_page_titles = []

    # First, add call-sign-like redirect pages since these are the most likely
    # source of data.
    general_query_page_info_responses.each do |general_page_info|
      # Is the page a redirect? Add it if the redirect link looks like a call
      # sign page.
      content = get_page_content(general_page_info)
      if /#REDIRECT/.match(content)
        # The first link in a redirect page is the page to which it redirects.
        redirect_page_title = get_page_link_titles(general_page_info, limit: 1)
          .first

        if starts_with_call_sign_regex.match?(redirect_page_title)
          promising_page_titles << redirect_page_title
        end
      end
    end

    # Next, add links that look like they point at call sign page titles.
    #
    # This "pattern" is indicative of disambiguation pages that link off to
    # other TV stations, e.g. because the call sign overlaps with some other
    # thing that uses the same four general call sign letters.
    #
    # Luckily for us, it seems that most (all?) television station pages have
    # titles that start with their general call signs!
    general_query_page_info_responses.each do |general_page_info|
      page_link_titles = get_page_link_titles(general_page_info)
      page_link_titles.each do |title|
        if starts_with_call_sign_regex.match?(title)
          promising_page_titles << title
        end
      end
    end

    # Remove any duplicate page titles so we don't waste our time re-fetching
    # and re-parsing duplicate responses.
    promising_page_titles.uniq!

    # Try each page title one-by-one until (if...) we find a page that matches
    # our requirements.
    #
    # For an example request for a particular page, see
    # https://en.wikipedia.org/w/api.php?action=query&format=json&prop=revisions&generator=allpages&formatversion=2&rvprop=content&rvslots=main&gapfrom=KADT-LD&gapto=KADT-LD&gaplimit=1
    promising_page_titles.each do |page_title|
      potential_page_info_response = fetch(
        WIKIPEDIA_API_HOST,

        query: {
          format: :json,
          formatversion: 2,

          action: :query,

          prop: 'revisions',
          generator: :allpages,

          rvprop: :content,
          rvslots: :main,

          # Look for one page with the exact title we're looking for.
          gapfrom: page_title,
          gapto: page_title,
          gaplimit: 1,
        },

        cache: cache,
        ttl_seconds: ttl_seconds,
      )

      # If we got no response at all, we're out of luck.
      potential_page_info = potential_page_info_response.dig(:query, :pages, 0)
      next if potential_page_info.nil?

      # If it's not a TV station page, keep looking.
      next unless does_page_contain_tv_affiliation_info(potential_page_info)

      # Otherwise, we found our page!
      page_info = potential_page_info
      break
    end
  end

  # If we _still_ didn't find what we're looking for, there's no hope left.
  return nil if page_info.nil?

  # Grab the Wikitext markup from the first (and only) revision that should have
  # been returned; we'll parse this to obtain our data.
  page_wikitext = get_page_content(page_info)
  return nil if page_wikitext.nil?

  # Parse the page and its required TV station infobox template. If we can't do
  # either of these, we're not going to be able to extract any useful
  # information from the page.
  wikitext_tokens = tokenize_wikitext(page_wikitext)
  infobox_attributes_to_tokens = parse_station_infobox(wikitext_tokens)
  return nil if infobox_attributes_to_tokens.nil?

  parse_station_affiliations(infobox_attributes_to_tokens)
end

# Tokenizes some text, assumed to be Wikitext, and returns a lazy enumerable
# over its tokens.
def tokenize_wikitext(
  s # String
) # Array<String>
  s
    .split(WIKITEXT_TOKENIZER)
    .lazy
    .map(&:strip)
    .reject(&:empty?)
    .to_a
end

# Extracts the first television station infobox template from the given token
# stream and returns it, or returns `nil` if no such template could be found.
def parse_station_infobox(
  wikitext_tokens # Array<String>
) # Hash<Symbol parameter name, Array<String Wikitext tokens>> | nil
  # We're looking for text much like the following:
  #
  # ```wikitext
  # {{Infobox television station
  # | foo = ...
  # ...
  # | bar = ...
  # }}
  #
  # To obtain it, we search through the token stream to find its first
  # occurrence, then parse each attribute's tokens into a hash until the
  # template ends.
  # ```

  infobox_hash = {}

  template_start_pattern = [
    WIKITEXT_TOKEN_TEMPLATE_START,
    WIKIPEDIA_TV_STATION_INFOBOX_TEMPLATE_TITLE,
  ].freeze
  found_infobox_template = false

  # Whether we're parsing an argument of the template, i.e. something like:
  # ```wikitext
  # | foo = [[bar]]
  # ```
  current_argument_name = nil

  seen_tokens = []
  depth = 0
  wikitext_tokens.each do |token|
    seen_tokens << token

    # Look for the beginning of our station info template.
    if (
        !found_infobox_template &&
        seen_tokens[-template_start_pattern.length..] == template_start_pattern
    )
      # Throw everything else away since at this point we only care about
      # looking for the coming argument token patterns.
      seen_tokens.clear
      depth = 0
      found_infobox_template = true
      next
    end

    # Find the affiliations section once we're inside the station info template.
    if (
        found_infobox_template &&
        current_argument_name.nil? &&
        depth.zero? &&

        # Check whether the token stream looks like an argument definition,
        # something like:
        # ```wikitext
        # | foo =
        # ```
        seen_tokens[-3] == WIKITEXT_TOKEN_PIPE &&
        seen_tokens[-1] == WIKITEXT_TOKEN_EQUALS
    )
      current_argument_name = seen_tokens[-2]
      seen_tokens.clear
      next
    end

    # Once we've returned to zero depth and found the end of the argument
    # definition (either the end of the parent template or we've encountered the
    # next template attribute), all the tokens we've seen make up the contents
    # of the argument and we can continue to the next argument.
    if (
        found_infobox_template &&
        !current_argument_name.nil? &&
        depth.zero? &&
        [WIKITEXT_TOKEN_PIPE, WIKITEXT_TOKEN_TEMPLATE_END].include?(token)
    )
      # Remove the trailing token since this may be necessary to start parsing
      # the next argument by matching the expected `| foo =` token sequence.
      trailing_token = seen_tokens.pop

      # Stash this argument's tokens in the resulting hash.
      infobox_hash[current_argument_name.to_sym] = seen_tokens.dup

      # If we encounter a closing template token to finish parsing this
      # argument, we've finished parsing the entire template!
      break if trailing_token == WIKITEXT_TOKEN_TEMPLATE_END

      # Reset state so we can parse the next argument, including re-adding the
      # trailing token we saved earlier so we can match the next argument.
      seen_tokens.clear
      seen_tokens << trailing_token
      current_argument_name = nil
      next
    end

    # Generally, track the current parse depth so we'll know when to stop
    # parsing the current argument attribute and the overall template.
    case token
      when WIKITEXT_TOKEN_TEMPLATE_START
        depth += 1
      when WIKITEXT_TOKEN_CLASS_START
        depth += 1
      when WIKITEXT_TOKEN_LINK_START
        depth += 1

      when WIKITEXT_TOKEN_TEMPLATE_END
        depth -= 1
      when WIKITEXT_TOKEN_CLASS_END
        depth += 1
      when WIKITEXT_TOKEN_LINK_END
        depth -= 1
    end
  end

  # If we never found the template we were looking for, signal as much by
  # returning `nil`.
  return nil unless found_infobox_template

  infobox_hash
end

# Retrieve the channel numbers, digital and/or virtual, that a given station
# broadcasts under.
def parse_channel_numbers(
  infobox_attributes_to_tokens # Hash<Symbol parameter name, Array<String Wikitext tokens>>
) # Array<Integer channel numbers>
  # The first integer in a channel info attribute should be the channel number;
  # the attribute token text should look something like:
  # ```wikitext
  # 33 ([[ultra high frequency|UHF]])`
  # ```
  infobox_attributes_to_tokens
    .fetch_values(:digital, :virtual) { [] }
    .map(&:join)
    .inject([]) do |all_channel_numbers, channel_number_wikitext|
      match = /(?<channel_number>\d+)/.match(channel_number_wikitext)
      next all_channel_numbers if match.nil?

      all_channel_numbers << match[:channel_number].to_i
      all_channel_numbers
    end
end

# Given some Wikitext markup, attempts to parse out TV station affiliations from
# the corresponding tokens, returning `nil` if unable to do so.
def parse_station_affiliations(
  infobox_attributes_to_tokens # Hash<Symbol parameter name, Array<String Wikitext tokens>>
) # Hash<String station number, String station name> | nil
  # Either of these attributes can contain the information we seek, namely a
  # list of channel numbers to affiliate names. We prefer the attribute that
  # seems to most commonly contain the information, `affiliations`.
  affiliations_tokens = infobox_attributes_to_tokens.fetch(:affiliations)
  if affiliations_tokens.nil? || affiliations_tokens.empty?
    affiliations_tokens = infobox_attributes_to_tokens.fetch(:subchannels)
  end

  # If the infobox has no affiliations, we're out of luck.
  return nil if affiliations_tokens.nil? || affiliations_tokens.empty?

  # Affiliations infoboxes don't seem to have a "standard" channel number by
  # which they list their affiliations. E.g., a station may operate under
  # channels 21 and 43, but the affiliations will only be listed with channel
  # 21.
  #
  # Alternatively, an affiliations section may list only a single affiliate with
  # _no_ channel numbers at all!
  #
  # In these cases, we want to know what the actual channel number(s) for the
  # station are so we can map _all_ channels to their corresponding
  # affiliations, and/or do a "speculative" mapping if the affiliation(s) have
  # no channel numbers listed.
  channel_numbers = parse_channel_numbers(infobox_attributes_to_tokens)

  # We expect most affiliations to come in the form of an "unbulleted list"
  # (https://en.wikipedia.org/wiki/Template:Unbulleted_list). We parse out the
  # leading pipe so that from here on, every time we see a pipe we can take all
  # the tokens we've accumulated and turn them into a human-readable name and
  # channel number.
  ubl_template_start_pattern = [
    WIKITEXT_TOKEN_TEMPLATE_START,
    'ubl',
    WIKITEXT_TOKEN_PIPE,
  ].freeze

  br_tag_token = %r[^<\s*br\s*/?\s*>$]

  # We now attempt to detect and post-process the major affiliations attribute
  # variants we see.
  if affiliations_tokens[0...ubl_template_start_pattern.length] == ubl_template_start_pattern
    # This is the "classic" and most common by far variant where the
    # affiliations list contains a single `ubl` template of channel numbers
    # mapped to station names. In this case, we do nothing at all!
  elsif affiliations_tokens.find { |t| br_tag_token.match?(t) }
    # Sometimes, we get a "manually-constructed list" that was created with
    # `<br>` and/or `<br />` tags instead of a "fancy" list created with the
    # `ubl` template.
    #
    # When we see a manually-constructed list, we convert it into a `ubl`
    # template so we can parse the `ubl` format later.
    affiliations_tokens = [
      *ubl_template_start_pattern,

      # Replace all the `<br>` tags with pipes since the tags are effectively
      # acting as list item separators.
      *affiliations_tokens.map do |token|
        if br_tag_token.match?(token)
          WIKITEXT_TOKEN_PIPE
        else
          token
        end
      end,

      WIKITEXT_TOKEN_TEMPLATE_END,
    ]
  else
    # If it doesn't match either one of these, treat whatever's already there as
    # a single list item.

    # If it doesn't look like the affiliate text contains a channel number, give
    # it bunch of channel numbers corresponding to the channel numbers we parsed
    # earlier. If for some reason we didn't parse _any_ channel numbers, give it
    # a fake "catch all" channel number of `*`.
    #
    # E.g. we're going to turn something like `Network` into a bunch of pairs
    # like `'''21:''' Network`, `'''21.1:''' Network`, etc. assuming that we'll
    # catch the _actual_ channel number in there somewhere.

    # If we couldn't find even a single channel number for this channel, give up
    # entirely since the data we're looking at doesn't seem reliable anyway.
    return nil if channel_numbers.empty?

    # If it looks like the affiliations contain no channel numbers, re-map them
    # to a bunch of common channel numbers instead since presumably at least one
    # of the resulting entries is the actual channel number.
    remapped_affiliations_tokens =
      if affiliations_tokens.count(WIKITEXT_TOKEN_BOLD) < 2
        channel_numbers.flat_map do |channel_parent_number|
          # Include both the "parent" number as well as variants for a bunch of
          # "sub" numbers, e.g. if the channel number is 21 then return `["21",
          # "21.1", "21.2", ...]`
          (0..9).flat_map do |channel_sub_number|
            channel_number =
              if channel_sub_number.zero?
                channel_parent_number.to_s
              else
                "#{channel_parent_number}.#{channel_sub_number}"
              end

            [
              # Channel number
              WIKITEXT_TOKEN_BOLD,
              "#{channel_number}:",
              WIKITEXT_TOKEN_BOLD,

              # Name
              *affiliations_tokens,

              # Argument separator for the next channel number
              WIKITEXT_TOKEN_PIPE,
            ]
          end
        end[0...-1] # Omit the trailing argument separator
      else
        affiliations_tokens
      end

    affiliations_tokens = [
      *ubl_template_start_pattern,
      *remapped_affiliations_tokens,
      WIKITEXT_TOKEN_TEMPLATE_END,
    ]
  end

  if affiliations_tokens[0...ubl_template_start_pattern.length] != ubl_template_start_pattern
    return nil
  else
    # Strip off the template start tokens since we don't need them.
    ubl_template_start_pattern.length.times { affiliations_tokens.shift }
  end

  raw_affiliations = []

  found_link = false
  found_tag = false
  affiliation_tokens = []
  affiliations_tokens.each do |token|
    affiliation_tokens << token

    # Skip bold text markers entirely, effectively un-bolding all bolded text.
    if token == WIKITEXT_TOKEN_BOLD
      affiliation_tokens.pop
      next
    end

    # Skip all tags and their contents. Typically, these are `ref` tags and we
    # don't care about them.
    if !found_tag && WIKITEXT_TOKEN_START_TAG.match?(token)
      affiliation_tokens.pop
      found_tag = true
      next
    end

    # Discard all tokens until we find a matching end tag.
    if found_tag
      affiliation_tokens.pop

      if WIKITEXT_TOKEN_END_TAG.match?(token)
        found_tag = false
      end

      next
    end

    if !found_link && token == WIKITEXT_TOKEN_LINK_START
      found_link = true
      next
    end

    # "Unwrap" links to obtain only their text, if any. See
    # https://en.wikipedia.org/wiki/Help:Wikitext#Links_and_URLs for more
    # information on Wikitext link syntax.
    if found_link && token == WIKITEXT_TOKEN_LINK_END
      # Rewind the token stream to the start of the link, consuming all the
      # tokens for specific processing here.
      link_text_tokens = []
      until link_text_tokens.last == WIKITEXT_TOKEN_LINK_START || affiliation_tokens.empty? do
        link_text_tokens << affiliation_tokens.pop
      end

      # Reverse the tokens we just collected them in reverse order to obtain the
      # "natural" order for further processing.
      link_text_tokens.reverse!

      # Strip off the leading and trailing link markers. If we didn't find the
      # appropriate tokens there, explode since something is wrong.
      first_link_token = link_text_tokens.shift
      last_link_token = link_text_tokens.pop
      if first_link_token != WIKITEXT_TOKEN_LINK_START && last_link_token != WIKITEXT_TOKEN_LINK_END
        raise IOError, 'Encountered invalid Wikitext link'
      end

      # "Unwrap" the link to contain only its actual text.
      #
      # These are most of the potential Wikitext link formats:
      # ```wikitext
      # [[standard link]]
      # [[link|renamed link]]
      # [[auto-renamed link|]]
      # [[blended link]]ing
      # [[Wikipedia:Page link#To a section]]
      # [[it:Otra lingua]]
      # [[Wiktionary:intra-wiki link]]
      # ```
      #
      # In practice, we hope to only see standard and renamed links here, though
      # technically any of these is possible.
      if link_text_tokens.include?(WIKITEXT_TOKEN_PIPE)
        unpiped_tokens = []
        found_pipe = false
        link_text_tokens.each do |token|
          unpiped_tokens << token

          if !found_pipe && token == WIKITEXT_TOKEN_PIPE
            unpiped_tokens.clear
            found_pipe = true
          end
        end

        # If we found no tokens after the pipe, simply remove the pipe from the
        # original tokens and treat the remaining tokens as the text of the
        # link. This happens with auto-renamed links, which look like
        # `[[Foo|]]`; in this case, we use the standard link text before the
        # pipe.
        if unpiped_tokens.empty?
          link_text_tokens.reject! { |t| t == WIKITEXT_TOKEN_PIPE }
        else
          link_text_tokens = unpiped_tokens
        end
      end

      # Add the unwrapped link text tokens back to the original token stream.
      affiliation_tokens.concat(link_text_tokens)
      found_link = false
      next
    end

    # If we find a bare pipe token or the end of the `ubl` template, this
    # indicates we've completed an item and can output it for later processing.
    if !found_link && token == WIKITEXT_TOKEN_PIPE || token == WIKITEXT_TOKEN_TEMPLATE_END
      affiliation_tokens.pop
      raw_affiliations << affiliation_tokens.join
      affiliation_tokens.clear
    end
  end

  # We should have consumed _all_ the affiliation tokens since they should have
  # been only a single `ubl` template instance. If they weren't we have no idea
  # what the affiliations contained and are unlikely to be able to parse out
  # anything useful from our resulting text affiliations.
  return nil unless affiliation_tokens.empty?

  # If we found multiple channel numbers, we'll re-map those we _did_ find in
  # the affiliations to their associated "parent" numbers. By looking only for
  # channel numbers we know we have, we avoid "weirdness" of re-mapping
  # affilations with channel numbers we don't already know about.
  channel_numbers_matcher =
    if channel_numbers.length < 2
      nil
    else
      /^(?:#{channel_numbers.map { |n| Regexp.escape(n.to_s) }.join('|')})/
    end

  # Turn the text affiliations into channel numbers and channel names.
  raw_affiliations.inject({}) do |all, raw_affiliation|
    # We expect the text we have at this point to look like:
    # ```
    # 1.2: Channel name (maybe some text in parenthesis)
    # ```

    # Split into the "channel" and "name" parts we expect.
    channel, raw_name = raw_affiliation
      .split(':', 2)
      .map(&:strip)
      .reject(&:empty?)

    # This channel/name didn't conform to our expected format.
    next all if channel.nil? || raw_name.nil?

    # Strip all parenthesized text from the channel name since it's usually
    # something like `(O&O)` for "owned and operated", which is irrelevant for
    # our purposes.
    name = raw_name
      .gsub(/\([^)]+\)/, '')
      .strip

    # If the name was _only_ parenthesized, it'd be a very odd name indeed!
    next all if name.empty?


    if channel_numbers_matcher.nil?
      all[channel] = name
    else
      channel_numbers.each do |channel_number|
        remapped_channel_number = channel.gsub(
          channel_numbers_matcher,
          channel_number.to_s
        )
        all[remapped_channel_number] = name
      end
    end

    all
  end
end

# Downloads and returns lineup (i.e. provider) data for the given country and
# postal code.
def fetch_providers(
  country, # :USA | :CAN
  postal_code, # String, e.g. `12345` or `A1A 1A1`
  cache: Cache.new # Cache
) # Array<Provider>
  LOG.info("Fetching providers for country #{country} and postal code #{postal_code}")

  provider_json = fetch(
    ZAP2IT_API_HOST,
    '/gapzap_webapi/api/Providers/getPostalCodeProviders',
    country,
    postal_code,
    '/gapzap/en',

    # This response is expected to change very infrequently, if ever.
    cache: cache,
    ttl_seconds: TimeInterval::WEEKS,
  )

  provider_json
    .fetch(:Providers)
    .sort_by { |p| [p.fetch(:type), p.fetch(:name)] } # A human-friendly sort for display
    .map do |p|
      Provider.new(
        country: country,
        id: p.fetch(:headendId),
        name: p.fetch(:name),
        postal_code: postal_code,
        type: p.fetch(:type),
      )
    end
end

# Fetch overall provider info and return the provider that corresponds to the
# given parameters. If no provider is found, returns `nil`.
def fetch_provider(
  country, # :USA | :CAN
  postal_code, # String
  provider_id, # String
  cache: Cache.new # Cache
) # Provider | nil
  LOG.info("Fetching single provider id #{provider_id} for country #{country} and postal code #{postal_code}")

  providers = fetch_providers(
    country,
    postal_code,
    cache: cache,
  )

  providers.find do |provider|
    (
      provider.country == country &&
      provider.postal_code == postal_code &&
      provider.id == provider_id
    )
  end
end

# Downloads and returns the channels available for the given provider at the
# given time/time span, potentially limiting them to some list of channels.
def fetch_channels(
  provider, # Provider
  date = Time.now.getutc, # Time, but will be truncated to its date component
  time_span_hours = 6, # 1 | 2 | 3 | 4 | 5 | 6
  channel_tokens: Set.new, # Enumerable<String>. If non-empty, channels to fetch
  cache: Cache.new # Cache
) # Array<Channel>
  LOG.info do
    channels_string =
      if channel_tokens.empty?
        'all channels'
      else
        "channels for tokens #{channel_tokens.to_a.sort.join(', ')}"
      end

    "Fetching #{channels_string} from provider #{provider} (time span: #{time_span_hours}h, date: #{date})"
  end

  # We need fast random access to these.
  channel_tokens = channel_tokens.to_set

  # When fetching channels, we really only care about "right now" and don't
  # expect the values to change much over time. However, we cache the response
  # for a while anyway.
  ttl_seconds = 1 * TimeInterval::DAYS

  lineup_json = fetch(
    ZAP2IT_API_HOST,
    '/api/grid',
    query: {
      # Unused since we give `headendId`, but apparently necessary anyway!
      lineupId: '-',

      # Provider info
      headendId: provider.id,
      country: provider.country,
      postalCode: provider.postal_code,

      # Time info, truncated to "today" for caching purposes.
      timespan: time_span_hours,
      time: truncate_time(date, ttl_seconds).to_i, # Needs to be Unix time, apparently
    },

    cache: cache,
    ttl_seconds: ttl_seconds,
  )

  # We sort by channel number for a human-friendly display and limit our
  # channels to the ones requested.
  raw_channels = lineup_json
    .fetch(:channels)
    .sort_by do |c|
      [
        (Float(c.fetch(:channelNo)) rescue c.fetch(:channelNo)),
        c.fetch(:callSign),
      ]
    end
    .select do |c|
      (
        # If we weren't given any channels to which to limit our fetch, keep all
        # channels.
        channel_tokens.empty? ||

        # Otherwise, keep only channels that have an identifier that matches one
        # of the given tokens.
        channel_tokens.include?(c.fetch(:channelId)) ||
        channel_tokens.include?(c.fetch(:callSign)) ||
        channel_tokens.include?(c.fetch(:channelNo))
      )
    end

  # If we got no channels after filtering things down, clearly tell the user
  # what happened.
  if raw_channels.empty?
    channel_tokens_string = channel_tokens
      .to_a
      .sort
      .join(', ')
    raise ArgumentError, "Could not find any channels matching these tokens: #{channel_tokens_string}"
  end

  raw_channels.map do |c|
    affiliates = fetch_station_affiliate_info(
      provider.country,
      c.fetch(:callSign),
      cache: cache,
    ) || {}

    channel_name = affiliates[c.fetch(:channelNo)]

    Channel.new(
      provider: provider,
      call_sign: c.fetch(:callSign),
      name: channel_name,
      id: c.fetch(:channelId),
      number: c.fetch(:channelNo),
      image_url: c.fetch(:thumbnail),
    )
  end
end

# Fetch overall channel info and return the channel that corresponds to the
# given token. If no channel is found, returns `nil`.
def fetch_channel(
  provider, # Provider
  token, # String, one of a channel call sign, number, or id
  date = Time.now.getutc, # Time, but will be truncated to its date component
  time_span_hours = 6, # 1 | 2 | 3 | 4 | 5 | 6
  cache: Cache.new # Cache
) # Channel | nil
  LOG.info do
    "Fetching single channel for token #{token} from provider #{provider} (time span: #{time_span_hours}h, date: #{date})"
  end

  channels = fetch_channels(
    provider,
    date,
    time_span_hours,
    cache: cache
  )

  channels.find do |channel|
    (
      channel.call_sign == token ||
      channel.id == token ||
      channel.number == token
    )
  end
end

def get_lineup_date_cache_key(
  channel, # Channel
  start_time # Time, but will be truncated to its date
) # String
  digest(
    # These are the same values as required by the actual API request, so we use
    # them for symmetry.
    channel.id,
    channel.provider.country,
    channel.provider.postal_code,
    channel.provider.id,

    # The date string for which we want to find guide data.
    #
    # We use our own to ensure that the cached date is always consistent,
    # regardless of what/how the zap2it API decides to return the date keys for
    # the lineup. Mainly, this is guarding against zap2it returning something
    # like `2020-1-1` instead of `2020-01-01`.
    truncate_time(start_time, TimeInterval::DAYS).strftime('%Y-%m-%d'),
  )
end

# Returns `true` when the given series id looks like that of a movie.
def is_movie_id(
  series_id # String
) # Boolean
  # Movies have series ids that start with `MV`!
  /^MV/i.match?(series_id)
end

# Attempts to fetch and return detailed program information for the given
# program/series combination. This works for both movies _and_ televsion shows!
def fetch_program_details(
  series_id, # String, the `seriesId` value fetched from lineup program info
  program_id, # String, the `tmsId` fetched from lineup program info
  season_index: nil, # Integer, one-based, the season to fetch. `nil` means "latest season", 1 means "first season", etc.
  cache: Cache.new # Cache
) # JSON program details
  Arg
    .name('series_id')
    .is_a(String)
    .non_empty
    .parse(series_id)
  Arg
    .name('program_id')
    .is_a(String)
    .non_empty
    .parse(program_id)
  unless season_index.nil?
    Arg
      .name('season_index')
      .is_a(Integer)
      .min(1)
      .parse(season_index)
  end

  LOG.info("Fetching detailed program information for series id #{series_id}, program id #{program_id} (season index: #{season_index.inspect})")

  # The API only allows us to look up detailed information for a whole season at
  # a time. Since we often don't even know which season a particular episode
  # came from since the initial API call returns null, we instead always start
  # by fetching the latest season, then walking the season list backwards until
  # we find the episode we're looking for.
  #
  # We fetch from latest to earliest under the assumption that most often newer
  # episodes are aired in preference to older ones.

  # When fetching the latest season, we don't cache the result for very long.
  # This ensures that unfinished seasons are re-checked regularly, but "old"
  # seasons are cached for a good long time since presumably they won't change
  # very often at all!
  ttl_seconds =
    if season_index.nil?
      TimeInterval::DAYS
    else
      90 * TimeInterval::DAYS
    end

  # TODO: "Local programming" (and presumably other things) have a series id but
  # making this request doesn't return anything _at all_, i.e. returns zero
  # bytes, when we attempt to fetch it. Deal with this!
  season_info =
    begin
      fetch(
        ZAP2IT_API_HOST,
        '/gapzap_webapi/api/program/PostEpisodeGuide',

        data: {
          # A magical required value!
          aid: :gapzap,

          # Essentially, "fetch all the data in a single page". This seems to work
          # without issue in all attempted cases, so... we don't have to paginate!
          pageNo: 1,
          pageSize: 999,

          # Fetch the series/season we care about right now.
          programSeriesID: series_id,
          season: season_index || -1, # Convert `nil` to `-1` for "latest season"
        },

        should_retry_posts: true,
        cache: cache,
        ttl_seconds: ttl_seconds,
      )
    rescue IOError => err
      # If one of these requests perma-fails, return `nil` and pretend we simply
      # couldn't find the info. Since this info is nice-to-have, we can proceed
      # without it.
      LOG.warn("Detailed info request for series id #{series_id}, program id #{program_id}, season index #{season_index.inspect} failed: '#{err}'; returning nothing instead of exploding")
      return nil
    end

  episodes_info = season_info
    .fetch(:episodeGuideTab)
    .fetch(:season)
    .fetch(:episodes)

  # See if this response contained the episode information we're looking for.
  #
  # The `tmsId` value we get in `program_id` is prefixed with `SH` or `sh`
  # coming from the program guide, but is listed as `ep` in the season
  # details...
  #
  # We munge during the comparison with the program details episode id to
  # account for this and any general variation on it.
  id_munge_prefix_regex = /^(SH|EP)/i
  munged_query_episode_id = program_id.gsub(id_munge_prefix_regex, '')
  episode_details = episodes_info.find do |episode_info|
    munged_candidate_episode_id = episode_info.fetch(:tmsID).gsub(id_munge_prefix_regex, '')
    munged_candidate_episode_id == munged_query_episode_id
  end

  # If we didn't find what we were looking for in this season, iterate backwards
  # to the next season until we either find our episode or run out of seasons.
  if episode_details.nil?
    potential_season_indices = season_info
      .fetch(:episodeGuideTab)
      .fetch(:seasons)
      .map { |s| s.to_i(10) } # We expect all seasons to be string integers
      .sort
      .reverse

    # Turn `nil` into the latest season id.
    current_season_index =
      if season_index.nil?
        # Use the largest season index, i.e. the latest season, if we used the
        # "shortcut" of `nil` to fetch the season.
        potential_season_indices.max
      else
        # Otherwise, use the value we were given.
        season_index
      end

    previous_season_index = current_season_index - 1
    return (
      # Season 0 appears to be a nonsense season and doesn't actually count for
      # anything. It looks like it contains a bunch of dummy episodes most of
      # the time, none of which appears to contain useful information anyway!
      if !potential_season_indices.include?(previous_season_index) || previous_season_index.zero?
        # If we get to the point where we want to search through the episodes of
        # the prior season, but there isn't a season that fits the bill, we're
        # out of seasons and aren't going to find the episode details we want.
        nil
      else
        # Recursively fetch and look through the prior season's details.
        fetch_program_details(
          series_id,
          program_id,
          season_index: previous_season_index,
          cache: cache,
        )
      end
    )
  end

  # Since we found the desired episode details, we can now fetch the general
  # overview information for the series, which contains things like writers,
  # directory, actors, etc. that we can use for program credits.
  overview_info =
    begin
      fetch(
        ZAP2IT_API_HOST,
        '/api/program/overviewDetails',

        data: {
          programSeriesID: series_id,
        },

        should_retry_posts: true,
        cache: cache,
        ttl_seconds: (
          if is_movie_id(series_id)
            # Movies are considered "done", so we can cache their information for a
            # long time.
            30 * TimeInterval::DAYS
          else
            # Cache TV series overview information for a shorter time than that for
            # movies since many series are ongoing and may have changing cast and
            # crew credits.
            7 * TimeInterval::DAYS
          end
        ),
      )
    rescue IOError => err
      LOG.warn("Overview info request for series id #{series_id} failed with error '#{err}'; proceeding without it")

      # If we can't fetch this data, ignore it since it's not critical that we
      # get it.
      nil
    end

  # Since we found our desired episode details, we can now parse them!

  # Can be the empty string when a rating is (presumably) unknown.
  display_rating = episode_details.fetch(:displayRating)
  rating =
    if display_rating.nil? || display_rating.empty?
      nil
    else
      display_rating
    end

  # Movies have their release year populated with a string year like `1976` but
  # an original air date of `1000-01-01`, whereas TV show episodes have an
  # accurate original air date but an empty string release year. We'll also
  # occasionally get neither!
  raw_release_year = episode_details.fetch(:releaseYear, '')
  raw_original_air_date = episode_details.fetch(:originalAirDate, '')
  original_release_date =
    if !raw_release_year.empty?
      # We use the first of the year as a dummy day since having a release year
      # almost certainly means we're dealing with a movie for which we _only_
      # have a year anyway.
      Date.parse("#{raw_release_year}-01-01").to_time.getutc
    elsif !raw_original_air_date.empty?
      Date.parse(raw_original_air_date).to_time.getutc
    else
      # This is the typical "dummy date" we see in the API responses, so we use
      # it for parity.
      Date.parse('1000-01-01').to_time.getutc
    end

  # If one or both of the potential dates was nonsense, we set the value to
  # `nil` to clearly indicate as much.
  original_release_date = nil if original_release_date.year < 1500

  title = episode_details.fetch(:episodeTitle).strip
  title = nil if title.empty?

  description = episode_details.fetch(:synopsis).strip
  description = nil if description.empty?

  # A season of zero appears to indicate some sort of dummy, placeholder season
  # most (all?) of the time and hence should act as if it's not present at all.
  # If present, these are 1-based, though "special" episodes (e.g. clip shows or
  # series retrospectives) might get an episode number of `0`.
  season = episode_details.fetch(:seasonNumber, '-1').to_i(10)
  season = nil if season <= 0

  episode = episode_details.fetch(:episodeNumber, '-1').to_i(10)
  episode = nil if episode < 0

  tags = episode_details
    .fetch(:tags)
    .split('|')
    .map(&:strip)
    .reject(&:empty?)

  genres = episode_details
    .fetch(:programGenres)
    .split('|')
    .map(&:strip)
    .reject(&:empty?)

  is_new = episode_details.fetch(:isNew)
  is_live = episode_details.fetch(:isLive)
  is_premier = episode_details.fetch(:isPremier)
  is_finale = episode_details.fetch(:isFinale)

  # The credits seem to always come to us in priority order, so we simply keep
  # them in the order in which they were received.
  credits = []
  unless overview_info.nil?
    cast_and_crew = [
      *overview_info.fetch(:overviewTab).fetch(:cast),
      *overview_info.fetch(:overviewTab).fetch(:crew),
    ]
    credits = cast_and_crew.map do |person|
      role = person.fetch(:role)
      name = person.fetch(:name)
      character_name =
        if (raw_character_name = person.fetch(:characterName).strip).empty?
          nil
        else
          raw_character_name
        end

      # Attempt to map the "role" value we were given to a supported kind of
      # XMLTV credit.
      type =
        if /director/i.match?(role)
          :director
        elsif /guest/i.match?(role)
          :guest
        elsif /actor/i.match?(role)
          :actor
        elsif /narrator/i.match?(role)
          # Narrators are just special actors to us!
          role = 'Narrator'
          :actor
        elsif /writer/i.match?(role)
          :writer
        elsif /adapter/i.match?(role)
          :adapter
        elsif /producer/i.match?(role)
          :producer
        elsif /composer/i.match?(role)
          :composer
        elsif /editor/i.match?(role)
          :editor
        elsif /presenter|host/i.match?(role)
          :presenter
        elsif /commentator/i.match?(role)
          :commentator
        else
          # If we can't map the kind of credit this person should receive, call
          # them a "guest" since it's better than nothing at all.
          LOG.warn("Couldn't parse credit type for role #{role.inspect}, falling back to 'guest'")
          :guest
        end

      {
        type: type,
        name: name,
        role: character_name,
      }
    end.compact
  end

  {
    # We use the id we were given since it may differ from the munged one the
    # episode info contains; we'll treat the input id as the canonical one.
    program_id: program_id,
    season: season,
    episode: episode,
    title: title,
    description: description,
    original_release_date: original_release_date,
    rating: rating,
    tags: tags,
    genres: genres,
    new?: is_new,
    live?: is_live,
    premier?: is_premier,
    finale?: is_finale,
    credits: credits,
  }
end

# Downloads and returns all the program data for the given date, if possible.
def fetch_lineup(
  channel, # Channel
  start_date = Time.now.getutc, # Time, but will be truncated to its date component
  end_date = start_date, # Time, but will be truncated to its date component
  cache: Cache.new # Cache
)
  LOG.info("Fetching program lineup channel #{channel} (start date: #{start_date}, end date: #{end_date})")

  start_date = truncate_time(start_date, TimeInterval::DAYS)
  end_date = truncate_time(end_date, TimeInterval::DAYS)

  # We allow these to be equal since our time span is inclusive.
  if end_date < start_date
    raise ArgumentError, "end_date (#{end_date}) must be greater than or equal to start_date (#{start_date})"
  end

  # Our start and end dates must fall within the period starting today and
  # ending 14 days from now. This range is unfortunately all the zap2it API will
  # allow us to retrieve.
  #
  # Technically the API will allow you to request yesterday's data through 14
  # days from _then_, however supporting this technically-15-day-period is more
  # trouble than it's worth as we'd need to make two highly-overlapping requests
  # to retrieve the ends of the ranges.
  now = Time.now.getutc
  min_start_time = truncate_time(now, TimeInterval::DAYS)
  max_end_time = truncate_time(now, TimeInterval::DAYS) + (ZAP2IT_MAXIMUM_DAYS_TO_FETCH * TimeInterval::DAYS)
  if start_date.to_i < min_start_time.to_i
    raise ArgumentError, "Start date of #{start_date} is too early, cannot be earlier than #{min_start_time}"
  elsif end_date.to_i > max_end_time.to_i
    raise ArgumentError, "End date of #{end_date} is too late, cannot be later than #{max_end_time}"
  end

  # Enumerate all the dates between the start and end dates (inclusive) so we
  # know whether we have to fetch new data or not.
  dates_to_fetch = []
  cur_date = start_date
  while cur_date <= end_date
    dates_to_fetch << truncate_time(cur_date, TimeInterval::DAYS)
    cur_date = cur_date + TimeInterval::DAYS
  end

  # First, we check the cache for any existing data for the given dates. This is
  # necessary since every request to the API contains 14 days of data, however
  # it gets cached by `fetch` as one giant blob.
  #
  # If/when we do a _real_ fetch for all this data, we cache it individually by
  # date so we can look up only the required date when necessary and save a
  # bunch of effort around effectively only being able to fetch data for "today
  # + 14 days" from zap2it and not being able to specify an arbitrary single
  # date.
  cached_lineup_data = dates_to_fetch.inject({}) do |all, date|
    cache_key = get_lineup_date_cache_key(channel, date)
    all[date] = cache.get(cache_key)
    all
  end

  # If any of the dates for which we want lineup data wasn't cached, we have to
  # fetch new data.
  need_to_fetch_from_api = cached_lineup_data.values.any?(&:nil?)

  # If there were any dates we couldn't find already-cached data for, we need to
  # attempt to fetch them from the API.
  if need_to_fetch_from_api
    # We cache for only a single day since program lineup info might change for
    # whatever reason, especially further in the future where there's less
    # certainty.
    ttl_seconds = TimeInterval::DAYS

    lineup_data_for_fortnight = fetch(
      ZAP2IT_API_HOST,
      '/api/sslgrid',

      data: {
        # A magical required value!
        aid: :gapzap,

        # Channel/provider info
        prgsvcid: channel.id,
        countryCode: channel.provider.country,
        postalCode: channel.provider.postal_code,
        headendId: channel.provider.id,

        # The number of hours of program lineup data to fetch.
        #
        # Only seems to support extremely specific values in certain contexts,
        # `336` (i.e. 14 days) being the most (only?) reliable value.
        timespan: ZAP2IT_MAXIMUM_DAYS_TO_FETCH * TimeInterval::DAYS / TimeInterval::HOURS,

        # To simplify reasoning about what date(s) to return etc., we truncate
        # the current date to midnight UTC and fetch data starting then.
        timestamp: truncate_time(now, TimeInterval::DAYS).to_i
      },

      should_retry_posts: true,
      cache: cache,
      ttl_seconds: ttl_seconds,
    )

    # The lineup data comes in mapped date string (e.g. `2020-01-02`) to array
    # of programs; we convert this into a cache entry per date.
    lineup_data_for_fortnight.each_pair do |date_sym, lineup_programs|
      # Update the cache with the information for this date alone, allowing it
      # to be accessed separately from the other dated lineups from the API
      # request from which it was fetched.
      lineup_date = Date.parse(date_sym.to_s).to_time.getutc
      cache_key = get_lineup_date_cache_key(channel, lineup_date)
      cache.set(cache_key, lineup_programs, ttl_seconds, now: now)
    end

    # Now that we've (hopefully...) cached the lineup for all of our original
    # dates, we can re-fetch the data directly from the cache.
    cached_lineup_data = cached_lineup_data.inject({}) do |all, (date, lineup_data)|
      # Re-fetch the data from the cache if we didn't originally find it there;
      # now that we've intentionally downloaded it from the API, it should
      # certainly exist in the cache!
      if lineup_data.nil?
        lineup_data = cache.get(get_lineup_date_cache_key(channel, date))
      end

      if lineup_data.nil?
        raise IOError, "Unexpectedly failed to look up fetched lineup data in cache for date #{date} and channel #{channel}"
      end

      all[date] = lineup_data
      all
    end
  end

  cached_lineup_data.values.flat_map do |lineup_data|
    lineup_data.map do |basic_program_info|
      series_id = basic_program_info.fetch(:seriesId)
      program_id = basic_program_info.fetch(:program).fetch(:tmsId)

      # First, grab whatever information we can from the program info we already
      # have. This often isn't complete, but if we can't fetch any more details
      # than this we'll have to make due with all that we got.
      start_time = Time.at(basic_program_info.fetch(:startTime)).getutc
      end_time = Time.at(basic_program_info.fetch(:endTime)).getutc

      title = basic_program_info.fetch(:program).fetch(:title)
      secondary_title =  basic_program_info.fetch(:program).fetch(:episodeTitle)
      description = basic_program_info.fetch(:program).fetch(:shortDesc)
      image_url = "https://zap2it.tmsimg.com/assets/#{basic_program_info.fetch(:thumbnail)}.jpg"

      # Season and episode come in as either `nil` or a string integer.
      season = basic_program_info.fetch(:program).fetch(:season)
      season = season.to_i(10) unless season.nil?
      episode = basic_program_info.fetch(:program).fetch(:episode)
      episode = episode.to_i(10) unless episode.nil?

      rating = basic_program_info.fetch(:rating)

      # These don't show up at all in the basic info :(
      genres = []
      credits = []

      raw_release_year = basic_program_info.fetch(:program).fetch(:releaseYear)
      original_release_date =
        if raw_release_year.nil?
          nil
        else
          Date.parse("#{raw_release_year.rjust(4, '0')}-01-01").to_time.getutc
        end

      # Generic comes in as `"0"` or `"1"` and appears to indicate things like
      # "paid programming".
      is_generic = basic_program_info.fetch(:program).fetch(:isGeneric) == '1'
      is_movie = is_movie_id(basic_program_info.fetch(:seriesId))

      # These "bits" are special in that in the basic info they come back as
      # tags, but in the enhanced info they come back as their own individual
      # flags. We search the basic info for any tags that indicate these flags,
      # then Boolean "or" them with any that come back from the enhanced info.
      #
      # TODO: Verify each tag name. I've only actually _seen_ `New`, so the rest
      # of the tag names are guesses for now!
      flag_tags = [
        new_tag = 'New',
        live_tag = 'Live',
        premier_tag = 'Premier',
        finale_tag = 'Finale',
      ]
      is_new = basic_program_info.fetch(:tag).include?(new_tag)
      is_live = basic_program_info.fetch(:tag).include?(live_tag)
      is_premier = basic_program_info.fetch(:tag).include?(premier_tag)
      is_finale = basic_program_info.fetch(:tag).include?(finale_tag)

      # Omit any of the "special" tags we just checked for since they're
      # redundant if included in both places.
      #
      # NOTE: The tags at the "top level" here in the basic info are actually
      # _better_ than the tags we get from the enhanced info! The "enhanced"
      # tags come in something like `STEREO|CC` and have to be manually cleaned,
      # so we prefer the "basic" tags instead.
      tags = (basic_program_info.fetch(:tag, [])).reject { |t| flag_tags.include?(t) }

      # Now, fetch detailed program info if possible and improve what we've
      # already gathers.
      enhanced_program_info = fetch_program_details(series_id, program_id, cache: cache)
      unless enhanced_program_info.nil?
        # We prefer the detailed version of this data if available, otherwise we
        # fall back to the "basic" version
        season = enhanced_program_info.fetch(:season, season)
        episode = enhanced_program_info.fetch(:episode, episode)

        # We "merge" these basic values with the enhanced ones.
        is_new = is_new || enhanced_program_info.fetch(:new?)
        is_live = is_live || enhanced_program_info.fetch(:live?)
        is_premier = is_premier || enhanced_program_info.fetch(:premier?)
        is_finale = is_finale || enhanced_program_info.fetch(:finale?)

        # If any of these values seem to be more "substantial" than the ones we
        # got from the basic info, we'll use them instead.
        secondary_title = secondary_title || enhanced_program_info.fetch(:title)
        if (enhanced_program_info.fetch(:title) || '').length > (secondary_title || '').length
          secondary_title = enhanced_program_info.fetch(:title)
        end

        description = description || enhanced_program_info.fetch(:description)
        if (enhanced_program_info.fetch(:description) || '').length > (description || '').length
          description = enhanced_program_info.fetch(:description)
        end

        # For these, we prefer the "enhanced" version wherever available.
        original_release_date = enhanced_program_info.fetch(:original_release_date, original_release_date)
        rating = enhanced_program_info.fetch(:rating, rating)

        # These don't show up at all in the "basic" info, so we _must_ use those
        # that come back from the enhanced info.
        genres = enhanced_program_info.fetch(:genres)
        credits = enhanced_program_info.fetch(:credits)
      end

      Program.new(
        channel: channel,
        series_id: series_id,
        program_id: program_id,
        start_time: start_time,
        end_time: end_time,
        image_url: image_url,
        season: season,
        episode: episode,
        title: title,
        secondary_title: secondary_title,
        description: description,
        rating: rating,
        original_release_date: original_release_date,
        tags: tags,
        genres: genres,
        is_movie: is_movie,
        is_new: is_new,
        is_live: is_live,
        is_premier: is_premier,
        is_finale: is_finale,
        is_generic: is_generic,
        credits: credits,

        # TODO: Where would we find this?
        previously_shown_time: nil,
      )
    end
  end
end

# Formats the given data nicely for display in the terminal as a table and
# returns the result.
#
# No more columns than those given will be displayed. If any row contains too
# few values for the given columns, empty strings will be displayed in the
# "holes".
def display_table(
  columns, # Array<Symbol | String>
  rows, # Array<Tuple<any>>
  column_padding: 2 # Integer, the space between columns
) # String
  # Turn everything into a string and resize all the rows to be no longer than
  # the number of columns. Any "missing" columns will be omitted from the output
  # entirely.
  max_column_value_lengths = columns.map { 0 }
  prepared_rows = [columns, *rows].map do |row|
     row
       .slice(0, columns.length) # Truncate to column length
       .map(&:to_s) # Stringify contents
       .each_with_index do |value, column_index| # Update max value size
         value_length = value.length
         if max_column_value_lengths[column_index] < value_length
           max_column_value_lengths[column_index] = value_length
         end
       end
  end

  column_padder = ' ' * column_padding
  prepared_rows.each_with_index.map do |row, index|
    row
      .each_with_index
      .map { |r, i| r.ljust(max_column_value_lengths[i]) }
      .join(column_padder)
      .strip
  end.join("\n")
end

# Returns a string for terminal output representing provider information.
def display_providers(
  country, # :USA | :CAN
  postal_code, # String
  cache: Cache.new # Cache
) # String
  providers = fetch_providers(
    country,
    postal_code,
    cache: cache
  )

  display_table(
    [:NAME, :TYPE, :ID],
    providers.map { |p| [p.name, p.type, p.id] },
  )
end

# Returns a string for terminal output representing the channel list for the
# given provider.
def display_channels(
  provider, # Provider
  channel_tokens: Set.new, # Enumerable<String> | nil. If non-empty, channels to display
  cache: Cache.new # Cache
) # String
  channels = fetch_channels(
    provider,
    channel_tokens: channel_tokens,
    cache: cache,
  )

  display_table(
    [:'CALL_SIGN', :NUMBER, :NAME, :ID],
    channels.map { |c| [c.call_sign, c.number, c.name, c.id] },
  )
end

# Returns a string for terminal output representing the program lineup for the
# given channel.
def display_lineup(
  channels, # Channel
  start_date = Time.now.getutc, # Time, inclusive, but will be truncated to its date component
  end_date = start_date, # Time, exclusive, but will be truncated to its date component
  cache: Cache.new # Cache
)
  programs = channels.flat_map do |channel|
    fetch_lineup(
      channel,
      start_date,
      end_date,
      cache: cache,
    )
  end

  date_format = '%Y-%m-%d'
  time_format = '%l:%M %p'
  display_table(
    [:CHANNEL, :DATE, :START, :END, :TITLE],
    programs.map do |p|
      [
        p.channel.call_sign,
        p.start_time.strftime(date_format),
        p.start_time.strftime(time_format),
        p.end_time.strftime(time_format),
        p.title
      ]
    end,
  )
end

def main!
  config = parse_config_from_args(ARGV).freeze

  # Set up logging as requested before we do anything else so we can ensure any
  # chose log settings are respected for as long as possible.
  LOG.level = config.log_level
  LOG.info("Logging configured for #{config.log_level.to_s.upcase}-level output")

  LOG.debug { "Parsed program configuration:\n#{config}" }

  $output = config.output_file
  LOG.debug("Directing standard output to #{$output}")

  cache = config.cache
  LOG.debug do
    if cache.path.nil?
      "Using in-memory cache only"
    else
      "Loaded cache from #{cache.path}"
    end
  end

  # Output program help text.
  #
  # This is handled as the very first "mode" since it's the most important part
  # of any command; its presence anywhere in the arguments indicates the user
  # doesn't know what to do and needs our guidance!
  if config.return_help_text?
    LOG.debug('Returning program help text')

    $output.puts(config.option_parser.help())
    return
  end

  # Output program version information.
  if config.return_version_info?
    LOG.debug('Returning program version string')

    $output.puts(VERSION_INFO)
    return
  end

  # Output our XMLTV description.
  if config.return_description?
    LOG.debug('Returning XMLTV description')

    $output.puts(XMLTV_DESCRIPTION)
    return
  end

  # Output our XMLTV capabilities.
  if config.return_capabilities?
    LOG.debug('Returning XMLTV capabilities')

    XMLTV_CAPABILITIES.each do |capability|
      $output.puts(capability)
    end
    return
  end

  # Download and print out the configured lineup information.
  if config.return_providers?
    LOG.debug('Displaying providers')

    country = config.require(:country)
    postal_code = config.require(:postal_code)

    output = display_providers(
      country,
      postal_code,
      cache: cache,
    )
    $output.puts(output)
    return
  end

  # Download and print out a listing of the channels for the given provider
  # information.
  if config.return_channels?
    LOG.debug('Displaying channels')

    country = config.require(:country)
    postal_code = config.require(:postal_code)
    provider_id = config.require(:provider_id)

    provider = fetch_provider(
      country,
      postal_code,
      provider_id,
      cache: cache,
    )

    if provider.nil?
      raise ArgumentError, "No provider found for country #{country}, postal code #{postal_code}, and id #{provider_id}"
    end

    output = display_channels(
      provider,
      channel_tokens: config.channel_tokens,
      cache: cache,
    )
    $output.puts(output)
    return
  end

  # Download and print out a listing of the channel lineups for the given
  # channels and times.
  if config.return_lineup?
    LOG.debug('Displaying lineup')

    country = config.require(:country)
    postal_code = config.require(:postal_code)
    provider_id = config.require(:provider_id)
    days_to_fetch = config.require(:days_to_fetch, 'number of days to fetch')
    day_offset = config.require(:day_offset, 'number of days to offset')

    provider = fetch_provider(
      country,
      postal_code,
      provider_id,
      cache: cache,
    )

    if provider.nil?
      raise ArgumentError, "No provider found for country #{country}, postal code #{postal_code}, and id #{provider_id}"
    end

    channels = fetch_channels(
      provider,
      channel_tokens: config.channel_tokens,
      cache: cache,
    )

    # Obey the offset and span values we were given. Note that we re-truncate
    # the end date to the prior date so that it'll be inclusive rather than
    # exclusive, as our offset and days-to-fetch values require.
    now = truncate_time(Time.now.getutc, TimeInterval::DAYS)
    start_date = now + (day_offset * TimeInterval::DAYS)
    end_date = truncate_time(
      start_date + (days_to_fetch * TimeInterval::DAYS) - 1,
      TimeInterval::DAYS
    )

    output = display_lineup(
      channels,
      start_date,
      end_date,
      cache: cache,
    )
    $output.puts(output)
    return
  end

  # The default action is always to output the XMLTV XML described at
  # https://github.com/XMLTV/xmltv/blob/80e893e0b6c77224f2ed353a0fb577e2f82c69d4/xmltv.dtd.
  # To do this, we need to fetch all the requisite channels and providers.
  LOG.debug('Outputting XML')
  country = config.require(:country)
  postal_code = config.require(:postal_code)
  provider_id = config.require(:provider_id)
  days_to_fetch = config.require(:days_to_fetch, 'number of days to fetch')
  day_offset = config.require(:day_offset, 'number of days to offset')

  provider = fetch_provider(
    country,
    postal_code,
    provider_id,
    cache: cache,
  )

  if provider.nil?
    raise ArgumentError, "No provider found for country #{country}, postal code #{postal_code}, and id #{provider_id}"
  end

  channels = fetch_channels(
    provider,
    channel_tokens: config.channel_tokens,
    cache: cache,
  )

  # Obey the offset and span values we were given. Note that we re-truncate
  # the end date to the prior date so that it'll be inclusive rather than
  # exclusive, as our offset and days-to-fetch values require.
  now = truncate_time(Time.now.getutc, TimeInterval::DAYS)
  start_date = now + (day_offset * TimeInterval::DAYS)
  end_date = truncate_time(
    start_date + (days_to_fetch * TimeInterval::DAYS) - 1,
    TimeInterval::DAYS
  )

  programs = channels.flat_map do |channel|
    fetch_lineup(
      channel,
      start_date,
      end_date,
      cache: cache,
    )
  end

  # Generate and write the channel and program XML to the output stream.
  $output.puts('<?xml version="1.0" encoding="utf-8"?>')
  $output.puts(
    Xml.new(
      :tv,
      {
        source_info_url: "https://tvlistings.zap2it.com",
        source_info_name: 'zap2it',
        generator_info_name: PROGRAM_NAME,
        generator_info_url: 'https://github.com/jasontbradshaw/tv_grab_zap2it',
      },
      *channels.map(&:to_xml),
      *programs.map(&:to_xml),
    )
  )
rescue StandardError => err
  # If we explode for any reason, exit with an error status and log the error
  # appropriately.
  LOG.fatal(err)

  # Re-raise the error if we're in debug mode. This gets us nice stack trace
  # information.
  raise if LOG.debug?

  exit 1
ensure
  # Always attempt to persist the cache once we're done, assuming it got
  # initialized.
  #
  # We don't have to worry about some partially-valid state since the cache
  # doesn't store entries that generate an exception during population, meaning
  # that whatever _does_ end up in the cache was at least valid enough to have
  # been generated by the code.
  if cache.nil?
    LOG.debug("Unable to persist a cache that doesn't exist")
  else
    cache.persist!
  end
end

main! if __FILE__ == $0
